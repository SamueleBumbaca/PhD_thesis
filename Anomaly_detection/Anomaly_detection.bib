
@misc{simonyanVeryDeepConvolutional2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	doi = {10.48550/arXiv.1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv:1409.1556 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{hughesOpenAccessRepository2016,
	title = {An open access repository of images on plant health to enable the development of mobile disease diagnostics},
	url = {http://arxiv.org/abs/1511.08060},
	doi = {10.48550/arXiv.1511.08060},
	abstract = {Human society needs to increase food production by an estimated 70\% by 2050 to feed an expected population size that is predicted to be over 9 billion people. Currently, infectious diseases reduce the potential yield by an average of 40\% with many farmers in the developing world experiencing yield losses as high as 100\%. The widespread distribution of smartphones among crop growers around the world with an expected 5 billion smartphones by 2020 offers the potential of turning the smartphone into a valuable tool for diverse communities growing food. One potential application is the development of mobile disease diagnostics through machine learning and crowdsourcing. Here we announce the release of over 50,000 expertly curated images on healthy and infected leaves of crops plants through the existing online platform PlantVillage. We describe both the data and the platform. These data are the beginning of an on-going, crowdsourcing effort to enable computer vision approaches to help solve the problem of yield losses in crop plants due to infectious diseases.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Hughes, David P. and Salathe, Marcel},
	month = apr,
	year = {2016},
	note = {arXiv:1511.08060 [cs]},
	keywords = {Computer Science - Computers and Society},
}

@misc{thapaPlantPathology20202020,
	title = {The {Plant} {Pathology} 2020 challenge dataset to classify foliar disease of apples},
	url = {http://arxiv.org/abs/2004.11958},
	doi = {10.48550/arXiv.2004.11958},
	abstract = {Apple orchards in the U.S. are under constant threat from a large number of pathogens and insects. Appropriate and timely deployment of disease management depends on early disease detection. Incorrect and delayed diagnosis can result in either excessive or inadequate use of chemicals, with increased production costs, environmental, and health impacts. We have manually captured 3,651 high-quality, real-life symptom images of multiple apple foliar diseases, with variable illumination, angles, surfaces, and noise. A subset, expert-annotated to create a pilot dataset for apple scab, cedar apple rust, and healthy leaves, was made available to the Kaggle community for 'Plant Pathology Challenge'; part of the Fine-Grained Visual Categorization (FGVC) workshop at CVPR 2020 (Computer Vision and Pattern Recognition). We also trained an off-the-shelf convolutional neural network (CNN) on this data for disease classification and achieved 97\% accuracy on a held-out test set. This dataset will contribute towards development and deployment of machine learning-based automated plant disease classification algorithms to ultimately realize fast and accurate disease detection. We will continue to add images to the pilot dataset for a larger, more comprehensive expert-annotated dataset for future Kaggle competitions and to explore more advanced methods for disease classification and quantification.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Thapa, Ranjita and Snavely, Noah and Belongie, Serge and Khan, Awais},
	month = apr,
	year = {2020},
	note = {arXiv:2004.11958 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{heDeepResidualLearning2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	doi = {10.48550/arXiv.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{huangDenselyConnectedConvolutional2017,
	title = {Densely {Connected} {Convolutional} {Networks}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.html},
	urldate = {2025-03-23},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
	year = {2017},
	pages = {4700--4708},
}

@misc{tanEfficientNetRethinkingModel2020,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {http://arxiv.org/abs/1905.11946},
	doi = {10.48550/arXiv.1905.11946},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Tan, Mingxing and Le, Quoc V.},
	month = sep,
	year = {2020},
	note = {arXiv:1905.11946 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{dosovitskiyImageWorth16x162021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{liuSwinTransformerHierarchical2021,
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}},
	shorttitle = {Swin {Transformer}},
	url = {http://arxiv.org/abs/2103.14030},
	doi = {10.48550/arXiv.2103.14030},
	abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with {\textbackslash}textbf\{S\}hifted {\textbackslash}textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	month = aug,
	year = {2021},
	note = {arXiv:2103.14030 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{oquabDINOv2LearningRobust2024,
	title = {{DINOv2}: {Learning} {Robust} {Visual} {Features} without {Supervision}},
	shorttitle = {{DINOv2}},
	url = {http://arxiv.org/abs/2304.07193},
	doi = {10.48550/arXiv.2304.07193},
	abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
	month = feb,
	year = {2024},
	note = {arXiv:2304.07193 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{FacebookresearchDinov22025,
	title = {facebookresearch/dinov2},
	copyright = {Apache-2.0},
	url = {https://github.com/facebookresearch/dinov2},
	abstract = {PyTorch code and models for the DINOv2 self-supervised learning method.},
	urldate = {2025-03-23},
	publisher = {Meta Research},
	month = mar,
	year = {2025},
	note = {original-date: 2023-03-29T16:00:37Z},
}

@misc{ModelsPretrainedWeights,
	title = {Models and pre-trained weights — {Torchvision} main documentation},
	url = {https://pytorch.org/vision/master/models.html},
	urldate = {2025-03-23},
}

@misc{RobustDeepLearningBasedDetector,
	title = {A {Robust} {Deep}-{Learning}-{Based} {Detector} for {Real}-{Time} {Tomato} {Plant} {Diseases} and {Pests} {Recognition}},
	url = {https://www.mdpi.com/1424-8220/17/9/2022},
	urldate = {2025-03-23},
}

@article{todaHowConvolutionalNeural2019,
	title = {How {Convolutional} {Neural} {Networks} {Diagnose} {Plant} {Disease}},
	volume = {2019},
	issn = {2643-6515},
	doi = {10.34133/2019/9237136},
	abstract = {Deep learning with convolutional neural networks (CNNs) has achieved great success in the classification of various plant diseases. However, a limited number of studies have elucidated the process of inference, leaving it as an untouchable black box. Revealing the CNN to extract the learned feature as an interpretable form not only ensures its reliability but also enables the validation of the model authenticity and the training dataset by human intervention. In this study, a variety of neuron-wise and layer-wise visualization methods were applied using a CNN, trained with a publicly available plant disease image dataset. We showed that neural networks can capture the colors and textures of lesions specific to respective diseases upon diagnosis, which resembles human decision-making. While several visualization methods were used as they are, others had to be optimized to target a specific layer that fully captures the features to generate consequential outputs. Moreover, by interpreting the generated attention maps, we identified several layers that were not contributing to inference and removed such layers inside the network, decreasing the number of parameters by 75\% without affecting the classification accuracy. The results provide an impetus for the CNN black box users in the field of plant science to better understand the diagnosis process and lead to further efficient use of deep learning for plant disease diagnosis.},
	language = {eng},
	journal = {Plant Phenomics (Washington, D.C.)},
	author = {Toda, Yosuke and Okura, Fumio},
	year = {2019},
	pmid = {33313540},
	pmcid = {PMC7706313},
	pages = {9237136},
}

@misc{zotero-1449,
	url = {https://openaccess.thecvf.com/content/CVPR2021/papers/Salehi_Multiresolution_Knowledge_Distillation_for_Anomaly_Detection_CVPR_2021_paper.pdf},
	urldate = {2025-03-23},
}

@misc{zotero-1450,
	url = {https://people.csail.mit.edu/wrvb/files/publications/liu2018deep.pdf},
	urldate = {2025-03-23},
}

@misc{chalapathyDeepLearningAnomaly2019,
	title = {Deep {Learning} for {Anomaly} {Detection}: {A} {Survey}},
	shorttitle = {Deep {Learning} for {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/1901.03407},
	doi = {10.48550/arXiv.1901.03407},
	abstract = {Anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. The aim of this survey is two-fold, firstly we present a structured and comprehensive overview of research methods in deep learning-based anomaly detection. Furthermore, we review the adoption of these methods for anomaly across various application domains and assess their effectiveness. We have grouped state-of-the-art research techniques into different categories based on the underlying assumptions and approach adopted. Within each category we outline the basic anomaly detection technique, along with its variants and present key assumptions, to differentiate between normal and anomalous behavior. For each category, we present we also present the advantages and limitations and discuss the computational complexity of the techniques in real application domains. Finally, we outline open issues in research and challenges faced while adopting these techniques.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Chalapathy, Raghavendra and Chawla, Sanjay},
	month = jan,
	year = {2019},
	note = {arXiv:1901.03407 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{10.1145/342009.335388,
author = {Breunig, Markus M. and Kriegel, Hans-Peter and Ng, Raymond T. and Sander, J\"{o}rg},
title = {LOF: identifying density-based local outliers},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335388},
doi = {10.1145/342009.335388},
abstract = {For many KDD applications, such as detecting criminal activities in E-commerce, finding the rare instances or the outliers, can be more interesting than finding the common patterns. Existing work in outlier detection regards being an outlier as a binary property. In this paper, we contend that for many scenarios, it is more meaningful to assign to each object a degree of being an outlier. This degree is called the local outlier factor (LOF) of an object. It is local in that the degree depends on how isolated the object is with respect to the surrounding neighborhood. We give a detailed formal analysis showing that LOF enjoys many desirable properties. Using real-world datasets, we demonstrate that LOF can be used to find outliers which appear to be meaningful, but can otherwise not be identified with existing approaches. Finally, a careful performance evaluation of our algorithm confirms we show that our approach of finding local outliers can be practical.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {93–104},
numpages = {12},
keywords = {outlier detection, database mining},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@article{breunig2000lof,
author = {Breunig, Markus M. and Kriegel, Hans-Peter and Ng, Raymond T. and Sander, J\"{o}rg},
title = {LOF: identifying density-based local outliers},
year = {2000},
issue_date = {June 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/335191.335388},
doi = {10.1145/335191.335388},
abstract = {For many KDD applications, such as detecting criminal activities in E-commerce, finding the rare instances or the outliers, can be more interesting than finding the common patterns. Existing work in outlier detection regards being an outlier as a binary property. In this paper, we contend that for many scenarios, it is more meaningful to assign to each object a degree of being an outlier. This degree is called the local outlier factor (LOF) of an object. It is local in that the degree depends on how isolated the object is with respect to the surrounding neighborhood. We give a detailed formal analysis showing that LOF enjoys many desirable properties. Using real-world datasets, we demonstrate that LOF can be used to find outliers which appear to be meaningful, but can otherwise not be identified with existing approaches. Finally, a careful performance evaluation of our algorithm confirms we show that our approach of finding local outliers can be practical.},
journal = {SIGMOD Rec.},
month = may,
pages = {93–104},
numpages = {12},
keywords = {outlier detection, database mining}
}

@article{scholkopf2001estimating,
    author = {Schölkopf, Bernhard and Platt, John C. and Shawe-Taylor, John and Smola, Alex J. and Williamson, Robert C.},
    title = {Estimating the Support of a High-Dimensional Distribution},
    journal = {Neural Computation},
    volume = {13},
    number = {7},
    pages = {1443-1471},
    year = {2001},
    month = {07},
    abstract = {Suppose you are given some data set drawn from an underlying probability distribution P and you want to estimate a “simple” subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified value between 0 and 1.We propose a method to approach this problem by trying to estimate a function f that is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a potentially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. The expansion coefficients are found by solving a quadratic programming problem, which we do by carrying out sequential optimization over pairs of input patterns. We also provide a theoretical analysis of the statistical performance of our algorithm.The algorithm is a natural extension of the support vector algorithm to the case of unlabeled data.},
    issn = {0899-7667},
    doi = {10.1162/089976601750264965},
    url = {https://doi.org/10.1162/089976601750264965},
    eprint = {https://direct.mit.edu/neco/article-pdf/13/7/1443/814849/089976601750264965.pdf},
}

@INPROCEEDINGS{liu2008isolation,
  author={Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
  booktitle={2008 Eighth IEEE International Conference on Data Mining}, 
  title={Isolation Forest}, 
  year={2008},
  volume={},
  number={},
  pages={413-422},
  keywords={Application software;Credit cards;Detectors;Constraint optimization;Data mining;Information technology;Laboratories;Isolation technology;Performance evaluation;Astronomy;anomaly detection;outlier detection;novelty detection;isolation forest;binary trees;model based},
  doi={10.1109/ICDM.2008.17}}


@article{ruffUnifyingReviewDeep2021,
	title = {A {Unifying} {Review} of {Deep} and {Shallow} {Anomaly} {Detection}},
	volume = {109},
	issn = {0018-9219, 1558-2256},
	url = {http://arxiv.org/abs/2009.11732},
	doi = {10.1109/JPROC.2021.3052449},
	abstract = {Deep learning approaches to anomaly detection have recently improved the state of the art in detection performance on complex datasets such as large collections of images or text. These results have sparked a renewed interest in the anomaly detection problem and led to the introduction of a great variety of new methods. With the emergence of numerous such methods, including approaches based on generative models, one-class classification, and reconstruction, there is a growing need to bring methods of this field into a systematic and unified perspective. In this review we aim to identify the common underlying principles as well as the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic 'shallow' and novel deep approaches and show how this relation might cross-fertilize or extend both directions. We further provide an empirical assessment of major existing methods that is enriched by the use of recent explainability techniques, and present specific worked-through examples together with practical advice. Finally, we outline critical open challenges and identify specific paths for future research in anomaly detection.},
	number = {5},
	urldate = {2025-03-23},
	journal = {Proceedings of the IEEE},
	author = {Ruff, Lukas and Kauffmann, Jacob R. and Vandermeulen, Robert A. and Montavon, Grégoire and Samek, Wojciech and Kloft, Marius and Dietterich, Thomas G. and Müller, Klaus-Robert},
	month = may,
	year = {2021},
	note = {arXiv:2009.11732 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {756--795},
}

@article{sajithaDeepLearningApproach2024,
	title = {A deep learning approach to detect diseases in pomegranate fruits via hybrid optimal attention capsule network},
	volume = {84},
	issn = {1574-9541},
	url = {https://www.sciencedirect.com/science/article/pii/S1574954124004011},
	doi = {10.1016/j.ecoinf.2024.102859},
	abstract = {In 2022, the production rate of pomegranate is estimated at approximately 4.8 million metric tons. Unfortunately, these fruits are susceptible to many different kinds of diseases caused by bacterial, viral, and fungal infections. Such diseases can have a major negative impact on fruit quality, production, and the profitability of pomegranate cultivation. Nowadays, several machine learning and deep learning methods are used to identify pomegranate fruit diseases automatically and effectively. In post-harvest pomegranate fruit disease detection, deep learning has great potential to extract complex patterns and features from large datasets. This can improve disease identification accuracy, enabling more efficient disease control, lower crop losses, and better resource management. The proposed work introduces an intelligent deep learning-based approach for accurately detecting pomegranate diseases, begins with Improved Guided Image Filtering (Improved GIF) and resizing to pre-process fruit images, followed by feature extraction (shape, color, texture) using GLCM and GLRLM to streamline classification. Extracted features are then fed into a novel Hybrid Optimal Attention Capsule Network (Hybrid OACapsNet), which classifies the images as normal or diseased, conditions such as bacterial blight, heart rot, and scab. Our analysis indicates that the proposed classifier has a classification accuracy of 99.19 \%, precision of 98.45 \%, recall of 98.41 \%, F1-score of 98.43 \%, and specificity of 99.45 \% compared to other techniques. So this approach offers a framework, which is a feasible solution for automated detection of diseases in fruits, thereby benefiting farmers and supporting their farming operations.},
	urldate = {2025-03-23},
	journal = {Ecological Informatics},
	author = {Sajitha, P. and Diana Andrushia, A. and Anand, N. and Naser, M. Z. and Lubloy, Eva},
	month = dec,
	year = {2024},
	keywords = {Deep learning, Fruit disease detection, Hybrid OACapsNet, Pomegranate, Post-harvest technique},
	pages = {102859},
}

@article{mohantyUsingDeepLearning2016,
	title = {Using {Deep} {Learning} for {Image}-{Based} {Plant} {Disease} {Detection}},
	volume = {7},
	issn = {1664-462X},
	url = {https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2016.01419/full},
	doi = {10.3389/fpls.2016.01419},
	abstract = {{\textless}p{\textgreater}Crop diseases are a major threat to food security, but their rapid identification remains difficult in many parts of the world due to the lack of the necessary infrastructure. The combination of increasing global smartphone penetration and recent advances in computer vision made possible by deep learning has paved the way for smartphone-assisted disease diagnosis. Using a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions, we train a deep convolutional neural network to identify 14 crop species and 26 diseases (or absence thereof). The trained model achieves an accuracy of 99.35\% on a held-out test set, demonstrating the feasibility of this approach. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a massive global scale.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-03-23},
	journal = {Frontiers in Plant Science},
	author = {Mohanty, Sharada P. and Hughes, David P. and Salathé, Marcel},
	month = sep,
	year = {2016},
	note = {Publisher: Frontiers},
	keywords = {machine learning, deep learning, Disease diagnosis, Crop diseases, digital epidemiology},
}

@article{ferentinosDeepLearningModels2018,
	title = {Deep learning models for plant disease detection and diagnosis},
	volume = {145},
	issn = {0168-1699},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169917311742},
	doi = {10.1016/j.compag.2018.01.009},
	abstract = {In this paper, convolutional neural network models were developed to perform plant disease detection and diagnosis using simple leaves images of healthy and diseased plants, through deep learning methodologies. Training of the models was performed with the use of an open database of 87,848 images, containing 25 different plants in a set of 58 distinct classes of [plant, disease] combinations, including healthy plants. Several model architectures were trained, with the best performance reaching a 99.53\% success rate in identifying the corresponding [plant, disease] combination (or healthy plant). The significantly high success rate makes the model a very useful advisory or early warning tool, and an approach that could be further expanded to support an integrated plant disease identification system to operate in real cultivation conditions.},
	urldate = {2025-03-23},
	journal = {Computers and Electronics in Agriculture},
	author = {Ferentinos, Konstantinos P.},
	month = feb,
	year = {2018},
	keywords = {Machine learning, Artificial intelligence, Convolutional neural networks, Pattern recognition, Plant disease identification},
	pages = {311--318},
}

@article{barbedoFactorsInfluencingUse2018,
	title = {Factors influencing the use of deep learning for plant disease recognition},
	volume = {172},
	issn = {1537-5110},
	url = {https://www.sciencedirect.com/science/article/pii/S1537511018303027},
	doi = {10.1016/j.biosystemseng.2018.05.013},
	abstract = {Deep learning is quickly becoming one of the most important tools for image classification. This technology is now beginning to be applied to the tasks of plant disease classification and recognition. The positive results that are being obtained using this approach hide some issues that are seldom taken into account in the respective experiments. This article presents an investigation into the main factors that affect the design and effectiveness of deep neural nets applied to plant pathology. An in-depth analysis of the subject, in which advantages and shortcomings are highlighted, should lead to more realistic conclusions on the subject. The arguments used throughout the text are built upon both studies found in the literature and experiments carried out using an image database carefully built to reflect and reproduce many of the conditions expected to be found in practice. This database, which contains almost 50,000 images, is being made freely available for academic purposes.},
	urldate = {2025-03-23},
	journal = {Biosystems Engineering},
	author = {Barbedo, Jayme G. A.},
	month = aug,
	year = {2018},
	keywords = {Image processing, Deep neural nets, Disease classification, Image database, Transfer learning},
	pages = {84--91},
}

@article{martinelliAdvancedMethodsPlant2015,
	title = {Advanced methods of plant disease detection. {A} review},
	volume = {35},
	url = {https://hal.science/hal-01284270},
	doi = {10.1007/s13593-014-0246-1},
	abstract = {Plant diseases are responsible for major economic losses in the agricultural industry worldwide. Monitoring plant health and detecting pathogen early are essential to reduce disease spread and facilitate effective management practices. DNA-based and serological methods now provide essential tools for accurate plant disease diagnosis, in addition to the traditional visual scouting for symptoms. Although DNA-based and serological methods have revolutionized plant disease detection, they are not very reliable at asymptomatic stage, especially in case of pathogen with systemic diffusion. They need at least 1–2 days for sample harvest, processing, and analysis. Here, we describe modern methods based on nucleic acid and protein analysis. Then, we review innovative approaches currently under development. Our main findings are the following: (1) novel sensors based on the analysis of host responses, e.g., differential mobility spectrometer and lateral flow devices, deliver instantaneous results and can effectively detect early infections directly in the field; (2) biosensors based on phage display and biophotonics can also detect instantaneously infections although they can be integrated with other systems; and (3) remote sensing techniques coupled with spectroscopy-based methods allow high spatialization of results, these techniques may be very useful as a rapid preliminary identification of primary infections. We explain how these tools will help plant disease management and complement serological and DNA-based methods. While serological and PCR-based methods are the most available and effective to confirm disease diagnosis, volatile and biophotonic sensors provide instantaneous results and may be used to identify infections at asymptomatic stages. Remote sensing technologies will be extremely helpful to greatly spatialize diagnostic results. These innovative techniques represent unprecedented tools to render agriculture more sustainable and safe, avoiding expensive use of pesticides in crop protection.},
	number = {1},
	urldate = {2025-03-23},
	journal = {Agronomy for Sustainable Development},
	author = {Martinelli, Federico and Scalenghe, Riccardo and Davino, Salvatore and Panno, Stefano and Scuderi, Giuseppe and Ruisi, Paolo and Villa, Paolo and Stroppiana, Daniela and Boschetti, Mirco and Goulart, Luiz R. and Davis, Cristina E. and Dandekar, Abhaya M.},
	year = {2015},
	note = {Publisher: Springer Verlag/EDP Sciences/INRA},
	keywords = {Remote sensing, Spectroscopy, Plant disease, Biophotonics, Commercial kits, DNA-based methods, Immunological assays, Volatile organic compounds},
	pages = {1--25},
}

@article{savaryGlobalBurdenPathogens2019,
	title = {The global burden of pathogens and pests on major food crops},
	volume = {3},
	issn = {2397-334X},
	doi = {10.1038/s41559-018-0793-y},
	abstract = {Crop pathogens and pests reduce the yield and quality of agricultural production. They cause substantial economic losses and reduce food security at household, national and global levels. Quantitative, standardized information on crop losses is difficult to compile and compare across crops, agroecosystems and regions. Here, we report on an expert-based assessment of crop health, and provide numerical estimates of yield losses on an individual pathogen and pest basis for five major crops globally and in food security hotspots. Our results document losses associated with 137 pathogens and pests associated with wheat, rice, maize, potato and soybean worldwide. Our yield loss (range) estimates at a global level and per hotspot for wheat (21.5\% (10.1-28.1\%)), rice (30.0\% (24.6-40.9\%)), maize (22.5\% (19.5-41.1\%)), potato (17.2\% (8.1-21.0\%)) and soybean (21.4\% (11.0-32.4\%)) suggest that the highest losses are associated with food-deficit regions with fast-growing populations, and frequently with emerging or re-emerging pests and diseases. Our assessment highlights differences in impacts among crop pathogens and pests and among food security hotspots. This analysis contributes critical information to prioritize crop health management to improve the sustainability of agroecosystems in delivering services to societies.},
	language = {eng},
	number = {3},
	journal = {Nature Ecology \& Evolution},
	author = {Savary, Serge and Willocquet, Laetitia and Pethybridge, Sarah Jane and Esker, Paul and McRoberts, Neil and Nelson, Andy},
	month = mar,
	year = {2019},
	pmid = {30718852},
	keywords = {Agriculture, Crops, Agricultural, Animals, Climate Change, Food Supply, Host-Pathogen Interactions, Insecta, Mites, Plant Weeds},
	pages = {430--439},
}

@article{sajithaDeepLearningApproach2024a,
	title = {A deep learning approach to detect diseases in pomegranate fruits via hybrid optimal attention capsule network},
	volume = {84},
	issn = {1574-9541},
	url = {https://www.sciencedirect.com/science/article/pii/S1574954124004011},
	doi = {10.1016/j.ecoinf.2024.102859},
	abstract = {In 2022, the production rate of pomegranate is estimated at approximately 4.8 million metric tons. Unfortunately, these fruits are susceptible to many different kinds of diseases caused by bacterial, viral, and fungal infections. Such diseases can have a major negative impact on fruit quality, production, and the profitability of pomegranate cultivation. Nowadays, several machine learning and deep learning methods are used to identify pomegranate fruit diseases automatically and effectively. In post-harvest pomegranate fruit disease detection, deep learning has great potential to extract complex patterns and features from large datasets. This can improve disease identification accuracy, enabling more efficient disease control, lower crop losses, and better resource management. The proposed work introduces an intelligent deep learning-based approach for accurately detecting pomegranate diseases, begins with Improved Guided Image Filtering (Improved GIF) and resizing to pre-process fruit images, followed by feature extraction (shape, color, texture) using GLCM and GLRLM to streamline classification. Extracted features are then fed into a novel Hybrid Optimal Attention Capsule Network (Hybrid OACapsNet), which classifies the images as normal or diseased, conditions such as bacterial blight, heart rot, and scab. Our analysis indicates that the proposed classifier has a classification accuracy of 99.19 \%, precision of 98.45 \%, recall of 98.41 \%, F1-score of 98.43 \%, and specificity of 99.45 \% compared to other techniques. So this approach offers a framework, which is a feasible solution for automated detection of diseases in fruits, thereby benefiting farmers and supporting their farming operations.},
	urldate = {2025-03-23},
	journal = {Ecological Informatics},
	author = {Sajitha, P. and Diana Andrushia, A. and Anand, N. and Naser, M. Z. and Lubloy, Eva},
	month = dec,
	year = {2024},
	keywords = {Deep learning, Fruit disease detection, Hybrid OACapsNet, Pomegranate, Post-harvest technique},
	pages = {102859},
}

@misc{zotero-1464,
	url = {https://pdf.sciencedirectassets.com/273474/1-s2.0-S1574954124X00059/1-s2.0-S1574954124004011/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEIX%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQD%2ByF0jnLEk0ShVLoli4s9AldcT06iPuHBM8qFj0cfjtwIgLW%2F%2FNp9t397jt6H8hS5kBxtbODHHdeYA8yYZy2ZwolMqvAUI3f%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDNUjJMeWp%2FPXgAOLyCqQBWcMZqqOlvNO9PsQWrdcK8lsqoG31Jot1jl3Ze7ddmb9NvvaklQeeDbaMrwkCF9%2BZmYPyr3XWVbd5edLuNjde5EeaxDMRUOUcEZX2xSUQAXtUAfp8WKYZlzWiDkRto0GC612yI5f9FXt57fmLe6LLK%2FiKZWNdhfK1lTqAbxas2uLr22sj5M9bfvZ5FaYMmZ0IwtaL1jLokfQG0hpIPTuLRzHmBhoURaOPfI9RdtzELeBENPOrC06P2IndVSq7KqbF1DxgjdimuNgNUisTBR5a5CtXkfw1dnRqNb8LceojxmKp9bjNKIDzIxu6ZVFSl1x6eF%2Fsnbq4e3%2BPMjRHC2lEp3xy%2B5vZN9n0rCA5%2Biiuc%2F60iGHKlv4qGPMZ84pi4ngD4alImmp%2F8xYb5WSS%2FGnbaG1GcgcFawPR5LXGJdfrWRc6WO9LQuP84WN1OGlvcFwUWKK%2FIS8xQPIwkESDEMYEQJqsIRDjqdAOBe1hI9s9RihqzwLHdU0QhWf0E6QbGtSHE5miN95sVp7OEHYAnfwp%2BIf%2B%2BcuxTAotvXIRjLV7iwEKx6n8F%2BQ4%2BfnddzXFEzaDCrGxfy7NLXWdUV69CYvxCtN2wUI%2F7yNH7vwPcI90aAtiISFFrxn%2BwlDpHpjE41ZalORoJWx6AhEYNdtTj%2FlA0ORklOF4HmDvTJStmXKJSrrytxVzJkbKjE8jcWG6R9tF5KZA2i253cGqN%2BKMDYCSbv8aYMcH3G03Xs37DpqCFswzRo6d8xi%2FncvVIoERrqxK%2FFHbXlpDI6oV5wsCmp0%2BYJWEp1pns5hM57GeoHifG%2Ftehry4w5IKETUfNxS9so2pI5ox%2BfB5WNDf30l2os2XGfBmN%2FUxj54Zi3Ab7YJKYTcMNPVgb8GOrEBdNlrJaodVDo2iSMhiUGufqOnXYqV2XMBUOzZPybMiSZtt8iKWkpQ%2Fx8v3ugBXkO21XSuSxgyNYC03nC6T09UddwBD62lpJWU%2BtOiMQus7bzrJMY4gWRixjM8KZUpRxrFqHmAGcjWDjVDRX%2B7swxL3URJqjWbBGdm3HILY8qt2hNjjxaXUfhBkKgbxUdP010F1W2fWHtq3Ul%2F8FXDSvwPn1GPVxF0wZwtRy9xlVKzdjtr&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250323T211138Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY3T3Y7MGC%2F20250323%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=14c743ad138e9da44a7d502dab6e56386653ec2b43218faa05df686323637671&hash=df35effb639e24a37a09e14a32f5724aa94d054b70eae3791e1070efde6d7f31&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1574954124004011&tid=spdf-5b9f07a9-b7b7-464a-a50d-1cbf2f221732&sid=b0c16a9e643f20470e4954013ab4a852191agxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=13115c5155550b065052&rr=9250ec2368e10e0f&cc=it},
	urldate = {2025-03-23},
}

@article{singhEffectivePlantDisease2024,
	title = {Effective plant disease diagnosis using {Vision} {Transformer} trained with leafy-generative adversarial network-generated images},
	volume = {254},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417424012533},
	doi = {10.1016/j.eswa.2024.124387},
	abstract = {Agriculture, as the foundation of human civilization, is critical to the global economy, providing food for billions. Plant diseases, caused by factors such as bacteria, fungi, viruses, and others, loom large over crop yields, jeopardizing farmers’ livelihoods worldwide. Rapid and accurate identification of these diseases is critical for agricultural productivity protection and to date, several automated plant disease diagnosis methods have been developed by researchers worldwide. However, the issue of having limited labeled datasets for certain plant leaf diseases poses a significant challenge in training classification models effectively. This scarcity often results in class imbalance which adversely affects a model’s ability to accurately predict all the disease classes. It appears there is a need to explore synthetic data generation techniques to train the model for making a better prediction. Further, the disease prediction model should be lightweight so that it can be conveniently integrated with low-end devices with less computational power that farmers can afford to purchase. In this work, we aim to develop an effective neural augmentation model that can render synthetic disease patterns on uninfected leaf images thereby enhancing the leaf disease dataset by adding artificial samples corresponding to those disease classes for which only minor ground truth information is available. Our work extends the state-of-the-art by introducing a new model for leaf disease augmentation, termed “LeafyGAN”, that comprises two key elements: a segmentation model and a disease translation model, both of which are GAN-based. The segmentation model is a pix2pix GAN that is trained to separate foreground leaf images from the background and is trained using a combination of L1 loss and standard GAN loss. The disease translation model is a CycleGAN which is trained using a combination of adversarial loss and cycle consistency loss, which uses the generated segmented mask to render synthetic disease patterns to the extracted leaf regions. A lightweight MobileViT model trained using this augmented data has been seen to perform disease diagnosis with a remarkable accuracy of 99.92\% on the PlantVillage dataset and 75.72\% on the PlantDoc dataset. Notably, our model achieves an accuracy that is comparable with the recent CNN and Transformer-based models with a significantly lesser number of parameters.},
	urldate = {2025-03-23},
	journal = {Expert Systems with Applications},
	author = {Singh, Aadarsh Kumar and Rao, Akhil and Chattopadhyay, Pratik and Maurya, Rahul and Singh, Lokesh},
	month = nov,
	year = {2024},
	keywords = {Disease pattern generation, Generative adversarial networks, Lightweight Vision Transformers, Plant disease diagnosis},
	pages = {124387},
}

@article{vallabhajosyulaNovelHierarchicalFramework2024,
	title = {A novel hierarchical framework for plant leaf disease detection using residual vision transformer},
	volume = {10},
	issn = {2405-8440},
	url = {https://www.sciencedirect.com/science/article/pii/S2405844024059437},
	doi = {10.1016/j.heliyon.2024.e29912},
	abstract = {Early detection of plant leaf diseases accurately and promptly is very crucial for safeguarding agricultural crop productivity and ensuring food security. During their life cycle, plant leaves get diseased because of multiple factors like bacteria, fungi, weather conditions, etc. In this work, the authors propose a model that aids in the early detection of leaf diseases using a novel hierarchical residual vision transformer using improved Vision Transformer and ResNet9 models. The proposed model can extract more meaningful and discriminating details by reducing the number of trainable parameters with a smaller number of computations. The proposed method is evaluated on the Local Crop dataset, Plant Village dataset, and Extended Plant Village Dataset with 13, 38, and 51 different leaf disease classes. The proposed model is trained using the best trail parameters of Improved Vision Transformer and classified the features using ResNet 9. Performance evaluation is carried out on a wide aspects over the aforementioned datasets and results revealed that the proposed model outperforms other models such as InceptionV3, MobileNetV2, and ResNet50.},
	number = {9},
	urldate = {2025-03-23},
	journal = {Heliyon},
	author = {Vallabhajosyula, Sasikala and Sistla, Venkatramaphanikumar and Kolli, Venkata Krishna Kishore},
	month = may,
	year = {2024},
	keywords = {Deep leaning, Inception V3, MobileNetV2, Plant leaf disease detection, Vision transformer},
	pages = {e29912},
}

@misc{katafuchiImagebasedPlantDisease2021,
	title = {Image-based {Plant} {Disease} {Diagnosis} with {Unsupervised} {Anomaly} {Detection} {Based} on {Reconstructability} of {Colors}},
	url = {http://arxiv.org/abs/2011.14306},
	doi = {10.48550/arXiv.2011.14306},
	abstract = {This paper proposes an unsupervised anomaly detection technique for image-based plant disease diagnosis. The construction of large and publicly available datasets containing labeled images of healthy and diseased crop plants led to growing interest in computer vision techniques for automatic plant disease diagnosis. Although supervised image classifiers based on deep learning can be a powerful tool for plant disease diagnosis, they require a huge amount of labeled data. The data mining technique of anomaly detection includes unsupervised approaches that do not require rare samples for training classifiers. We propose an unsupervised anomaly detection technique for image-based plant disease diagnosis that is based on the reconstructability of colors; a deep encoder-decoder network trained to reconstruct the colors of {\textbackslash}textit\{healthy\} plant images should fail to reconstruct colors of symptomatic regions. Our proposed method includes a new image-based framework for plant disease detection that utilizes a conditional adversarial network called pix2pix and a new anomaly score based on CIEDE2000 color difference. Experiments with PlantVillage dataset demonstrated the superiority of our proposed method compared to an existing anomaly detector at identifying diseased crop images in terms of accuracy, interpretability and computational efficiency.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Katafuchi, Ryoya and Tokunaga, Terumasa},
	month = sep,
	year = {2021},
	note = {arXiv:2011.14306 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{bumbacaSupportingScreeningNew2024,
	title = {Supporting {Screening} of {New} {Plant} {Protection} {Products} through a {Multispectral} {Photogrammetric} {Approach} {Integrated} with {AI}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-4395},
	url = {https://www.mdpi.com/2073-4395/14/2/306},
	doi = {10.3390/agronomy14020306},
	abstract = {This work was aimed at developing a prototype system based on multispectral digital photogrammetry to support tests required by international regulations for new Plant Protection Products (PPPs). In particular, the goal was to provide a system addressing the challenges of a new PPP evaluation with a higher degree of objectivity with respect to the current one, which relies on expert evaluations. The system uses Digital Photogrammetry, which is applied to multispectral acquisitions and Artificial Intelligence (AI). The goal of this paper is also to simplify the present screening process, moving it towards more objective and quantitative scores about phytotoxicity. The implementation of an opportunely trained AI model for phytotoxicity prediction aims to convert ordinary human visual observations, which are presently provided with a discrete scale (forbidding a variance analysis), into a continuous variable. The technical design addresses the need for a reduced dataset for training the AI model and relating discrete observations, as usually performed, to some proxy variables derived from the photogrammetric multispectral 3D model. To achieve this task, an appropriate photogrammetric multispectral system was designed. The system operates in multi-nadiral-view mode over a bench within a greenhouse exploiting an active system for lighting providing uniform and diffuse illumination. The whole system is intended to reduce the environmental variability of acquisitions tending to a standard situation. The methodology combines advanced image processing, image radiometric calibration, and machine learning techniques to predict the General Phytotoxicity percentage index (PHYGEN), a crucial measure of phytotoxicity. Results show that the system can generate reliable estimates of PHYGEN, compliant with existing accuracy standards (even from previous PPPs symptom severity models), using limited training datasets. The proposed solution addressing this challenge is the adoption of the Logistic Function with LASSO model regularization that has been shown to overcome the limitations of a small sample size (typical of new PPP trials). Additionally, it provides the estimate of a numerical continuous index (a percentage), which makes it possible to tackle the objectivity problem related to human visual evaluation that is presently based on an ordinal discrete scale. In our opinion, the proposed prototype system could have significant potential in improving the screening process for new PPPs. In fact, it works specifically for new PPPs screening and, despite this, it has an accuracy consistent with the one ordinarily accepted for human visual approaches. Additionally, it provides a higher degree of objectivity and repeatability.},
	language = {en},
	number = {2},
	urldate = {2025-03-23},
	journal = {Agronomy},
	author = {Bumbaca, Samuele and Borgogno-Mondino, Enrico},
	month = feb,
	year = {2024},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {machine learning, computer vision, diagnostic, digitalization, plant protection product},
	pages = {306},
}
