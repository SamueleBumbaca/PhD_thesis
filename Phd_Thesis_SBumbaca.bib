@book{agrestiIntroductionCategoricalData2018,
  title = {An {{Introduction}} to {{Categorical Data Analysis}}},
  author = {Agresti, Alan},
  year = {2018},
  month = nov,
  publisher = {John Wiley \& Sons},
  abstract = {A valuable new edition of a standard reference The use of statistical methods for categorical data has increased dramatically, particularly for applications in the biomedical and social sciences. An Introduction to Categorical Data Analysis, Third Edition summarizes these methods and shows readers how to use them using software. Readers will find a unified generalized linear models approach that connects logistic regression and loglinear models for discrete data with normal regression for continuous data. Adding to the value in the new edition is: {$\bullet$} Illustrations of the use of R software to perform all the analyses in the book {$\bullet$} A new chapter on alternative methods for categorical data, including smoothing and regularization methods (such as the lasso), classification methods such as linear discriminant analysis and classification trees, and cluster analysis {$\bullet$} New sections in many chapters introducing the Bayesian approach for the methods of that chapter {$\bullet$} More than 70 analyses of data sets to illustrate application of the methods, and about 200 exercises, many containing other data sets {$\bullet$} An appendix showing how to use SAS, Stata, and SPSS, and an appendix with short solutions to most odd-numbered exercises Written in an applied, nontechnical style, this book illustrates the methods using a wide variety of real data, including medical clinical trials, environmental questions, drug use by teenagers, horseshoe crab mating, basketball shooting, correlates of happiness, and much more. An Introduction to Categorical Data Analysis, Third Edition is an invaluable tool for statisticians and biostatisticians as well as methodologists in the social and behavioral sciences, medicine and public health, marketing, education, and the biological and agricultural sciences.},
  googlebooks = {ukNxDwAAQBAJ},
  isbn = {978-1-119-40526-9},
  langid = {english},
  keywords = {Mathematics / General,Mathematics / Probability & Statistics / General,Mathematics / Probability & Statistics / Stochastic Processes}
}

@article{agresti_exact_2001,
	title = {Exact inference for categorical data: recent advances and continuing controversies},
	volume = {20},
	copyright = {Copyright © 2001 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	shorttitle = {Exact inference for categorical data},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.738},
	doi = {10.1002/sim.738},
	abstract = {Methods for exact small-sample analyses with categorical data have been increasingly well developed in recent years. A variety of exact methods exist, primarily using the approach that eliminates unknown parameters by conditioning on their sufficient statistics. In addition, a variety of algorithms now exist for implementing the methods. This paper briefly summarizes the exact approaches and describes recent developments. Controversy continues about the appropriateness of some exact methods, primarily relating to their conservative nature because of discreteness. This issue is examined for two simple problems in which discreteness can be severe – interval estimation of a proportion and the odds ratio. In general, adjusted exact methods based on the mid-P-value seem a reasonable way of reducing the severity of this problem. Copyright © 2001 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {17-18},
	urldate = {2025-06-29},
	journal = {Statistics in Medicine},
	author = {Agresti, Alan},
	year = {2001},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.738},
	pages = {2709--2722},
}

@article{alomStateoftheArtSurveyDeep2019,
  title = {A {{State-of-the-Art Survey}} on {{Deep Learning Theory}} and {{Architectures}}},
  author = {Alom, Md Zahangir and Taha, Tarek M. and Yakopcic, Chris and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst Shamima and Hasan, Mahmudul and Van Essen, Brian C. and Awwal, Abdul A. S. and Asari, Vijayan K.},
  year = {2019},
  month = mar,
  journal = {Electronics},
  volume = {8},
  number = {3},
  pages = {292},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2079-9292},
  doi = {10.3390/electronics8030292},
  urldate = {2025-03-30},
  abstract = {In recent years, deep learning has garnered tremendous success in a variety of application domains. This new field of machine learning has been growing rapidly and has been applied to most traditional application domains, as well as some new areas that present more opportunities. Different methods have been proposed based on different categories of learning, including supervised, semi-supervised, and un-supervised learning. Experimental results show state-of-the-art performance using deep learning when compared to traditional machine learning approaches in the fields of image processing, computer vision, speech recognition, machine translation, art, medical imaging, medical information processing, robotics and control, bioinformatics, natural language processing, cybersecurity, and many others. This survey presents a brief survey on the advances that have occurred in the area of Deep Learning (DL), starting with the Deep Neural Network (DNN). The survey goes on to cover Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), including Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). Additionally, we have discussed recent developments, such as advanced variant DL techniques based on these DL approaches. This work considers most of the papers published after 2012 from when the history of deep learning began. Furthermore, DL approaches that have been explored and evaluated in different application domains are also included in this survey. We also included recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys that have been published on DL using neural networks and a survey on Reinforcement Learning (RL). However, those papers have not discussed individual advanced techniques for training large-scale deep learning models and the recently developed method of generative models.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {auto-encoder (AE),convolutional neural network (CNN),deep belief network (DBN),deep learning,deep reinforcement learning (DRL),generative adversarial network (GAN),recurrent neural network (RNN),restricted Boltzmann machine (RBM),transfer learning}
}

@article{araujoMachineLearningApplications2023,
  title = {Machine {{Learning Applications}} in {{Agriculture}}: {{Current Trends}}, {{Challenges}}, and {{Future Perspectives}}},
  shorttitle = {Machine {{Learning Applications}} in {{Agriculture}}},
  author = {Ara{\'u}jo, Sara Oleiro and Peres, Ricardo Silva and Ramalho, Jos{\'e} Cochicho and Lidon, Fernando and Barata, Jos{\'e}},
  year = {2023},
  month = dec,
  journal = {Agronomy},
  volume = {13},
  number = {12},
  pages = {2976},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2073-4395},
  doi = {10.3390/agronomy13122976},
  urldate = {2025-03-30},
  abstract = {Progress in agricultural productivity and sustainability hinges on strategic investments in technological research. Evolving technologies such as the Internet of Things, sensors, robotics, Artificial Intelligence, Machine Learning, Big Data, and Cloud Computing are propelling the agricultural sector towards the transformative Agriculture 4.0 paradigm. The present systematic literature review employs the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology to explore the usage of Machine Learning in agriculture. The study investigates the foremost applications of Machine Learning, including crop, water, soil, and animal management, revealing its important role in revolutionising traditional agricultural practices. Furthermore, it assesses the substantial impacts and outcomes of Machine Learning adoption and highlights some challenges associated with its integration in agricultural systems. This review not only provides valuable insights into the current landscape of Machine Learning applications in agriculture, but it also outlines promising directions for future research and innovation in this rapidly evolving field.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Agriculture 4.0,machine learning,PRISMA,systematic reviews and meta analytics}
}

@article{arnalbarbedoDigitalImageProcessing2013,
  title = {Digital Image Processing Techniques for Detecting, Quantifying and Classifying Plant Diseases},
  author = {Arnal Barbedo, Jayme Garcia},
  year = {2013},
  month = dec,
  journal = {SpringerPlus},
  volume = {2},
  number = {1},
  pages = {660},
  issn = {2193-1801},
  doi = {10.1186/2193-1801-2-660},
  urldate = {2025-03-26},
  abstract = {This paper presents a survey on methods that use digital image processing techniques to detect, quantify and classify plant diseases from digital images in the visible spectrum. Although disease symptoms can manifest in any part of the plant, only methods that explore visible symptoms in leaves and stems were considered. This was done for two main reasons: to limit the length of the paper and because methods dealing with roots, seeds and fruits have some peculiarities that would warrant a specific survey. The selected proposals are divided into three classes according to their objective: detection, severity quantification, and classification. Each of those classes, in turn, are subdivided according to the main technical solution used in the algorithm. This paper is expected to be useful to researchers working both on vegetable pathology and pattern recognition, providing a comprehensive and accessible overview of this important field of research.},
  langid = {english},
  keywords = {Diseased Region,Powdery Mildew,Radial Basis Function,Texture Feature}
}

@article{barbedoAutomaticMethodDetect2014,
  title = {An {{Automatic Method}} to {{Detect}} and {{Measure Leaf Disease Symptoms Using Digital Image Processing}}},
  author = {Barbedo, Jayme Garcia Arnal},
  year = {2014},
  month = dec,
  journal = {Plant Disease},
  volume = {98},
  number = {12},
  pages = {1709--1716},
  publisher = {Scientific Societies},
  issn = {0191-2917},
  doi = {10.1094/PDIS-03-14-0290-RE},
  urldate = {2025-03-26},
  abstract = {A method is presented to detect and quantify leaf symptoms using conventional color digital images. The method was designed to be completely automatic, eliminating the possibility of human error and reducing time taken to measure disease severity. The program is capable of dealing with images containing multiple leaves, further reducing the time taken. Accurate results are possible when the symptoms and leaf veins have similar color and shade characteristics. The algorithm is subject to one constraint: the background must be as close to white or black as possible. Tests showed that the method provided accurate estimates over a wide variety of conditions, being robust to variation in size, shape, and color of leaves; symptoms; and leaf veins. Low rates of false positives and false negatives occurred due to extrinsic factors such as issues with image capture and the use of extreme file compression ratios.}
}

@article{barbedoFactorsInfluencingUse2018,
  title = {Factors Influencing the Use of Deep Learning for Plant Disease Recognition},
  author = {Barbedo, Jayme G. A.},
  year = {2018},
  month = aug,
  journal = {Biosystems Engineering},
  volume = {172},
  pages = {84--91},
  issn = {1537-5110},
  doi = {10.1016/j.biosystemseng.2018.05.013},
  urldate = {2025-03-23},
  abstract = {Deep learning is quickly becoming one of the most important tools for image classification. This technology is now beginning to be applied to the tasks of plant disease classification and recognition. The positive results that are being obtained using this approach hide some issues that are seldom taken into account in the respective experiments. This article presents an investigation into the main factors that affect the design and effectiveness of deep neural nets applied to plant pathology. An in-depth analysis of the subject, in which advantages and shortcomings are highlighted, should lead to more realistic conclusions on the subject. The arguments used throughout the text are built upon both studies found in the literature and experiments carried out using an image database carefully built to reflect and reproduce many of the conditions expected to be found in practice. This database, which contains almost 50,000 images, is being made freely available for academic purposes.},
  keywords = {Deep neural nets,Disease classification,Image database,Image processing,Transfer learning}
}

@article{baySpeededUpRobustFeatures2008,
  title = {Speeded-{{Up Robust Features}} ({{SURF}})},
  author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and Van Gool, Luc},
  year = {2008},
  month = jun,
  journal = {Computer Vision and Image Understanding},
  series = {Similarity {{Matching}} in {{Computer Vision}} and {{Multimedia}}},
  volume = {110},
  number = {3},
  pages = {346--359},
  issn = {1077-3142},
  doi = {10.1016/j.cviu.2007.09.014},
  urldate = {2025-03-29},
  abstract = {This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF's application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF's usefulness in a broad range of topics in computer vision.},
  keywords = {Camera calibration,Feature description,Interest points,Local features,Object recognition}
}

@article{berryResistedRiseRandomisation2015,
  title = {The Resisted Rise of Randomisation in Experimental Design: {{British}} Agricultural Science, c.1910-1930},
  shorttitle = {The Resisted Rise of Randomisation in Experimental Design},
  author = {Berry, Dominic},
  year = {2015},
  month = sep,
  journal = {History and Philosophy of the Life Sciences},
  volume = {37},
  number = {3},
  pages = {242--260},
  issn = {0391-9714},
  doi = {10.1007/s40656-015-0076-8},
  abstract = {The most conspicuous form of agricultural experiment is the field trial, and within the history of such trials, the arrival of the randomised control trial (RCT) is considered revolutionary. Originating with R.A. Fisher within British agricultural science in the 1920s and 1930s, the RCT has since become one of the most prodigiously used experimental techniques throughout the natural and social sciences. Philosophers of science have already scrutinised the epistemological uniqueness of RCTs, undermining their status as the 'gold standard' in experimental design. The present paper introduces a historical case study from the origins of the RCT, uncovering the initially cool reception given to this method by agricultural scientists at the University of Cambridge and the (Cambridge based) National Institute of Agricultural Botany. Rather than giving further attention to the RCT, the paper focuses instead on a competitor method-the half-drill strip-which both predated the RCT and remained in wide use for at least a decade beyond the latter's arrival. In telling this history, John Pickstone's Ways of Knowing is adopted, as the most flexible and productive way to write the history of science, particularly when sciences and scientists have to work across a number of different kinds of place. It is shown that those who resisted the RCT did so in order to preserve epistemic and social goals that randomisation would have otherwise run a tractor through.},
  langid = {english},
  pmid = {26205200},
  keywords = {Agriculture,History 20th Century,Randomized Controlled Trials as Topic,Research Design,United Kingdom}
}

@article{bockPhytopathometryGlossaryTwentyfirst2022,
  title = {A Phytopathometry Glossary for the Twenty-First Century: Towards Consistency and Precision in Intra- and Inter-Disciplinary Dialogues},
  shorttitle = {A Phytopathometry Glossary for the Twenty-First Century},
  author = {Bock, Clive H. and Pethybridge, Sarah J. and Barbedo, Jayme G. A. and Esker, Paul D. and Mahlein, Anne-Katrin and Del Ponte, Emerson M.},
  year = {2022},
  month = feb,
  journal = {Tropical Plant Pathology},
  volume = {47},
  number = {1},
  pages = {14--24},
  issn = {1983-2052},
  doi = {10.1007/s40858-021-00454-0},
  urldate = {2025-03-26},
  abstract = {Phytopathometry can be defined as the branch of plant pathology (phytopathology) that is concerned with estimation or measurement of the amount of plant disease expressed by symptoms of disease or signs of a pathogen on a single or group of specimens. Phytopathometry is critical for many reasons, including analyzing yield loss due to disease, breeding for disease resistance, evaluating and comparing disease control methods, understanding coevolution, and studying disease epidemiology and pathogen ecology. Phytopathometry underpins all activities in plant pathology and extends into related disciplines, such as agronomy, horticulture, and plant breeding. Considering this central role, phytopathometry warrants status as a formally recognized branch of plant pathology. The glossary defines terms and concepts used in phytopathometry based on disease symptoms or visible pathogen structures and includes those terms commonly used in the visual estimation of disease severity and sensor-based methods of disease measurement. Relevant terms from the intersecting disciplines of measurement science, statistics, psychophysics, robotics, and artificial intelligence are also included. In particular, a new, broader definition is proposed for ``disease severity,'' and the terms ``disease measurement'' and ``disease estimate'' are specifically defined. It is hoped that the glossary contributes to a more unified cross-discipline approach to research in, and application of the tools available to phytopathometry.},
  langid = {english},
  keywords = {Accuracy,Disease severity,Estimate,Imaging,Measurement,Plant pathology,Reliability,Sensors,Visual}
}

@article{bockPlantDiseaseSeverity2010,
  title = {Plant {{Disease Severity Estimated Visually}}, by {{Digital Photography}} and {{Image Analysis}}, and by {{Hyperspectral Imaging}}},
  author = {Bock, C. H. and , G. H., Poole and , P. E., Parker and {and Gottwald}, T. R.},
  year = {2010},
  month = mar,
  journal = {Critical Reviews in Plant Sciences},
  volume = {29},
  number = {2},
  pages = {59--107},
  publisher = {Taylor \& Francis},
  issn = {0735-2689},
  doi = {10.1080/07352681003617285},
  urldate = {2025-03-26},
  abstract = {Reliable, precise and accurate estimates of disease severity are important for predicting yield loss, monitoring and forecasting epidemics, for assessing crop germplasm for disease resistance, and for understanding fundamental biological processes including co-evolution. Disease assessments that are inaccurate and/or imprecise might lead to faulty conclusions being drawn from the data, which in turn can lead to incorrect actions being taken in disease management decisions. Plant disease can be quantified in several different ways. This review considers plant disease severity assessment at the scale of individual plant parts or plants, and describes our current understanding of the sources and causes of assessment error, a better understanding of which is required before improvements can be targeted. The review also considers how these can be identified using various statistical tools. Indeed, great strides have been made in the last thirty years in identifying the sources of assessment error inherent to visual rating, and this review highlights ways that assessment errors can be reduced---particularly by training raters or using assessment aids. Lesion number in relation to area infected is known to influence accuracy and precision of visual estimates---the greater the number of lesions for a given area infected results in more overestimation. Furthermore, there is a widespread tendency to overestimate disease severity at low severities ({$<$}10\%). Both interrater and intrarater reliability can be variable, particularly if training or rating aids are not used. During the last eighty years acceptable accuracy and precision of visual disease assessments have often been achieved using disease scales, particularly because of the time they allegedly save, and the ease with which they can be learned, but recent work suggests there can be some disadvantages to their use. This review considers new technologies that offer opportunity to assess disease with greater objectivity (reliability, precision, and accuracy). One of these, visible light photography and digital image analysis has been increasingly used over the last thirty years, as software has become more sophisticated and user-friendly. Indeed, some studies have produced very accurate estimates of disease using image analysis. In contrast, hyperspectral imagery is relatively recent and has not been widely applied in plant pathology. Nonetheless, it offers interesting and potentially discerning opportunities to assess disease. As plant disease assessment becomes better understood, it is against the backdrop of concepts of reliability, precision and accuracy (and agreement) in plant pathology and measurement science. This review briefly describes these concepts in relation to plant disease assessment. Various advantages and disadvantages of the different approaches to disease assessment are described. For each assessment method some future research priorities are identified that would be of value in better understanding the theory of disease assessment, as it applies to improving and fully realizing the potential of image analysis and hyperspectral imagery.},
  keywords = {error,hyperspectral imagery,image analysis,plant disease assessment,remote sensing,variance}
}

@article{bockSpecialIssuePhytopathometry2022,
  title = {A Special Issue on Phytopathometry --- Visual Assessment, Remote Sensing, and Artificial Intelligence in the Twenty-First Century},
  author = {Bock, Clive H. and Barbedo, Jayme G. A. and Mahlein, Anne-Katrin and Del Ponte, Emerson M.},
  year = {2022},
  month = feb,
  journal = {Tropical Plant Pathology},
  volume = {47},
  number = {1},
  pages = {1--4},
  issn = {1983-2052},
  doi = {10.1007/s40858-022-00498-w},
  urldate = {2025-03-26},
  langid = {english}
}

@article{bockVisualEstimatesFully2020,
  title = {From Visual Estimates to Fully Automated Sensor-Based Measurements of Plant Disease Severity: Status and Challenges for Improving Accuracy},
  shorttitle = {From Visual Estimates to Fully Automated Sensor-Based Measurements of Plant Disease Severity},
  author = {Bock, Clive H. and Barbedo, Jayme G. A. and Del Ponte, Emerson M. and Bohnenkamp, David and Mahlein, Anne-Katrin},
  year = {2020},
  month = apr,
  journal = {Phytopathology Research},
  volume = {2},
  number = {1},
  pages = {9},
  issn = {2524-4167},
  doi = {10.1186/s42483-020-00049-8},
  urldate = {2025-03-26},
  abstract = {The severity of plant diseases, traditionally the proportion of the plant tissue exhibiting symptoms, is a key quantitative variable to know for many diseases and is prone to error. Good quality disease severity data should be accurate (close to the true value). Earliest quantification of disease severity was by visual estimates. Sensor-based image analysis including visible spectrum and hyperspectral and multispectral sensors are established technologies that promise to substitute, or complement visual ratings. Indeed, these technologies have measured disease severity accurately under controlled conditions but are yet to demonstrate their full potential for accurate measurement under field conditions. Sensor technology is advancing rapidly, and artificial intelligence may help overcome issues for automating severity measurement under hyper-variable field conditions. The adoption of appropriate scales, training, instruction and aids (standard area diagrams) has contributed to improved accuracy of visual estimates. The apogee of accuracy for visual estimation is likely being approached, and any remaining increases in accuracy are likely to be small. Due to automation and rapidity, sensor-based measurement offers potential advantages compared with visual estimates, but the latter will remain important for years to come. Mobile, automated sensor-based systems will become increasingly common in controlled conditions and, eventually, in the field for measuring plant disease severity for the purpose of research and decision making.},
  langid = {english},
  keywords = {Accuracy,Artificial intelligence,Assessment,Deep learning,Digital technologies,Disease severity,Machine learning,Mobile device,Phenotyping,Precision,Precision agriculture,Sensor}
}

@article{bockVisualEstimatesFully2020a,
  title = {From Visual Estimates to Fully Automated Sensor-Based Measurements of Plant Disease Severity: Status and Challenges for Improving Accuracy},
  shorttitle = {From Visual Estimates to Fully Automated Sensor-Based Measurements of Plant Disease Severity},
  author = {Bock, Clive H. and Barbedo, Jayme G. A. and Del Ponte, Emerson M. and Bohnenkamp, David and Mahlein, Anne-Katrin},
  year = {2020},
  month = apr,
  journal = {Phytopathology Research},
  volume = {2},
  number = {1},
  pages = {9},
  issn = {2524-4167},
  doi = {10.1186/s42483-020-00049-8},
  urldate = {2025-03-26},
  abstract = {The severity of plant diseases, traditionally the proportion of the plant tissue exhibiting symptoms, is a key quantitative variable to know for many diseases and is prone to error. Good quality disease severity data should be accurate (close to the true value). Earliest quantification of disease severity was by visual estimates. Sensor-based image analysis including visible spectrum and hyperspectral and multispectral sensors are established technologies that promise to substitute, or complement visual ratings. Indeed, these technologies have measured disease severity accurately under controlled conditions but are yet to demonstrate their full potential for accurate measurement under field conditions. Sensor technology is advancing rapidly, and artificial intelligence may help overcome issues for automating severity measurement under hyper-variable field conditions. The adoption of appropriate scales, training, instruction and aids (standard area diagrams) has contributed to improved accuracy of visual estimates. The apogee of accuracy for visual estimation is likely being approached, and any remaining increases in accuracy are likely to be small. Due to automation and rapidity, sensor-based measurement offers potential advantages compared with visual estimates, but the latter will remain important for years to come. Mobile, automated sensor-based systems will become increasingly common in controlled conditions and, eventually, in the field for measuring plant disease severity for the purpose of research and decision making.},
  langid = {english},
  keywords = {Accuracy,Artificial intelligence,Assessment,Deep learning,Digital technologies,Disease severity,Machine learning,Mobile device,Phenotyping,Precision,Precision agriculture,Sensor}
}

@article{bockVisualEstimatesFully2020b,
  title = {From Visual Estimates to Fully Automated Sensor-Based Measurements of Plant Disease Severity: Status and Challenges for Improving Accuracy},
  shorttitle = {From Visual Estimates to Fully Automated Sensor-Based Measurements of Plant Disease Severity},
  author = {Bock, Clive H. and Barbedo, Jayme G. A. and Del Ponte, Emerson M. and Bohnenkamp, David and Mahlein, Anne-Katrin},
  year = {2020},
  month = apr,
  journal = {Phytopathology Research},
  volume = {2},
  number = {1},
  pages = {9},
  issn = {2524-4167},
  doi = {10.1186/s42483-020-00049-8},
  urldate = {2025-03-30},
  abstract = {The severity of plant diseases, traditionally the proportion of the plant tissue exhibiting symptoms, is a key quantitative variable to know for many diseases and is prone to error. Good quality disease severity data should be accurate (close to the true value). Earliest quantification of disease severity was by visual estimates. Sensor-based image analysis including visible spectrum and hyperspectral and multispectral sensors are established technologies that promise to substitute, or complement visual ratings. Indeed, these technologies have measured disease severity accurately under controlled conditions but are yet to demonstrate their full potential for accurate measurement under field conditions. Sensor technology is advancing rapidly, and artificial intelligence may help overcome issues for automating severity measurement under hyper-variable field conditions. The adoption of appropriate scales, training, instruction and aids (standard area diagrams) has contributed to improved accuracy of visual estimates. The apogee of accuracy for visual estimation is likely being approached, and any remaining increases in accuracy are likely to be small. Due to automation and rapidity, sensor-based measurement offers potential advantages compared with visual estimates, but the latter will remain important for years to come. Mobile, automated sensor-based systems will become increasingly common in controlled conditions and, eventually, in the field for measuring plant disease severity for the purpose of research and decision making.},
  langid = {english},
  keywords = {Accuracy,Artificial intelligence,Assessment,Deep learning,Digital technologies,Disease severity,Machine learning,Mobile device,Phenotyping,Precision,Precision agriculture,Sensor}
}

@misc{bommasaniOpportunitiesRisksFoundation2022,
  title = {On the {{Opportunities}} and {{Risks}} of {{Foundation Models}}},
  author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and {Fei-Fei}, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and R{\'e}, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tram{\`e}r, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
  year = {2022},
  month = jul,
  number = {arXiv:2108.07258},
  eprint = {2108.07258},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.07258},
  urldate = {2025-03-26},
  abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning}
}

@article{brienMultiphaseExperimentsLeast2011,
  title = {Multiphase {{Experiments}} with at {{Least One Later Laboratory Phase}}. {{I}}. {{Orthogonal Designs}}},
  author = {Brien, C. J. and Harch, B. D. and Correll, R. L. and Bailey, R. A.},
  year = {2011},
  month = sep,
  journal = {Journal of Agricultural, Biological, and Environmental Statistics},
  volume = {16},
  number = {3},
  pages = {422--450},
  issn = {1537-2693},
  doi = {10.1007/s13253-011-0060-z},
  urldate = {2025-03-30},
  abstract = {The paper provides a systematic approach to designing the laboratory phase of a multiphase experiment, taking into account previous phases. General principles are outlined for experiments in which orthogonal designs can be employed. Multiphase experiments occur widely, although their multiphase nature is often not recognized. The need to randomize the material produced from the first phase in the laboratory phase is emphasized. Factor-allocation diagrams are used to depict the randomizations in a design and the use of skeleton analysis-of-variance (ANOVA) tables to evaluate their properties discussed. The methods are illustrated using a scenario and a case study. A basis for categorizing designs is suggested. This article has supplementary material online.},
  langid = {english},
  keywords = {Analysis of variance,Experimental design,Laboratory experiments,Multi-phase experiments,Multiple randomizations,Multitiered experiments,Two-phase experiments}
}

@article{brownCloseRangeCameraCalibration2002,
  title = {Close-{{Range Camera Calibration}}},
  author = {Brown, Duane},
  year = {2002},
  month = dec,
  journal = {Photogramm. Eng.},
  volume = {37}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.14165},
  urldate = {2025-03-26},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@article{bullockDataIntensiveFarmManagement2019,
  title = {The {{Data-Intensive Farm Management Project}}: {{Changing Agronomic Research Through On-Farm Precision Experimentation}}},
  shorttitle = {The {{Data-Intensive Farm Management Project}}},
  author = {Bullock, David S. and Boerngen, Maria and Tao, Haiying and Maxwell, Bruce and Luck, Joe D. and Shiratsuchi, Luciano and Puntel, Laila and Martin, Nicolas F.},
  year = {2019},
  journal = {Agronomy Journal},
  volume = {111},
  number = {6},
  pages = {2736--2746},
  issn = {1435-0645},
  doi = {10.2134/agronj2019.03.0165},
  urldate = {2025-03-29},
  abstract = {The Data-Intensive Farm Management (DIFM) project works with participating farmers, using precision technology to inexpensively design and run randomized agronomic field trials on whole commercial farm fields, to provide data-based, site-specific farm input management guidance, thus providing economic and environmental benefits. This article lays out a conceptual framework used by the multidisciplinary DIFM research team to facilitate collaboration and then presents details of DIFM's procedures for what it calls on-farm precision experimentation (OFPE), which includes field trial design and implementation, data generation, processing, and management, and analysis. It is argued that DIFM's data and the agricultural ``Big Data'' currently being collected with remote and proximal sensors are complementary; that is, more of either increases the value of the other. In 2019, DIFM and affiliates conducted over 120 trials, ranging from 10 to 100 ha in size, on maize, wheat, soybeans, cotton, and barley in eight US states, Argentina, Brazil, and South Africa. The DIFM is developing cyberinfrastructure to ``scale up'' its activities, to permit researchers and crop consultants worldwide to work with farmers to conduct trials, then process and manage the data. In Addition, DIFM is in the early stages of developing a software system for semi-automatic data analytics, and a cloud-based farm management aid, the purpose of which is to facilitate conversations between agronomists and farmers about implementing data-driven input management decisions. The proposed framework allows researchers, agronomists, and farmers to carry out on-farm precision experimentation using novel digital tools. Core Ideas The Data-Intensive Farm Management project's on-farm trials can generate massive amounts varied managed input data. The Data-Intensive Farm Management project's data fill a gap in agricultural ``Big Data,'' to enable data-intensive crop management. The Data-Intensive Farm Management project's protocols support trial design, data processing and analysis. The Data-Intensive Farm Management project can be implemented by researchers, consultants, and farmers in diverse agronomic scenarios.},
  copyright = {{\copyright} 2019 The author(s).},
  langid = {english}
}

@article{bumbacaSupportingScreeningNew2024,
  title = {Supporting {{Screening}} of {{New Plant Protection Products}} through a {{Multispectral Photogrammetric Approach Integrated}} with {{AI}}},
  author = {Bumbaca, Samuele and {Borgogno-Mondino}, Enrico},
  year = {2024},
  month = feb,
  journal = {Agronomy},
  volume = {14},
  number = {2},
  pages = {306},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2073-4395},
  doi = {10.3390/agronomy14020306},
  urldate = {2025-03-23},
  abstract = {This work was aimed at developing a prototype system based on multispectral digital photogrammetry to support tests required by international regulations for new Plant Protection Products (PPPs). In particular, the goal was to provide a system addressing the challenges of a new PPP evaluation with a higher degree of objectivity with respect to the current one, which relies on expert evaluations. The system uses Digital Photogrammetry, which is applied to multispectral acquisitions and Artificial Intelligence (AI). The goal of this paper is also to simplify the present screening process, moving it towards more objective and quantitative scores about phytotoxicity. The implementation of an opportunely trained AI model for phytotoxicity prediction aims to convert ordinary human visual observations, which are presently provided with a discrete scale (forbidding a variance analysis), into a continuous variable. The technical design addresses the need for a reduced dataset for training the AI model and relating discrete observations, as usually performed, to some proxy variables derived from the photogrammetric multispectral 3D model. To achieve this task, an appropriate photogrammetric multispectral system was designed. The system operates in multi-nadiral-view mode over a bench within a greenhouse exploiting an active system for lighting providing uniform and diffuse illumination. The whole system is intended to reduce the environmental variability of acquisitions tending to a standard situation. The methodology combines advanced image processing, image radiometric calibration, and machine learning techniques to predict the General Phytotoxicity percentage index (PHYGEN), a crucial measure of phytotoxicity. Results show that the system can generate reliable estimates of PHYGEN, compliant with existing accuracy standards (even from previous PPPs symptom severity models), using limited training datasets. The proposed solution addressing this challenge is the adoption of the Logistic Function with LASSO model regularization that has been shown to overcome the limitations of a small sample size (typical of new PPP trials). Additionally, it provides the estimate of a numerical continuous index (a percentage), which makes it possible to tackle the objectivity problem related to human visual evaluation that is presently based on an ordinal discrete scale. In our opinion, the proposed prototype system could have significant potential in improving the screening process for new PPPs. In fact, it works specifically for new PPPs screening and, despite this, it has an accuracy consistent with the one ordinarily accepted for human visual approaches. Additionally, it provides a higher degree of objectivity and repeatability.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {computer vision,diagnostic,digitalization,machine learning,plant protection product}
}

@article{caslerFundamentalsExperimentalDesign2015,
  title = {Fundamentals of {{Experimental Design}}: {{Guidelines}} for {{Designing Successful Experiments}}},
  shorttitle = {Fundamentals of {{Experimental Design}}},
  author = {Casler, Michael D.},
  year = {2015},
  journal = {Agronomy Journal},
  volume = {107},
  number = {2},
  pages = {692--705},
  issn = {1435-0645},
  doi = {10.2134/agronj2013.0114},
  urldate = {2025-03-30},
  abstract = {We often think of experimental designs as analogous to recipes in a cookbook. We look for something that we like, something that satisfies our needs, and frequently return to those that have become our long-standing favorites. We can easily become complacent, favoring the tried-and-true designs (or recipes) over those that contain unknown or untried ingredients or those that are too complex for our tastes and skills. Instead, I prefer to think of experimental designs as a creative series of decisions that are meant to solve one or more problems. These problems may be real or imagined---we may have direct evidence of a past or current problem or we may simply want insurance against future potential problems. The most significant manifestation of a ``problem'' or a ``failed'' design is unsatisfactory P values that prevent us from developing inferences about treatment differences. Four basic tenets or pillars of experimental design--- replication, randomization, blocking, and size of experimental units--- can be used creatively, intelligently, and consciously to solve both real and perceived problems in comparative experiments. Because research is expensive, both in terms of grant funds and the emotional costs invested in grant competition and administration, biological experiments should be designed under the mantra ``failure is not an option.'' Guidelines and advice provided in this review are designed to reduce the probability of failure for researchers who are willing to question, evaluate, and possibly modify their decision-making processes.},
  copyright = {{\copyright} 2015 The Authors.},
  langid = {english}
}

@article{castrignanoGeostatisticalApproachModelling2017,
  title = {A Geostatistical Approach for Modelling and Combining Spatial Data with Different Support},
  author = {Castrignan{\`o}, A. and Quarto, R. and Venezia, A. and Buttafuoco, G.},
  year = {2017},
  month = jan,
  journal = {Advances in Animal Biosciences},
  volume = {8},
  number = {2},
  pages = {594--599},
  issn = {2040-4700},
  doi = {10.1017/S2040470017000048},
  urldate = {2025-03-29}
}

@misc{chalapathyDeepLearningAnomaly2019,
  title = {Deep {{Learning}} for {{Anomaly Detection}}: {{A Survey}}},
  shorttitle = {Deep {{Learning}} for {{Anomaly Detection}}},
  author = {Chalapathy, Raghavendra and Chawla, Sanjay},
  year = {2019},
  month = jan,
  number = {arXiv:1901.03407},
  eprint = {1901.03407},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1901.03407},
  urldate = {2025-03-23},
  abstract = {Anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. The aim of this survey is two-fold, firstly we present a structured and comprehensive overview of research methods in deep learning-based anomaly detection. Furthermore, we review the adoption of these methods for anomaly across various application domains and assess their effectiveness. We have grouped state-of-the-art research techniques into different categories based on the underlying assumptions and approach adopted. Within each category we outline the basic anomaly detection technique, along with its variants and present key assumptions, to differentiate between normal and anomalous behavior. For each category, we present we also present the advantages and limitations and discuss the computational complexity of the techniques in real application domains. Finally, we outline open issues in research and challenges faced while adopting these techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{chenSimpleFrameworkContrastive2020,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  year = {2020},
  month = jul,
  number = {arXiv:2002.05709},
  eprint = {2002.05709},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2002.05709},
  urldate = {2025-03-26},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{colominaUnmannedAerialSystems2014,
  title = {Unmanned Aerial Systems for Photogrammetry and Remote Sensing: {{A}} Review},
  shorttitle = {Unmanned Aerial Systems for Photogrammetry and Remote Sensing},
  author = {Colomina, I. and Molina, P.},
  year = {2014},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {92},
  pages = {79--97},
  issn = {09242716},
  doi = {10.1016/j.isprsjprs.2014.02.013},
  urldate = {2025-03-29},
  abstract = {(2014) Colomina, Molina. ISPRS Journal of Photogrammetry and Remote Sensing. We discuss the evolution and state-of-the-art of the use of Unmanned Aerial Systems (UAS) in the field of Photogrammetry...},
  langid = {british}
}

@misc{ControllableDataGeneration,
  title = {Controllable {{Data Generation}} by {{Deep Learning}}: {{A Review}}},
  urldate = {2025-03-26},
  howpublished = {https://arxiv.org/html/2207.09542}
}

@misc{ControllableDataGenerationa,
  title = {Controllable {{Data Generation}} by {{Deep Learning}}: {{A Review}}},
  urldate = {2025-03-26},
  howpublished = {https://arxiv.org/html/2207.09542}
}

@book{cressieStatisticsSpatialData2015,
  title = {Statistics for {{Spatial Data}}},
  author = {Cressie, Noel},
  year = {2015},
  month = mar,
  publisher = {John Wiley \& Sons},
  abstract = {The Wiley Classics Library consists of selected books that have been made more accessible to consumers in an effort to increase global appeal and general circulation. With these new unabridged softcover volumes, Wiley hopes to extend the lives of these works by making them available to future generations of statisticians, mathematicians, and scientists. Spatial statistics --- analyzing spatial data through statistical models --- has proven exceptionally versatile, encompassing problems ranging from the microscopic to the astronomic. However, for the scientist and engineer faced only with scattered and uneven treatments of the subject in the scientific literature, learning how to make practical use of spatial statistics in day-to-day analytical work is very difficult. Designed exclusively for scientists eager to tap into the enormous potential of this analytical tool and upgrade their range of technical skills, Statistics for Spatial Data is a comprehensive, single-source guide to both the theory and applied aspects of spatial statistical methods. The hard-cover edition was hailed by Mathematical Reviews as an "excellent book which will become a basic reference." This paper-back edition of the 1993 edition, is designed to meet the many technological challenges facing the scientist and engineer. Concentrating on the three areas of geostatistical data, lattice data, and point patterns, the book sheds light on the link between data and model, revealing how design, inference, and diagnostics are an outgrowth of that link. It then explores new methods to reveal just how spatial statistical models can be used to solve important problems in a host of areas in science and engineering. Discussion includes:  Exploratory spatial data analysis Spectral theory for stationary processes Spatial scale Simulation methods for spatial processes Spatial bootstrapping Statistical image analysis and remote sensing Computational aspects of model fitting Application of models to disease mapping  Designed to accommodate the practical needs of the professional, it features a unified and common notation for its subject as well as many detailed examples woven into the text, numerous illustrations (including graphs that illuminate the theory discussed) and over 1,000 references. Fully balancing theory with applications, Statistics for Spatial Data, Revised Edition is an exceptionally clear guide on making optimal use of one of the ascendant analytical tools of the decade, one that has begun to capture the imagination of professionals in biology, earth science, civil, electrical, and agricultural engineering, geography, epidemiology, and ecology.},
  googlebooks = {MzN\_BwAAQBAJ},
  isbn = {978-1-119-11518-2},
  langid = {english},
  keywords = {Mathematics / General,Mathematics / Probability & Statistics / General,Mathematics / Probability & Statistics / Stochastic Processes}
}

@misc{DeepTransferLearning,
  title = {Deep Transfer Learning for Image Classification: A Survey},
  shorttitle = {Deep Transfer Learning for Image Classification},
  journal = {ar5iv},
  urldate = {2025-03-26},
  abstract = {Deep neural networks such as convolutional neural networks (CNNs) and transformers have achieved many successes in image classification in recent years. It has been consistently demonstrated that best practice for imag{\dots}},
  howpublished = {https://ar5iv.labs.arxiv.org/html/2205.09904},
  langid = {english}
}

@article{depetrisRPASbasedPhotogrammetrySupport2020,
  title = {{{RPAS-based}} Photogrammetry to Support Tree Stability Assessment: {{Longing}} for Precision Arboriculture},
  shorttitle = {{{RPAS-based}} Photogrammetry to Support Tree Stability Assessment},
  author = {De Petris, Samuele and Sarvia, Filippo and {Borgogno-Mondino}, Enrico},
  year = {2020},
  month = nov,
  journal = {Urban Forestry \& Urban Greening},
  volume = {55},
  pages = {126862},
  issn = {1618-8667},
  doi = {10.1016/j.ufug.2020.126862},
  urldate = {2025-03-29},
  abstract = {Tree stability evaluation is an important issue with great practical implications. In the recent years, tree potential to cause harm has been increasing in consequence of climate change effects, mainly related to windstorms and tree diseases that represent the main tree failure causes. A tree owner has a duty of safety, imposed by civil and penal laws; consequently, he must operate an appropriate tree management to avoid foreseeable injuries or harms. A relevant problem arises when tree monitoring concern wide areas (extensive contexts), like natural park or urban forest; in these situations a variety of management factors have to be taken into account: the spatial size of the monitored areas; the great heterogeneity of trees vegetative conditions; the relevant number of trees; the balance between environmental protection and safe use of the area; the conspicuous cost of controls and technical interventions. With these premises an efficient planning tool is mandatory to manage this complex resource. Geomatics can support these requirements by integrating different techniques like survey, spatialization and modelling of territorial/environmental variables. In this work authors propose a new approach, hereinafter called ``Precision Arboriculture'' (PA), for tree management, fitting extensive contexts requirements. The proposed workflow is mainly based on RPAS photogrammetry technique and is specifically aimed at (i) accurately estimating single tree parameters; (ii) developing a robust algorithm to assess tree stability with the aim of reducing costs by better addressing ground controls through a spatially based management tool. This technology proved to generate estimates of the main dendrometric parameters with accuracies consistent (sometime higher) than the one ordinary required in the arboricultural context. Nevertheless, some ground data are however needed to calibrate models and testing accuracy of estimates. The proposed methodology proved to be able to generate an easy to use tool (Tree Safety Factor map) for better address ground controls aimed at testing tree stability and reducing the correlated hazard. Safety Factor map enhances critical trees addressing mitigation actions like tree removal, pruning, static bracing, limitations of people transit under potential tree fall area. The adoption of a quantitative index permits to better balance costs and benefits in a more objective way, improving economic efficiency of urban forestry and natural park policies. The method is configuring a new approach in arboricultural field involving new technologies, like RPAS photogrammetric survey and skills moving towards a ``Precision Arboriculture'' concept.},
  keywords = {CHM,Single tree parameters,Static integrated assessment,Tree stability index}
}

@misc{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2025-03-26},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{directive_91_414_EEC,
  title = {Council {{Directive}} 91/414/{{EEC}} of 15 {{July}} 1991 Concerning the Placing of Plant Protection Products on the Market},
  author = {{Council of the European Communities}},
  year = {1991},
  volume = {L 230},
  pages = {1--32},
  urldate = {2025-03-12}
}

@misc{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  month = jun,
  number = {arXiv:2010.11929},
  eprint = {2010.11929},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.11929},
  urldate = {2025-03-23},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{EC_Regulation_1107_2009,
  title = {Regulation ({{EC}}) {{No}} 1107/2009 of the {{European Parliament}} and of the {{Council}} of 21 {{October}} 2009 Concerning the Placing of Plant Protection Products on the Market},
  author = {{European Parliament and Council}},
  year = {2009},
  volume = {L 309},
  pages = {1--50},
  urldate = {2025-03-12}
}

@techreport{EPPO_PP1_135,
  title = {{{PP}} 1/135(4) Phytotoxicity Assessment},
  author = {{EPPO}},
  year = {2014},
  institution = {{European and Mediterranean Plant Protection Organization}},
  urldate = {2025-03-12}
}

@techreport{EPPO_PP1_152,
  title = {{{PP}} 1/152 {{Design}} and Analysis of Efficacy Evaluation Trials},
  author = {{EPPO}},
  year = {2012},
  institution = {{European and Mediterranean Plant Protection Organization}},
  urldate = {2025-03-12}
}

@techreport{EPPO_PP1_181,
  title = {{{PP}} 1/181(5) {{Conduct}} and Reporting of Efficacy Evaluation Trials, Including Good Experimental Practice},
  author = {{EPPO}},
  year = {2021},
  institution = {{European and Mediterranean Plant Protection Organization}},
  urldate = {2025-03-12}
}

@techreport{EPPO_PP1_93,
  title = {{{PP}} 1/93(3) Weeds in Cereals},
  author = {{EPPO}},
  year = {2015},
  institution = {{European and Mediterranean Plant Protection Organization}},
  urldate = {2025-03-12}
}

@misc{EURLex1997265,
  title = {Council Directive 97/57/{{EC}} of 22 September 1997, Uniform {{Principles}} for Evaluation and Authorisation of Plant Protection Products},
  author = {{European Commission}},
  year = {1997},
  volume = {L 265},
  pages = {87--109},
  urldate = {2025-03-12}
}

@article{EWRS_score,
  title = {Einheitliche Codierung Der Ph{\"a}nologischen Entwicklungsstadien Mono- Und Dikotyler Pflanzen - Erweiterte {{BBCH-skala}}, Allgemein},
  author = {Bleiholder, H. and {van den Boom}, T. and Langel{\"u}ddeke, P. and Stauss, R.},
  year = {1991},
  journal = {Nachrichtenblatt des Deutschen Pflanzenschutzdienstes},
  volume = {43},
  pages = {265--270}
}

@misc{FacebookresearchDinov22025,
  title = {Facebookresearch/Dinov2},
  year = {2025},
  month = mar,
  urldate = {2025-03-23},
  abstract = {PyTorch code and models for the DINOv2 self-supervised learning method.},
  copyright = {Apache-2.0},
  howpublished = {Meta Research}
}

@article{ferentinosDeepLearningModels2018,
  title = {Deep Learning Models for Plant Disease Detection and Diagnosis},
  author = {Ferentinos, Konstantinos P.},
  year = {2018},
  month = feb,
  journal = {Computers and Electronics in Agriculture},
  volume = {145},
  pages = {311--318},
  issn = {0168-1699},
  doi = {10.1016/j.compag.2018.01.009},
  urldate = {2025-03-23},
  abstract = {In this paper, convolutional neural network models were developed to perform plant disease detection and diagnosis using simple leaves images of healthy and diseased plants, through deep learning methodologies. Training of the models was performed with the use of an open database of 87,848 images, containing 25 different plants in a set of 58 distinct classes of [plant, disease] combinations, including healthy plants. Several model architectures were trained, with the best performance reaching a 99.53\% success rate in identifying the corresponding [plant, disease] combination (or healthy plant). The significantly high success rate makes the model a very useful advisory or early warning tool, and an approach that could be further expanded to support an integrated plant disease identification system to operate in real cultivation conditions.},
  keywords = {Artificial intelligence,Convolutional neural networks,Machine learning,Pattern recognition,Plant disease identification}
}

@incollection{fisherStatisticalMethodsResearch1992,
  title = {Statistical {{Methods}} for {{Research Workers}}},
  booktitle = {Breakthroughs in {{Statistics}}: {{Methodology}} and {{Distribution}}},
  author = {Fisher, R. A.},
  editor = {Kotz, Samuel and Johnson, Norman L.},
  year = {1992},
  pages = {66--70},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-1-4612-4380-9_6},
  urldate = {2025-03-14},
  abstract = {The prime object of this book is to put into the hands of research workers, and especially of biologists, the means of applying statistical tests accurately to numerical data accumulated in their own laboratories or available in the literature.},
  isbn = {978-1-4612-4380-9},
  langid = {english}
}

@incollection{fisherStatisticalMethodsResearch1992a,
  title = {Statistical {{Methods}} for {{Research Workers}}},
  booktitle = {Breakthroughs in {{Statistics}}: {{Methodology}} and {{Distribution}}},
  author = {Fisher, R. A.},
  editor = {Kotz, Samuel and Johnson, Norman L.},
  year = {1992},
  pages = {66--70},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-1-4612-4380-9_6},
  urldate = {2025-03-17},
  abstract = {The prime object of this book is to put into the hands of research workers, and especially of biologists, the means of applying statistical tests accurately to numerical data accumulated in their own laboratories or available in the literature.},
  isbn = {978-1-4612-4380-9},
  langid = {english}
}

@article{furukawaAccurateDenseRobust2010,
  title = {Accurate, {{Dense}}, and {{Robust Multiview Stereopsis}}},
  author = {Furukawa, Yasutaka and Ponce, Jean},
  year = {2010},
  month = aug,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {32},
  number = {8},
  pages = {1362--1376},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2009.161},
  urldate = {2025-03-30},
  abstract = {This paper proposes a novel algorithm for multiview stereopsis that outputs a dense set of small rectangular patches covering the surfaces visible in the images. Stereopsis is implemented as a match, expand, and filter procedure, starting from a sparse set of matched keypoints, and repeatedly expanding these before using visibility constraints to filter away false matches. The keys to the performance of the proposed algorithm are effective techniques for enforcing local photometric consistency and global visibility constraints. Simple but effective methods are also proposed to turn the resulting patch model into a mesh which can be further refined by an algorithm that enforces both photometric consistency and regularization constraints. The proposed approach automatically detects and discards outliers and obstacles and does not require any initialization in the form of a visual hull, a bounding box, or valid depth ranges. We have tested our algorithm on various data sets including objects with fine surface details, deep concavities, and thin structures, outdoor scenes observed from a restricted set of viewpoints, and "crowded" scenes where moving obstacles appear in front of a static structure of interest. A quantitative evaluation on the Middlebury benchmark [1] shows that the proposed method outperforms all others submitted so far for four out of the six data sets.},
  keywords = {3D/stereo scene analysis,Benchmark testing,Buildings,Computer vision,Image motion analysis,Image reconstruction,Layout,Matched filters,modeling and recovery of physical attributes,motion,Motion analysis,Photometry,Robustness,shape.,Solid modeling}
}

@book{gburAnalysisGeneralizedLinear2020,
  title = {Analysis of {{Generalized Linear Mixed Models}} in the {{Agricultural}} and {{Natural Resources Sciences}}},
  author = {Gbur, Edward E. and Stroup, Walter W. and McCarter, Kevin S. and Durham, Susan and Young, Linda J. and Christman, Mary and West, Mark and Kramer, Matthew},
  year = {2020},
  month = jan,
  publisher = {John Wiley \& Sons},
  abstract = {Generalized Linear Mixed Models in the Agricultural and Natural Resources Sciences provides readers with an understanding and appreciation for the design and analysis of mixed models for non-normally distributed data. It is the only publication of its kind directed specifically toward the agricultural and natural resources sciences audience. Readers will especially benefit from the numerous worked examples based on actual experimental data and the discussion of pitfalls associated with incorrect analyses.},
  googlebooks = {BgnMDwAAQBAJ},
  isbn = {978-0-89118-182-8},
  langid = {english},
  keywords = {Science / Life Sciences / Horticulture,Technology & Engineering / Agriculture / Agronomy / Crop Science,Technology & Engineering / Agriculture / General}
}

@book{gburAnalysisGeneralizedLinear2020a,
  title = {Analysis of {{Generalized Linear Mixed Models}} in the {{Agricultural}} and {{Natural Resources Sciences}}},
  author = {Gbur, Edward E. and Stroup, Walter W. and McCarter, Kevin S. and Durham, Susan and Young, Linda J. and Christman, Mary and West, Mark and Kramer, Matthew},
  year = {2020},
  month = jan,
  publisher = {John Wiley \& Sons},
  abstract = {Generalized Linear Mixed Models in the Agricultural and Natural Resources Sciences provides readers with an understanding and appreciation for the design and analysis of mixed models for non-normally distributed data. It is the only publication of its kind directed specifically toward the agricultural and natural resources sciences audience. Readers will especially benefit from the numerous worked examples based on actual experimental data and the discussion of pitfalls associated with incorrect analyses.},
  googlebooks = {BgnMDwAAQBAJ},
  isbn = {978-0-89118-182-8},
  langid = {english},
  keywords = {Science / Life Sciences / Horticulture,Technology & Engineering / Agriculture / Agronomy / Crop Science,Technology & Engineering / Agriculture / General}
}

@misc{GeneralizedLinearModels,
  title = {Generalized Linear Models by {{P}}. {{McCullagh}} {\textbar} {{Open Library}}},
  urldate = {2025-03-26},
  howpublished = {https://openlibrary.org/books/OL1911874M/Generalized\_linear\_models}
}

@article{gilmourAccountingNaturalExtraneous1997,
  title = {Accounting for {{Natural}} and {{Extraneous Variation}} in the {{Analysis}} of {{Field Experiments}}},
  author = {Gilmour, Arthur R. and Cullis, Brian R. and Verbyla, Ar{\=u}nas P.},
  year = {1997},
  journal = {Journal of Agricultural, Biological, and Environmental Statistics},
  volume = {2},
  number = {3},
  eprint = {1400446},
  eprinttype = {jstor},
  pages = {269--293},
  publisher = {[International Biometric Society, Springer]},
  issn = {1085-7117},
  doi = {10.2307/1400446},
  urldate = {2025-03-29},
  abstract = {We identify three major components of spatial variation in plot errors from field experiments and extend the two-dimensional spatial procedures of Cullis and Gleeson (1991) to account for them. The components are nonstationary, large-scale (global) variation across the field, stationary variation within the trial (natural variation or local trend), and extraneous variation that is often induced by experimental procedures and is predominantly aligned with rows and columns. We present a strategy for identifying a model for the plot errors that uses a trellis plot of residuals, a perspective plot of the sample variogram and, where possible, likelihood ratio tests to identify which components are present. We demonstrate the strategy using two illustrative examples. We conclude that although there is no one model that adequately fits all field experiments, the separable autoregressive model is dominant. However, there is often additional identifiable variation present.}
}

@book{gomarascaBasicsGeomatics2009,
  title = {Basics of {{Geomatics}}},
  author = {Gomarasca, Mario A.},
  year = {2009},
  month = sep,
  publisher = {Springer Science \& Business Media},
  abstract = {Geomatics is a neologism, the use of which is becoming increasingly widespread, even if it is not still universally accepted. It includes several disciplines and te- niques for the study of the Earth's surface and its environments, and computer science plays a decisive role. A more meaningful and appropriate expression is G- spatial Information or GeoInformation. Geo-spatial Information embeds topography in its more modern forms (measurements with electronic instrumentation, sophisticated techniques of data analysis and network compensation, global satellite positioning techniques, laser scanning, etc.), analytical and digital photogrammetry, satellite and airborne remote sensing, numerical cartography, geographical information systems, decision support systems, WebGIS, etc. These specialized elds are intimately interrelated in terms of both the basic science and the results pursued: rigid separation does not allow us to discover several common aspects and the fundamental importance assumed in a search for solutions in the complex survey context. The objective pursued by Mario A. Gomarasca, one that is only apparently modest, is to publish an integrated text on the surveying theme, containing simple and comprehensible concepts relevant to experts in Geo-spatial Information and/or speci cally in one of the disciplines that compose it. At the same time, the book is rigorous and synthetic, describing with precision the main instruments and methods connected to the multiple techniques available today.},
  googlebooks = {BAQ3FJiXDGsC},
  isbn = {978-1-4020-9014-1},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / Computer Vision & Pattern Recognition,Nature / Natural Resources,Science / Earth Sciences / Geography,Technology & Engineering / Environmental / General,Technology & Engineering / Remote Sensing & Geographic Information Systems}
}

@book{gomarascaElementiDiGeomatica2004,
  title = {{Elementi di geomatica: con elementi di geodesia e cartografia, fotogrammetria, telerilevamento, informatica, sistemi di ripresa, sistemi di posizionamento satellitare, elaborazione digitale delle immagini, sistemi informativi territoriali, sistemi di supporto alle decisioni, SIT in rete, INSPIRE e GMES, dizionario tecnico, acronimi}},
  shorttitle = {{Elementi di geomatica}},
  author = {Gomarasca, Mario A.},
  year = {2004},
  publisher = {Associazione italiana di rilevamento},
  googlebooks = {FesfAQAAIAAJ},
  isbn = {978-88-900943-7-8},
  langid = {italian},
  keywords = {Science / Earth Sciences / Geography}
}

@book{goodfellowDeepLearning2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {MIT Press}
}

@book{goovaertsGeostatisticsNaturalResources1997,
  title = {Geostatistics for {{Natural Resources Evaluation}}},
  author = {Goovaerts, Pierre},
  year = {1997},
  publisher = {Oxford University Press},
  abstract = {This text fulfills a need for an advanced-level work covering both the theory and application of geostatistics. It covers the most important areas of geostatistical methodology, introducing tools for description, quantitative modeling of spatial continuity, spatial prediction, and assessment of local uncertainty and stochastic simulation. It also details the theoretical background underlying most GSLIB programs. The tools are applied to an environmental data set, but the book includes a general presentation of algorithms intended for students and practitioners in such diverse fields as soil science, mining, petroleum, remote sensing, hydrogeology, and the environmental sciences.},
  isbn = {978-0-19-511538-3},
  langid = {english},
  keywords = {Mathematics / Applied,Science / Earth Sciences / Geology,Science / Earth Sciences / Hydrology,Science / Life Sciences / Botany,Science / Life Sciences / Ecology,Social Science / Statistics,Technology & Engineering / Remote Sensing & Geographic Information Systems}
}

@book{hartleyMultipleViewGeometry2003,
  title = {Multiple {{View Geometry}} in {{Computer Vision}}},
  author = {Hartley, Richard and Zisserman, Andrew},
  year = {2003},
  publisher = {Cambridge University Press},
  abstract = {A basic problem in computer vision is to understand the structure of a real world scene given several images of it. Techniques for solving this problem are taken from projective geometry and photogrammetry. Here, the authors cover the geometric principles and their algebraic representation in terms of camera projection matrices, the fundamental matrix and the trifocal tensor. The theory and methods of computation of these entities are discussed with real examples, as is their use in the reconstruction of scenes from multiple images. The new edition features an extended introduction covering the key ideas in the book (which itself has been updated with additional examples and appendices) and significant new results which have appeared since the first edition. Comprehensive background material is provided, so readers familiar with linear algebra and basic numerical methods can understand the projective geometry and estimation algorithms presented, and implement the algorithms directly from the book.},
  googlebooks = {si3R3Pfa98QC},
  isbn = {978-0-521-54051-3},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / Computer Vision & Pattern Recognition,Computers / Software Development & Engineering / Computer Graphics,Mathematics / Applied,Mathematics / Geometry / General,Technology & Engineering / Robotics}
}

@book{hastieElementsStatisticalLearning2009,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-84858-7},
  urldate = {2025-03-26},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-0-387-84857-0 978-0-387-84858-7},
  keywords = {Averaging,Boosting,classification,clustering,data mining,machine learning,Projection pursuit,Random Forest,supervised learning,Support Vector Machine,unsupervised learning}
}

@misc{He2016deep,
  title = {He2016deep},
  journal = {Bing},
  urldate = {2025-03-26},
  abstract = {Intelligent search from Bing makes it easier to quickly find what you're looking for and rewards you.},
  howpublished = {https://www.bing.com/search?q=he2016deep\&cvid=0410f7e82dca4d7c967613a4434fb341\&gs\_lcrp=EgRlZGdlKgYIABBFGDkyBggAEEUYOTIICAEQ6QcY\_FXSAQczNzlqMGo5qAIIsAIB\&FORM=ANAB01\&PC=U531},
  langid = {english}
}

@misc{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  number = {arXiv:1512.03385},
  eprint = {1512.03385},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1512.03385},
  urldate = {2025-03-23},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{heMomentumContrastUnsupervised2020,
  title = {Momentum {{Contrast}} for {{Unsupervised Visual Representation Learning}}},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  year = {2020},
  month = mar,
  number = {arXiv:1911.05722},
  eprint = {1911.05722},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.05722},
  urldate = {2025-03-26},
  abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{henebryMakingSenseRemotely2011,
  title = {Making {{Sense}} of {{Remotely Sensing Vegetation}}},
  author = {Henebry, Geoffrey M.},
  year = {2011},
  month = jul,
  journal = {BioScience},
  volume = {61},
  number = {7},
  pages = {568--569},
  issn = {0006-3568},
  doi = {10.1525/bio.2011.61.7.13},
  urldate = {2025-03-29},
  abstract = {Remote Sensing of Vegetation: Principles, Techniques, and Applications. Hamlyn G. Jones and Robin A. Vaughan. Oxford University Press, 2010. 400 pp., illus. \$55.00 (ISBN 9780199207794 paper).An ever-expanding constellation of Earth-observing sensors provides us with a virtual tsunami of data, much of it now freely available. But channeling this digital torrent into useful information about the vegetated land surfaces requires a skillful blending of radiation physics, image processing, ecophysiology, and landscape ecology. This can be a complicated endeavor for the uninitiated. Most textbooks on remote sensing aim to survey the technology and applications broadly. Moreover, the recent rapid growth of online and open-access journals has increased the heterogeneity of an increasingly global remote-sensing literature.}
}

@book{henglPracticalGuideGeostatistical2007,
  title = {A {{Practical Guide}} to {{Geostatistical Mapping}} of {{Environmental Variables}}},
  author = {Hengl, Tomislav},
  year = {2007},
  publisher = {{European commission. Joint research centre. Institute for environment and sustainability (Ispra, Italie)}},
  abstract = {Geostatistical mapping can be defined as analytical production of maps by using field observations, auxiliary information and a computer program that calculates values at locations of interest. Today, increasingly the heart of a mapping project is, in fact, the computer program that implements some (geo)statistical algorithm to a given point data set. Purpose of this guide is to assist you in producing quality maps by using fully-operational tools, without a need for serious additional investments. It will first introduce you the to the basic principles of geostatistical mapping and regression-kriging, as the key prediction technique, then it will guide you through four software packages: ILWIS GIS, R+gstat, SAGA GIS and Google Earth, which will be used to prepare the data, run analysis and make final layouts. These materials have been used for the five-days advanced training course "Hands-on-geostatistics: merging GIS and spatial statistics", that is regularly organized by the author and collaborators. Visit the course website to obtain a copy of the datasets used in this exercise. [R{\'e}sum{\'e} de l'auteur].},
  googlebooks = {HdNEzQEACAAJ},
  isbn = {978-92-79-06904-8},
  langid = {english}
}

@inproceedings{huangDenselyConnectedConvolutional2017,
  title = {Densely {{Connected Convolutional Networks}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Huang, Gao and Liu, Zhuang and {van der Maaten}, Laurens and Weinberger, Kilian Q.},
  year = {2017},
  pages = {4700--4708},
  urldate = {2025-03-23}
}

@article{hueteOverviewRadiometricBiophysical2002,
  title = {Overview of the Radiometric and Biophysical Performance of the {{MODIS}} Vegetation Indices},
  author = {Huete, A and Didan, K and Miura, T and Rodriguez, E. P and Gao, X and Ferreira, L. G},
  year = {2002},
  month = nov,
  journal = {Remote Sensing of Environment},
  series = {The {{Moderate Resolution Imaging Spectroradiometer}} ({{MODIS}}): A New Generation of {{Land Surface Monitoring}}},
  volume = {83},
  number = {1},
  pages = {195--213},
  issn = {0034-4257},
  doi = {10.1016/S0034-4257(02)00096-2},
  urldate = {2025-03-29},
  abstract = {We evaluated the initial 12 months of vegetation index product availability from the Moderate Resolution Imaging Spectroradiometer (MODIS) on board the Earth Observing System-Terra platform. Two MODIS vegetation indices (VI), the normalized difference vegetation index (NDVI) and enhanced vegetation index (EVI), are produced at 1-km and 500-m resolutions and 16-day compositing periods. This paper presents an initial analysis of the MODIS NDVI and EVI performance from both radiometric and biophysical perspectives. We utilize a combination of site-intensive and regionally extensive approaches to demonstrate the performance and validity of the two indices. Our results showed a good correspondence between airborne-measured, top-of-canopy reflectances and VI values with those from the MODIS sensor at four intensively measured test sites representing semi-arid grass/shrub, savanna, and tropical forest biomes. Simultaneously derived field biophysical measures also demonstrated the scientific utility of the MODIS VI. Multitemporal profiles of the MODIS VIs over numerous biome types in North and South America well represented their seasonal phenologies. Comparisons of the MODIS-NDVI with the NOAA-14, 1-km AVHRR-NDVI temporal profiles showed that the MODIS-based index performed with higher fidelity. The dynamic range of the MODIS VIs are presented and their sensitivities in discriminating vegetation differences are evaluated in sparse and dense vegetation areas. We found the NDVI to asymptotically saturate in high biomass regions such as in the Amazon while the EVI remained sensitive to canopy variations.}
}

@misc{hughesOpenAccessRepository2016,
  title = {An Open Access Repository of Images on Plant Health to Enable the Development of Mobile Disease Diagnostics},
  author = {Hughes, David P. and Salathe, Marcel},
  year = {2016},
  month = apr,
  number = {arXiv:1511.08060},
  eprint = {1511.08060},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1511.08060},
  urldate = {2025-03-23},
  abstract = {Human society needs to increase food production by an estimated 70\% by 2050 to feed an expected population size that is predicted to be over 9 billion people. Currently, infectious diseases reduce the potential yield by an average of 40\% with many farmers in the developing world experiencing yield losses as high as 100\%. The widespread distribution of smartphones among crop growers around the world with an expected 5 billion smartphones by 2020 offers the potential of turning the smartphone into a valuable tool for diverse communities growing food. One potential application is the development of mobile disease diagnostics through machine learning and crowdsourcing. Here we announce the release of over 50,000 expertly curated images on healthy and infected leaves of crops plants through the existing online platform PlantVillage. We describe both the data and the platform. These data are the beginning of an on-going, crowdsourcing effort to enable computer vision approaches to help solve the problem of yield losses in crop plants due to infectious diseases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society}
}

@misc{IPPC,
  title = {International Standards for Phytosanitary Measures (Ispms)},
  author = {{International Plant Protection Convention}},
  year = {2022},
  urldate = {2025-03-12}
}

@article{jinEfficientGeostatisticalAnalysis2021,
  title = {An Efficient Geostatistical Analysis Tool for On-Farm Experiments Targeted at Localised Treatment},
  author = {Jin, Huidong and Shuvo Bakar, K. and Henderson, Brent L. and Bramley, Robert G. V. and Gobbett, David L.},
  year = {2021},
  month = may,
  journal = {Biosystems Engineering},
  volume = {205},
  pages = {121--136},
  issn = {1537-5110},
  doi = {10.1016/j.biosystemseng.2021.02.009},
  urldate = {2025-03-29},
  abstract = {On farm experimentation (OFE) has been a long-standing method for farmers to assess alternative management at scales relevant to their farming practices. Through the use of spatially distributed designs, whether simple strips or other `whole-of-block' trials, OFE can provide information such as which treatment should be recommended at specific locations, and make important contributions to precision agriculture. However, when treatment response data sets become large, such as with tens of thousands of field observations that are readily collected using on-the-go sensors, existing geostatistical systems for analysing such experiments become computationally intensive, if not impossible. To enable farmers, or their consultants, to generate high-resolution treatment response and recommendation maps on their own computers within a reasonable time, we present a fast and adaptive local cokriging tool for non-colocated and non-stationary OFE data. It uses a spatially-varying neighbourhood radius. It has a graphical user interface accessible via QGIS, a free and open source software. The adaptive local cokriging is demonstrated on three OFE examples. It performs indistinguishably from global cokriging on a small data set, but for large data sets, for which global cokriging is impractical, it predicts significantly more accurately than spatial splines or sampling-based cokriging. It outperforms cokriging base on a fixed number of nearest neighbours when this fixed number is not carefully chosen.},
  keywords = {Cokriging,Multivariate analysis tool,On-farm experimentation,Precision agriculture}
}

@book{jonesRemoteSensingVegetation2010,
  title = {Remote {{Sensing}} of {{Vegetation}}: {{Principles}}, {{Techniques}}, and {{Applications}}},
  shorttitle = {Remote {{Sensing}} of {{Vegetation}}},
  author = {Jones, Hamlyn G. and Vaughan, Robin A.},
  year = {2010},
  month = jul,
  publisher = {OUP Oxford},
  abstract = {Remote sensing is becoming an increasingly important tool for agriculturalists, ecologists, and land managers for the study of Earth's agricultural and natural vegetation, and can be applied to further our understanding of key environmental issues, including climate change and ecosystem management. This timely introduction offers an accessible yet rigorous treatment of the basics of remote sensing at all scales, illustrating its practical application to the study of vegetation. Despite a quantitative approach, the advanced mathematics and complex models common in modern remote sensing literature is demystified through clear explanations that emphasise the key underlying principles, and the core physical aspects are explained in the biological context of vegetation and its adaptation to its specific environment. Various techniques and instruments are addressed, making this a valuable source of reference, and the advantages and disadvantages of these are further illustrated through worked examples and case studies. {$\cdot$} Rigorous physical and mathematical principles presented in a way readily understood by those without a strong mathematical background {$\cdot$} Boxes throughout summarize key information and concepts {$\cdot$} The student is directed to carefully chosen further reading articles, allowing them to explore key topics in more detail Online Resource Centre The Online Resource Centre to accompany Remote Sensing of Vegetation features: For Students: {$\cdot$} Links to useful websites For lecturers: {$\cdot$} Figures from the book in electronic format, ready to download},
  googlebooks = {sTmcAQAAQBAJ},
  isbn = {978-0-19-920779-4},
  langid = {english},
  keywords = {Gardening / Vegetables,Nature / Plants / General,Science / Earth Sciences / Geography,Science / Life Sciences / Biology,Science / Life Sciences / Ecology,Science / Space Science / Astronomy,Technology & Engineering / Remote Sensing & Geographic Information Systems}
}

@book{journelMiningGeostatistics2003,
  title = {Mining {{Geostatistics}}},
  author = {Journel, A. G. and Journel, Andre G. and Huijbregts, Ch J.},
  year = {2003},
  publisher = {Blackburn Press},
  abstract = {First published in 1978, this book was the first complete reference work on the subject of mining geostatistics, an attempt to synthesize the practical experience gained by researchers from the Centre de Morphologie Mathematique in France and by mining engineers and geologists all over the world who contributed their ideas. It was designed for students and engineers who wished to apply geostatistics to practical problems occurring in the lifetime of a mine and for this reason was built around typical problems, progressing from the simplest to the most complicated: structural analysis, guiding exploration, estimation of in situ resources and recoverable reserves, numerical models of deposits, simulation of mining and homogenization processes, ore grade control in production. The techniques developed are illustrated by a large number of case studies and, as an aid to the reader, each chapter begins with a summary of the contents and there is a guide to the notation used. "The book is a practical treatise, written by practicing mining engineers and intended for other practicing engineers . the best summary of geostatistical theory as it stands at the present time and as one of the standard reference texts for the next few years." Mining Magazine "This is the book for which so many of us have been waiting: a practical, authoritative and scholarly work on geostatistics applied to mining. It is well written, well illustrated and usable both as a textbook for advanced students of mining geology and as a reference book for professionals.. Altogether a good book, the best available on the subject." C. J. Dixon in IMM Bulletin},
  googlebooks = {Id1GAAAAYAAJ},
  isbn = {978-1-930665-91-0},
  langid = {english},
  keywords = {Science / Earth Sciences / Geology,Technology & Engineering / Mining}
}

@article{kamilarisDeepLearningAgriculture2018,
  title = {Deep Learning in Agriculture: {{A}} Survey},
  shorttitle = {Deep Learning in Agriculture},
  author = {Kamilaris, Andreas and {Prenafeta-Boldu}, Francesc X.},
  year = {2018},
  month = apr,
  journal = {Computers and Electronics in Agriculture},
  volume = {147},
  eprint = {1807.11809},
  primaryclass = {cs},
  pages = {70--90},
  issn = {01681699},
  doi = {10.1016/j.compag.2018.02.016},
  urldate = {2025-03-26},
  abstract = {Deep learning constitutes a recent, modern technique for image processing and data analysis, with promising results and large potential. As deep learning has been successfully applied in various domains, it has recently entered also the domain of agriculture. In this paper, we perform a survey of 40 research efforts that employ deep learning techniques, applied to various agricultural and food production challenges. We examine the particular agricultural problems under study, the specific models and frameworks employed, the sources, nature and pre-processing of data used, and the overall performance achieved according to the metrics used at each work under study. Moreover, we study comparisons of deep learning with other existing popular techniques, in respect to differences in classification or regression performance. Our findings indicate that deep learning provides high accuracy, outperforming existing commonly used image processing techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{katafuchiImagebasedPlantDisease2021,
  title = {Image-Based {{Plant Disease Diagnosis}} with {{Unsupervised Anomaly Detection Based}} on {{Reconstructability}} of {{Colors}}},
  author = {Katafuchi, Ryoya and Tokunaga, Terumasa},
  year = {2021},
  month = sep,
  number = {arXiv:2011.14306},
  eprint = {2011.14306},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.14306},
  urldate = {2025-03-23},
  abstract = {This paper proposes an unsupervised anomaly detection technique for image-based plant disease diagnosis. The construction of large and publicly available datasets containing labeled images of healthy and diseased crop plants led to growing interest in computer vision techniques for automatic plant disease diagnosis. Although supervised image classifiers based on deep learning can be a powerful tool for plant disease diagnosis, they require a huge amount of labeled data. The data mining technique of anomaly detection includes unsupervised approaches that do not require rare samples for training classifiers. We propose an unsupervised anomaly detection technique for image-based plant disease diagnosis that is based on the reconstructability of colors; a deep encoder-decoder network trained to reconstruct the colors of {\textbackslash}textit\{healthy\} plant images should fail to reconstruct colors of symptomatic regions. Our proposed method includes a new image-based framework for plant disease detection that utilizes a conditional adversarial network called pix2pix and a new anomaly score based on CIEDE2000 color difference. Experiments with PlantVillage dataset demonstrated the superiority of our proposed method compared to an existing anomaly detector at identifying diseased crop images in terms of accuracy, interpretability and computational efficiency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{koldasbayevaChallengesDatadrivenGeospatial2024,
  title = {Challenges in Data-Driven Geospatial Modeling for Environmental Research and Practice},
  author = {Koldasbayeva, Diana and Tregubova, Polina and Gasanov, Mikhail and Zaytsev, Alexey and Petrovskaia, Anna and Burnaev, Evgeny},
  year = {2024},
  month = dec,
  journal = {Nature Communications},
  volume = {15},
  number = {1},
  pages = {10700},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-024-55240-8},
  urldate = {2025-03-26},
  abstract = {Machine learning-based geospatial applications offer unique opportunities for environmental monitoring due to domains and scales adaptability and computational efficiency. However, the specificity of environmental data introduces biases in straightforward implementations. We identify a streamlined pipeline to enhance model accuracy, addressing issues like imbalanced data, spatial autocorrelation, prediction errors, and the nuances of model generalization and uncertainty estimation. We examine tools and techniques for overcoming these obstacles and provide insights into future geospatial AI developments. A big picture of the field is completed from advances in data processing in general, including the demands of industry-related solutions relevant to outcomes of applied sciences.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Biogeography,Environmental impact}
}

@misc{kornblithBetterImageNetModels2019,
  title = {Do {{Better ImageNet Models Transfer Better}}?},
  author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
  year = {2019},
  month = jun,
  number = {arXiv:1805.08974},
  eprint = {1805.08974},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1805.08974},
  urldate = {2025-03-26},
  abstract = {Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classification networks on 12 image classification datasets. We find that, when networks are used as fixed feature extractors or fine-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy (\$r = 0.99\$ and \$0.96\$, respectively). In the former setting, we find that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield penultimate layer features that are much worse for transfer learning. Additionally, we find that, on two small fine-grained image classification datasets, pretraining on ImageNet provides minimal benefits, indicating the learned features from ImageNet do not transfer well to fine-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@incollection{kozaAutomatedDesignBoth1996,
  title = {Automated {{Design}} of {{Both}} the {{Topology}} and {{Sizing}} of {{Analog Electrical Circuits Using Genetic Programming}}},
  booktitle = {Artificial {{Intelligence}} in {{Design}} '96},
  author = {Koza, John R. and Bennett, Forrest H. and Andre, David and Keane, Martin A.},
  editor = {Gero, John S. and Sudweeks, Fay},
  year = {1996},
  pages = {151--170},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-009-0279-4_9},
  urldate = {2025-03-26},
  abstract = {This paper describes an automated process for designing analog electrical circuits based on the principles of natural selection, sexual recombination, and developmental biology. The design process starts with the random creation of a large population of program trees composed of circuit-constructing functions. Each program tree specifies the steps by which a fully developed circuit is to be progressively developed from a common embryonic circuit appropriate for the type of circuit that the user wishes to design. The fitness measure is a user-written computer program that may incorporate any calculable characteristic or combination of characteristics of the circuit. The population of program trees is genetically bred over a series of many generations using genetic programming. Genetic programming is driven by a fitness measure and employs genetic operations such as Darwinian reproduction, sexual recombination (crossover), and occasional mutation to create offspring. This automated evolutionary process produces both the topology of the circuit and the numerical values for each component. This paper describes how genetic programming can evolve the circuit for a difficult-to-design low-pass filter.},
  isbn = {978-94-009-0279-4},
  langid = {english}
}

@book{krausPhotogrammetryGeometryImages2007,
  title = {Photogrammetry: {{Geometry}} from {{Images}} and {{Laser Scans}}},
  shorttitle = {Photogrammetry},
  author = {Kraus, Karl},
  year = {2007},
  publisher = {Walter de Gruyter},
  abstract = {This textbook deals with the basics and methods of photogrammetry and laser scanning which are used to determine the form and location of objects, with measurements provided by sensors placed in air planes as well as on terrestrial platforms. Many examples and exercises with solutions are included.  Photogrammetry, Laserscanning.},
  isbn = {978-3-11-019007-6},
  langid = {english},
  keywords = {Photography / Techniques / General,Science / Earth Sciences / Geography,Science / Earth Sciences / Geology,Science / Earth Sciences / Hydrology,Science / Earth Sciences / Mineralogy,Science / General,Science / Physics / Geophysics,Technology & Engineering / Remote Sensing & Geographic Information Systems,Technology & Engineering / Signals & Signal Processing}
}

@article{kumleEstimatingPowerGeneralized2021,
  title = {Estimating Power in (Generalized) Linear Mixed Models: {{An}} Open Introduction and Tutorial in {{R}}},
  shorttitle = {Estimating Power in (Generalized) Linear Mixed Models},
  author = {Kumle, Levi and V{\~o}, Melissa L.-H. and Draschkow, Dejan},
  year = {2021},
  month = dec,
  journal = {Behavior Research Methods},
  volume = {53},
  number = {6},
  pages = {2528--2543},
  issn = {1554-3528},
  doi = {10.3758/s13428-021-01546-0},
  urldate = {2025-03-13},
  abstract = {Abstract                            Mixed-effects models are a powerful tool for modeling fixed and random effects simultaneously, but do not offer a feasible analytic solution for estimating the probability that a test correctly rejects the null hypothesis. Being able to estimate this probability, however, is critical for sample size planning, as power is closely linked to the reliability and replicability of empirical findings. A flexible and very intuitive alternative to               analytic               power solutions are               simulation-based               power analyses. Although various tools for conducting simulation-based power analyses for mixed-effects models are available, there is lack of guidance on how to appropriately use them. In this tutorial, we discuss how to estimate power for mixed-effects models in different use cases: first, how to use models that were fit on available (e.g. published) data to determine sample size; second, how to determine the number of stimuli required for sufficient power; and finally, how to conduct sample size planning without available data. Our examples cover both linear and generalized linear models and we provide code and resources for performing simulation-based power analyses on openly accessible data sets. The present work therefore helps researchers to navigate sound research design when using mixed-effects models, by summarizing resources, collating available knowledge, providing solutions and tools, and applying them to real-world problems in sample sizing planning when sophisticated analysis procedures like mixed-effects models are outlined as inferential procedures.},
  langid = {english}
}

@article{kumleEstimatingPowerGeneralized2021a,
  title = {Estimating Power in (Generalized) Linear Mixed Models: {{An}} Open Introduction and Tutorial in {{R}}},
  shorttitle = {Estimating Power in (Generalized) Linear Mixed Models},
  author = {Kumle, Levi and V{\~o}, Melissa L.-H. and Draschkow, Dejan},
  year = {2021},
  month = dec,
  journal = {Behavior Research Methods},
  volume = {53},
  number = {6},
  pages = {2528--2543},
  issn = {1554-3528},
  doi = {10.3758/s13428-021-01546-0},
  abstract = {Mixed-effects models are a powerful tool for modeling fixed and random effects simultaneously, but do not offer a feasible analytic solution for estimating the probability that a test correctly rejects the null hypothesis. Being able to estimate this probability, however, is critical for sample size planning, as power is closely linked to the reliability and replicability of empirical findings. A flexible and very intuitive alternative to analytic power solutions are simulation-based power analyses. Although various tools for conducting simulation-based power analyses for mixed-effects models are available, there is lack of guidance on how to appropriately use them. In this tutorial, we discuss how to estimate power for mixed-effects models in different use cases: first, how to use models that were fit on available (e.g. published) data to determine sample size; second, how to determine the number of stimuli required for sufficient power; and finally, how to conduct sample size planning without available data. Our examples cover both linear and generalized linear models and we provide code and resources for performing simulation-based power analyses on openly accessible data sets. The present work therefore helps researchers to navigate sound research design when using mixed-effects models, by summarizing resources, collating available knowledge, providing solutions and tools, and applying them to real-world problems in sample sizing planning when sophisticated analysis procedures like mixed-effects models are outlined as inferential procedures.},
  langid = {english},
  pmcid = {PMC8613146},
  pmid = {33954914},
  keywords = {Computer Simulation,Humans,Linear Models,lme4,Mixed models,mixedpower,Power,R,Reproducibility of Results,Sample Size,Simulation}
}

@article{larkOptimizedSpatialSampling2002,
  title = {Optimized Spatial Sampling of Soil for Estimation of the Variogram by Maximum Likelihood},
  author = {Lark, R. M},
  year = {2002},
  month = jan,
  journal = {Geoderma},
  volume = {105},
  number = {1},
  pages = {49--80},
  issn = {0016-7061},
  doi = {10.1016/S0016-7061(01)00092-1},
  urldate = {2025-03-29},
  abstract = {Recent studies have attempted to optimize the configuration of sample sites for estimation of the variogram by the usual method-of-moments. This paper shows that objective functions can readily be defined for estimation by the method of maximum likelihood. In both cases an objective function can only be defined for a specified variogram so some prior knowledge about the spatial variation of the property of interest is necessary. This paper describes the principles of the method, using Spatial Simulated Annealing for optimization, and applies optimized sample designs to simulated data. For practical applications it seems that the most fruitful way of using the technique is for supplementing simple systematic designs that provide an initial estimate of the variogram.},
  keywords = {Geostatistics,Optimization,Soil sampling,Spatial variation}
}

@article{leeEfficientTwodimensionalSmoothing2013,
  title = {Efficient Two-Dimensional Smoothing with {{P}}{$<$}math{$><$}mi Is="true"{$>$}{{P}}{$<$}/Mi{$><$}/Math{$>$}-Spline {{ANOVA}} Mixed Models and Nested Bases},
  author = {Lee, Dae-Jin and Durb{\'a}n, Mar{\'i}a and Eilers, Paul},
  year = {2013},
  month = may,
  journal = {Computational Statistics \& Data Analysis},
  volume = {61},
  pages = {22--37},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2012.11.013},
  urldate = {2025-03-29},
  abstract = {Low-rank smoothing techniques have gained much popularity in non-standard regression modeling. In particular, penalized splines and tensor product smooths are used as flexible tools to study non-parametric relationships among several covariates. The use of standard statistical software facilitates their use for several types of problems and applications. However, when interaction terms are considered in the modeling, and multiple smoothing parameters need to be estimated standard software does not work well when datasets are large or higher-order interactions are included or need to be tested. In this paper, a general approach for constructing and estimating bivariate smooth models for additive and interaction terms using penalized splines is proposed. The formulation is based on the mixed model representation of the smooth-ANOVA model by Lee and Durb{\'a}n (in~press), and several nested models in terms of random effects components are proposed. Each component has a clear interpretation in terms of function shape and model identifiability constraints. The term PS-ANOVA is coined for this type of models. The estimation method is relatively straightforward based on the algorithm by Schall (1991) for generalized linear mixed models. Further, a simplification of the smooth interaction term is used by constructing lower-rank basis (nested basis). Finally, some simulation studies and real data examples are presented to evaluate the new model and the estimation method.},
  keywords = {Mixed models,Penalized splines,Schall's algorithm,Smooth-ANOVA decomposition}
}

@misc{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021},
  month = aug,
  number = {arXiv:2103.14030},
  eprint = {2103.14030},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.14030},
  urldate = {2025-03-23},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with {\textbackslash}textbf\{S\}hifted {\textbackslash}textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{lopezEfficiencyIncompleteBlock1995,
  title = {Efficiency of an {{Incomplete Block Design Based}} on {{Geostatistics}} for {{Tillage Experiments}}},
  author = {L{\'o}pez, Mar{\'i}a V. and Arr{\'u}e, Jos{\'e} L.},
  year = {1995},
  journal = {Soil Science Society of America Journal},
  volume = {59},
  number = {4},
  pages = {1104--1111},
  issn = {1435-0661},
  doi = {10.2136/sssaj1995.03615995005900040023x},
  urldate = {2025-03-30},
  abstract = {Spatial dependence of soil properties often reduces the power of conventional statistical methods to detect treatment differences. Control of adverse effects of soil variability is of special interest in long-term experiments when small and slowly developing treatment effects are generally expected to occur, as in tillage research. The main objective of this study was to evaluate the ability of an incomplete block design based on geostatistical concepts to improve the precision of a conservation tillage experiment conducted at four sites in Arag{\'o}n, northeast Spain. A preliminary geostatistical characterization of plow layer variability at these sites showed that, in most instances, soil water content and silt plus clay content were spatially dependent. Maps of kriged estimates of these properties were used to locate the tillage plots according to the proposed design. Using incomplete blocks of Size 2, the method estimates treatment effects by making short-distance comparisons and ensures spatially balanced treatment contrasts through fixed comparison distances. This design was compared with a complete block design using soil and crop data obtained during the first two growing seasons of the tillage experiment. The results of a total of 1050 analysis of variance comparisons revealed that the incomplete block design was, on average, 24\% more efficient than the complete block design. The use of incomplete blocks reduced the average error mean square by 33\% and increased the number of cases with significant tillage treatment differences by 25\% compared with the use of complete blocks.},
  copyright = {{\copyright} 1995 Soil Science Society of America},
  langid = {english}
}

@article{loweDistinctiveImageFeatures2004,
  title = {Distinctive {{Image Features}} from {{Scale-Invariant Keypoints}}},
  author = {Lowe, David G.},
  year = {2004},
  month = nov,
  journal = {International Journal of Computer Vision},
  volume = {60},
  number = {2},
  pages = {91--110},
  issn = {1573-1405},
  doi = {10.1023/B:VISI.0000029664.99615.94},
  abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.}
}

@book{luhmannCloseRangePhotogrammetry3D2019,
  title = {Close-{{Range Photogrammetry}} and {{3D Imaging}}},
  author = {Luhmann, Thomas and Robson, Stuart and Kyle, Stephen and Boehm, Jan},
  year = {2019},
  month = nov,
  publisher = {De Gruyter},
  doi = {10.1515/9783110607253},
  urldate = {2025-03-29},
  abstract = {This is the third edition of the well-known guide to close-range photogrammetry. It provides a thorough presentation of the methods, mathematics, systems and applications which comprise the subject of close-range photogrammetry, which uses accurate imaging techniques to analyse the three-dimensional shape of a wide range of manufactured and natural objects.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  isbn = {978-3-11-060725-3},
  langid = {english}
}

@article{luInstanceFusionRealtimeInstancelevel2020,
  title = {{{InstanceFusion}}: {{Real-time Instance-level 3D Reconstruction Using}} a {{Single RGBD Camera}}},
  shorttitle = {{{InstanceFusion}}},
  author = {Lu, Feixiang and Peng, Haotian and Wu, Hongyu and Yang, Jun and Yang, Xinhang and Cao, Ruizhi and Zhang, Liangjun and Yang, Ruigang and Zhou, Bin},
  year = {2020},
  journal = {Computer Graphics Forum},
  volume = {39},
  number = {7},
  pages = {433--445},
  issn = {1467-8659},
  doi = {10.1111/cgf.14157},
  urldate = {2025-03-26},
  abstract = {We present InstanceFusion, a robust real-time system to detect, segment, and reconstruct instance-level 3D objects of indoor scenes with a hand-held RGBD camera. It combines the strengths of deep learning and traditional SLAM techniques to produce visually compelling 3D semantic models. The key success comes from our novel segmentation scheme and the efficient instance-level data fusion, which are both implemented on GPU. Specifically, for each incoming RGBD frame, we take the advantages of the RGBD features, the 3D point cloud, and the reconstructed model to perform instance-level segmentation. The corresponding RGBD data along with the instance ID are then fused to the surfel-based models. In order to sufficiently store and update these data, we design and implement a new data structure using the OpenGL Shading Language. Experimental results show that our method advances the state-of-the-art (SOTA) methods in instance segmentation and data fusion by a big margin. In addition, our instance segmentation improves the precision of 3D reconstruction, especially in the loop closure. InstanceFusion system runs 20.5Hz on a consumer-level GPU, which supports a number of augmented reality (AR) applications (e.g., 3D model registration, virtual interaction, AR map) and robot applications (e.g., navigation, manipulation, grasping). To facilitate future research and reproduce our system more easily, the source code, data, and the trained model are released on Github: https://github.com/Fancomi2017/InstanceFusion.},
  copyright = {{\copyright} 2020 The Author(s) Computer Graphics Forum {\copyright} 2020 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {CCS Concepts,Computing methodologies  Scene understanding,Perception,Vision for robotics}
}

@misc{MachineLearningApplications,
  title = {Machine {{Learning Applications}} in {{Agriculture}}: {{Current Trends}}, {{Challenges}}, and {{Future Perspectives}}},
  urldate = {2025-03-26},
  howpublished = {https://www.mdpi.com/2073-4395/13/12/2976}
}

@article{mahleinHyperspectralSensorsImaging2018,
  type = {Journal {{Article}}},
  title = {Hyperspectral {{Sensors}} and {{Imaging Technologies}} in {{Phytopathology}}: {{State}} of the {{Art}}},
  author = {Mahlein, A.-K. and Kuska, M.T. and Behmann, J. and Polder, G. and Walter, A.},
  year = {2018},
  journal = {Annual Review of Phytopathology},
  volume = {56},
  number = {Volume 56, 2018},
  pages = {535--558},
  publisher = {Annual Reviews},
  issn = {1545-2107},
  doi = {10.1146/annurev-phyto-080417-050100},
  abstract = {Plant disease detection represents a tremendous challenge for research and practical applications. Visual assessment by human raters is time-consuming, expensive, and error prone. Disease rating and plant protection need new and innovative techniques to address forthcoming challenges and trends in agricultural production that require more precision than ever before. Within this context, hyperspectral sensors and imaging techniques---intrinsically tied to efficient data analysis approaches---have shown an enormous potential to provide new insights into plant-pathogen interactions and for the detection of plant diseases. This article provides an overview of hyperspectral sensors and imaging technologies for assessing compatible and incompatible plant-pathogen interactions. Within the progress of digital technologies, the vision, which is increasingly discussed in the society and industry, includes smart and intuitive solutions for assessing plant features in plant phenotyping or for making decisions on plant protection measures in the context of precision agriculture.},
  keywords = {noninvasive}
}

@article{mahleinPlantDiseaseDetection2016,
  title = {Plant {{Disease Detection}} by {{Imaging Sensors}} -- {{Parallels}} and {{Specific Demands}} for {{Precision Agriculture}} and {{Plant Phenotyping}}},
  author = {Mahlein, Anne-Katrin},
  year = {2016},
  month = feb,
  journal = {Plant Disease},
  volume = {100},
  number = {2},
  pages = {241--251},
  publisher = {Scientific Societies},
  issn = {0191-2917},
  doi = {10.1094/PDIS-03-15-0340-FE},
  urldate = {2025-03-29},
  abstract = {Early and accurate detection and diagnosis of plant diseases are key factors in plant production and the reduction of both qualitative and quantitative losses in crop yield. Optical techniques, such as RGB imaging, multi- and hyperspectral sensors, thermography, or chlorophyll fluorescence, have proven their potential in automated, objective, and reproducible detection systems for the identification and quantification of plant diseases at early time points in epidemics. Recently, 3D scanning has also been added as an optical analysis that supplies additional information on crop plant vitality. Different platforms from proximal to remote sensing are available for multiscale monitoring of single crop organs or entire fields. Accurate and reliable detection of diseases is facilitated by highly sophisticated and innovative methods of data analysis that lead to new insights derived from sensor data for complex plant-pathogen systems. Nondestructive, sensor-based methods support and expand upon visual and/or molecular approaches to plant disease assessment. The most relevant areas of application of sensor-based analyses are precision agriculture and plant phenotyping.}
}

@article{martinelliAdvancedMethodsPlant2015,
  title = {Advanced Methods of Plant Disease Detection. {{A}} Review},
  author = {Martinelli, Federico and Scalenghe, Riccardo and Davino, Salvatore and Panno, Stefano and Scuderi, Giuseppe and Ruisi, Paolo and Villa, Paolo and Stroppiana, Daniela and Boschetti, Mirco and Goulart, Luiz R. and Davis, Cristina E. and Dandekar, Abhaya M.},
  year = {2015},
  journal = {Agronomy for Sustainable Development},
  volume = {35},
  number = {1},
  pages = {1--25},
  publisher = {Springer Verlag/EDP Sciences/INRA},
  doi = {10.1007/s13593-014-0246-1},
  urldate = {2025-03-23},
  abstract = {Plant diseases are responsible for major economic losses in the agricultural industry worldwide. Monitoring plant health and detecting pathogen early are essential to reduce disease spread and facilitate effective management practices. DNA-based and serological methods now provide essential tools for accurate plant disease diagnosis, in addition to the traditional visual scouting for symptoms. Although DNA-based and serological methods have revolutionized plant disease detection, they are not very reliable at asymptomatic stage, especially in case of pathogen with systemic diffusion. They need at least 1--2 days for sample harvest, processing, and analysis. Here, we describe modern methods based on nucleic acid and protein analysis. Then, we review innovative approaches currently under development. Our main findings are the following: (1) novel sensors based on the analysis of host responses, e.g., differential mobility spectrometer and lateral flow devices, deliver instantaneous results and can effectively detect early infections directly in the field; (2) biosensors based on phage display and biophotonics can also detect instantaneously infections although they can be integrated with other systems; and (3) remote sensing techniques coupled with spectroscopy-based methods allow high spatialization of results, these techniques may be very useful as a rapid preliminary identification of primary infections. We explain how these tools will help plant disease management and complement serological and DNA-based methods. While serological and PCR-based methods are the most available and effective to confirm disease diagnosis, volatile and biophotonic sensors provide instantaneous results and may be used to identify infections at asymptomatic stages. Remote sensing technologies will be extremely helpful to greatly spatialize diagnostic results. These innovative techniques represent unprecedented tools to render agriculture more sustainable and safe, avoiding expensive use of pesticides in crop protection.},
  keywords = {Biophotonics,Commercial kits,DNA-based methods,Immunological assays,Plant disease,Remote sensing,Spectroscopy,Volatile organic compounds}
}

@article{matheronPrinciplesGeostatistics1963,
  title = {Principles of Geostatistics},
  author = {{Matheron}},
  year = {1963},
  month = dec,
  journal = {Economic Geology},
  volume = {58},
  number = {8},
  pages = {1246--1266},
  publisher = {GeoScienceWorld},
  issn = {0013-0109},
  doi = {10.2113/GSECONGEO.58.8.1246},
  urldate = {2025-03-29},
  abstract = {Knowledge of ore grades and ore reserves as well as error estimation of these values, is fundamental for mining engineers and mining geologists. Until now no appropriate scientific approach to those estimation problems has existed: geostatistics, the principles of which are summarized in this paper, constitutes a new science leading to such an approach. The author criticizes classical statistical methods still in use, and shows some of the main results given by geostatistics. Any ore deposit evaluation as well as proper decision of starting mining operations should be preceded by a geostatistical investigation which may avoid economic failures.},
  langid = {english}
}

@book{meadStatisticalPrinciplesDesign2012,
  title = {Statistical {{Principles}} for the {{Design}} of {{Experiments}}: {{Applications}} to {{Real Experiments}}},
  shorttitle = {Statistical {{Principles}} for the {{Design}} of {{Experiments}}},
  author = {Mead, R. and Gilmour, S. G. and Mead, A.},
  year = {2012},
  month = sep,
  publisher = {Cambridge University Press},
  abstract = {This book is about the statistical principles behind the design of effective experiments and focuses on the practical needs of applied statisticians and experimenters engaged in design, implementation and analysis. Emphasising the logical principles of statistical design, rather than mathematical calculation, the authors demonstrate how all available information can be used to extract the clearest answers to many questions. The principles are illustrated with a wide range of examples drawn from real experiments in medicine, industry, agriculture and many experimental disciplines. Numerous exercises are given to help the reader practise techniques and to appreciate the difference that good design can make to an experimental research project. Based on Roger Mead's excellent Design of Experiments, this new edition is thoroughly revised and updated to include modern methods relevant to applications in industry, engineering and modern biology. It also contains seven new chapters on contemporary topics, including restricted randomisation and fractional replication.},
  googlebooks = {Z\_0LBAAAQBAJ},
  isbn = {978-1-139-57664-2},
  langid = {english},
  keywords = {Mathematics / Applied,Mathematics / Probability & Statistics / General,Medical / Research}
}

@book{meerImagingSpectrometryBasic2001,
  title = {Imaging Spectrometry : Basic Principles and Prospective Applications},
  shorttitle = {Imaging Spectrometry},
  author = {van der Meer, F. D. and de Jong, S. M.},
  year = {2001},
  publisher = {Kluwer Academic},
  urldate = {2025-03-29},
  isbn = {978-1-4020-0194-9},
  langid = {english}
}

@incollection{meerImagingSpectrometryBasic2002,
  title = {Imaging {{Spectrometry}}: {{Basic Analytical Techniques}}},
  shorttitle = {Imaging {{Spectrometry}}},
  booktitle = {Imaging {{Spectrometry}}},
  author = {Meer, Freek Van Der and Jong, Steven De and Bakker, Wim},
  editor = {Meer, Freek D. Van Der and Abrams, Michael and Curran, Paul and Dekker, Arnold and Jong, Steven M. De and Schaepman, Michael and Meer, Freek D. Van Der and Jong, Steven M. De},
  year = {2002},
  volume = {4},
  pages = {17--61},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-0-306-47578-8_2},
  urldate = {2025-03-30},
  isbn = {978-1-4020-0194-9 978-0-306-47578-8}
}

@book{mikhailIntroductionModernPhotogrammetry2001,
  title = {Introduction to {{Modern Photogrammetry}}},
  author = {Mikhail, Edward M. and Bethel, James S. and McGlone, J. Chris},
  year = {2001},
  month = mar,
  publisher = {John Wiley \& Sons},
  abstract = {This text is designed to give students a strong grounding in the mathematical basis of photogrammetry while introducing them to related fields, such as remote sensing and digital image processing.Suitable for undergraduate photogrammetry courses typically aimed at junior and senior students, and for graduate-level courses at the Master's level. Excellent reference for those working in related fields.},
  googlebooks = {D4h8EAAAQBAJ},
  isbn = {978-0-471-30924-6},
  langid = {english},
  keywords = {Technology & Engineering / Civil / General,Technology & Engineering / Environmental / General}
}

@article{minasnyEfficiencyVariousApproaches2002,
  title = {The Efficiency of Various Approaches to Obtaining Estimates of Soil Hydraulic Properties},
  author = {Minasny, Budiman and McBratney, Alex B},
  year = {2002},
  month = may,
  journal = {Geoderma},
  volume = {107},
  number = {1},
  pages = {55--70},
  issn = {0016-7061},
  doi = {10.1016/S0016-7061(01)00138-0},
  urldate = {2025-03-29},
  abstract = {A formal analysis was carried out to evaluate the efficiency of the different methods in predicting water retention and hydraulic conductivity. Efficiency can be defined in terms of effort, cost or value of information. The analysis identified the contribution of individual sources of measurement errors to the overall uncertainty. The value of information summarises the quality of the prediction, the cost of information, the application of predicted hydraulic properties, and the effect of spatial variability. For single measurements, the inverse disc-permeameter analysis is economically more efficient than using pedotransfer functions or measuring hydraulic properties in the laboratory. However, given the large amount of spatial variation of soil hydraulic properties it is found that lots of cheap and imprecise measurements, e.g. by hand texturing, are more efficient than a few expensive precise ones.},
  keywords = {Hydraulic conductivity,Inverse problem,Soil-water balance,Uncertainty analysis}
}

@misc{ModelsPretrainedWeights,
  title = {Models and Pre-Trained Weights --- {{Torchvision}} Main Documentation},
  urldate = {2025-03-23},
  howpublished = {https://pytorch.org/vision/master/models.html}
}

@article{mohantyUsingDeepLearning2016,
  title = {Using {{Deep Learning}} for {{Image-Based Plant Disease Detection}}},
  author = {Mohanty, Sharada P. and Hughes, David P. and Salath{\'e}, Marcel},
  year = {2016},
  month = sep,
  journal = {Frontiers in Plant Science},
  volume = {7},
  publisher = {Frontiers},
  issn = {1664-462X},
  doi = {10.3389/fpls.2016.01419},
  urldate = {2025-03-23},
  abstract = {{$<$}p{$>$}Crop diseases are a major threat to food security, but their rapid identification remains difficult in many parts of the world due to the lack of the necessary infrastructure. The combination of increasing global smartphone penetration and recent advances in computer vision made possible by deep learning has paved the way for smartphone-assisted disease diagnosis. Using a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions, we train a deep convolutional neural network to identify 14 crop species and 26 diseases (or absence thereof). The trained model achieves an accuracy of 99.35\% on a held-out test set, demonstrating the feasibility of this approach. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a massive global scale.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Crop diseases,deep learning,digital epidemiology,Disease diagnosis,machine learning}
}

@article{mullerGeostatisticalModellingPython2022,
  title = {Geostatistical Modelling in {{Python}}},
  shorttitle = {{{GSTools}} v1.3},
  author = {M{\"u}ller, Sebastian and Sch{\"u}ler, Lennart and Zech, Alraune and He{\ss}e, Falk},
  year = {2022},
  month = apr,
  journal = {Geoscientific Model Development},
  volume = {15},
  number = {7},
  pages = {3161--3182},
  publisher = {Copernicus GmbH},
  issn = {1991-959X},
  doi = {10.5194/gmd-15-3161-2022},
  urldate = {2025-03-29},
  abstract = {Geostatistics as a subfield of statistics accounts for the spatial correlations encountered in many applications of, for example, earth sciences. Valuable information can be extracted from these correlations, also helping to address the often encountered burden of data scarcity. Despite the value of additional data, the use of geostatistics still falls short of its potential. This problem is often connected to the lack of user-friendly software hampering the use and application of geostatistics. We therefore present GSTools, a Python-based software suite for solving a wide range of geostatistical problems. We chose Python due to its unique balance between usability, flexibility, and efficiency and due to its adoption in the scientific community. GSTools provides methods for generating random fields; it can perform kriging, variogram estimation and much more. We demonstrate its abilities by virtue of a series of example applications detailing their use.},
  langid = {english}
}

@article{nexUAV3DMapping2014,
  title = {{{UAV}} for {{3D}} Mapping Applications : A Review},
  shorttitle = {{{UAV}} for {{3D}} Mapping Applications},
  author = {Nex, F. C. and Remondino, F.},
  year = {2014},
  journal = {Applied geomatics},
  volume = {6},
  number = {1},
  pages = {1--2015},
  publisher = {Springer},
  issn = {1866-9298},
  doi = {10.1007/s12518-013-0120-x},
  urldate = {2025-03-29},
  langid = {english}
}

@book{oliverGeostatisticalApplicationsPrecision2010,
  title = {Geostatistical {{Applications}} for {{Precision Agriculture}}},
  editor = {Oliver, M.A.},
  year = {2010},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-90-481-9133-8},
  urldate = {2025-03-29},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-90-481-9132-1 978-90-481-9133-8},
  langid = {english},
  keywords = {geostatistics,kriging,paper,Precision Agriculture,simulation,Site-specific Management,Variogram}
}

@article{onofriNewMethodAnalysis2010,
  title = {A New Method for the Analysis of Germination and Emergence Data of Weed Species},
  author = {Onofri, A and Gresta, F and Tei, F},
  year = {2010},
  journal = {Weed Research},
  volume = {50},
  number = {3},
  pages = {187--198},
  issn = {1365-3180},
  doi = {10.1111/j.1365-3180.2010.00776.x},
  urldate = {2025-03-30},
  abstract = {Onofri A, Gresta F \& Tei F (2010). A new method for the analysis of germination and emergence data of weed species. Weed Research50, 187--198. Summary Due to their peculiar characteristics, seed germination and emergence assays may pose problems for data analysis, due to non-normal error distribution and serial correlation between the numbers of seeds counted on different dates from the same experimental unit (Petri dish, pot, plot). Furthermore, it is necessary to consider viable seeds that have not germinated/emerged at the end of an experiment (censored observations), as well as late germination/emergence flushes, that relate to genotypic differences within natural occurring seed populations. Traditional methods of data analysis may not be optimal for dealing with these problems. Therefore, survival analysis may represent an appropriate alternative. In this analysis, the time course of germination/emergence is described by using a non-parametric step function (`germination function') and the effect of factors and covariates on `germination functions' is assessed by Accelerated Failure Time regression and expressed in terms of `time ratios'. These parameters measure how a change in the explanatory variables changes (prolongs/shortens) the time to germination of a seed lot. This paper presents four examples of the application of survival analysis on seed germination/emergence studies. Results are discussed and compared with those obtained with more traditional techniques.},
  copyright = {{\copyright} 2010 The Authors. Journal Compilation {\copyright} 2010 European Weed Research Society},
  langid = {english},
  keywords = {accelerated failure time models,emergence,frailty effects,germination,survival analysis,time ratios,weed seeds}
}

@misc{oquabDINOv2LearningRobust2024,
  title = {{{DINOv2}}: {{Learning Robust Visual Features}} without {{Supervision}}},
  shorttitle = {{{DINOv2}}},
  author = {Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and {El-Nouby}, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Herv{\'e} and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
  year = {2024},
  month = feb,
  number = {arXiv:2304.07193},
  eprint = {2304.07193},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.07193},
  urldate = {2025-03-23},
  abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{other_scores,
  title = {Nonlinear Regression Analysis and Its Applications},
  author = {Bates, D. M. and Watts, D. G.},
  year = {1988},
  journal = {Journal of Agricultural, Biological, and Environmental Statistics},
  volume = {1},
  number = {1},
  pages = {120--135}
}

@book{paineAerialPhotographyImage2012,
  title = {Aerial {{Photography}} and {{Image Interpretation}}},
  author = {Paine, David P. and Kiser, James D.},
  year = {2012},
  month = feb,
  publisher = {John Wiley \& Sons},
  abstract = {The new, completely updated edition of the aerial photography classic Extensively revised to address today's technological advances, Aerial Photography and Image Interpretation, Third Edition offers a thorough survey of the technology, techniques, processes, and methods used to create and interpret aerial photographs. The new edition also covers other forms of remote sensing with topics that include the most current information on orthophotography (including digital), soft copy photogrammetry, digital image capture and interpretation, GPS, GIS, small format aerial photography, statistical analysis and thematic mapping errors, and more. A basic introduction is also given to nonphotographic and space-based imaging platforms and sensors, including Landsat, lidar, thermal, and multispectral. This new Third Edition features:  Additional coverage of the specialized camera equipment used in aerial photography A strong focus on aerial photography and image interpretation, allowing for a much more thorough presentation of the techniques, processes, and methods than is possible in the broader remote sensing texts currently available Straightforward, user-friendly writing style Expanded coverage of digital photography Test questions and summaries for quick review at the end of each chapter  Written in a straightforward style supplemented with hundreds of photographs and illustrations, Aerial Photography and Image Interpretation, Third Edition is the most in-depth resource for undergraduate students and professionals in such fields as forestry, geography, environmental science, archaeology, resource management, surveying, civil and environmental engineering, natural resources, and agriculture.},
  googlebooks = {ESZHAAAAQBAJ},
  isbn = {978-1-118-11264-9},
  langid = {english},
  keywords = {Nature / Natural Resources,Technology & Engineering / Civil / General,Technology & Engineering / Environmental / General}
}

@article{pandianjImprovedDeepResidual2022,
  title = {An {{Improved Deep Residual Convolutional Neural Network}} for {{Plant Leaf Disease Detection}}},
  author = {Pandian J, Arun and K, Kanchanadevi and Rajalakshmi, N. R. and G Arulkumaran, null},
  year = {2022},
  journal = {Computational Intelligence and Neuroscience},
  volume = {2022},
  pages = {5102290},
  issn = {1687-5273},
  doi = {10.1155/2022/5102290},
  abstract = {In this research, we proposed a novel deep residual convolutional neural network with 197 layers (ResNet197) for the detection of various plant leaf diseases. Six blocks of layers were used to develop ResNet197. ResNet197 was trained and tested using a combined plant leaf disease image dataset. Scaling, cropping, flipping, padding, rotation, affine transformation, saturation, and hue transformation techniques were used to create the augmentation data of the plant leaf disease image dataset. The dataset consisted of 103 diseased and healthy image classes of 22 plants and 154,500 images of healthy and diseased plant leaves. The evolutionary search technique was used to optimise the layers and hyperparameter values of ResNet197. ResNet197 was trained on the combined plant leaf disease image dataset using a graphics processing unit (GPU) environment for 1000 epochs. It produced a 99.58 percentage average classification accuracy on the test dataset. The experimental results were superior to existing ResNet architectures and recent transfer learning techniques.},
  langid = {english},
  pmcid = {PMC9492343},
  pmid = {36156945},
  keywords = {Image Processing Computer-Assisted,Neural Networks Computer,Plant Diseases,Plant Leaves,Rotation}
}

@article{paroliniEmergenceModernStatistics2015,
  title = {The Emergence of Modern Statistics in Agricultural Science: Analysis of Variance, Experimental Design and the Reshaping of Research at {{Rothamsted Experimental Station}}, 1919-1933},
  shorttitle = {The Emergence of Modern Statistics in Agricultural Science},
  author = {Parolini, Giuditta},
  year = {2015},
  journal = {Journal of the History of Biology},
  volume = {48},
  number = {2},
  pages = {301--335},
  issn = {0022-5010},
  doi = {10.1007/s10739-014-9394-z},
  abstract = {During the twentieth century statistical methods have transformed research in the experimental and social sciences. Qualitative evidence has largely been replaced by quantitative results and the tools of statistical inference have helped foster a new ideal of objectivity in scientific knowledge. The paper will investigate this transformation by considering the genesis of analysis of variance and experimental design, statistical methods nowadays taught in every elementary course of statistics for the experimental and social sciences. These methods were developed by the mathematician and geneticist R. A. Fisher during the 1920s, while he was working at Rothamsted Experimental Station, where agricultural research was in turn reshaped by Fisher's methods. Analysis of variance and experimental design required new practices and instruments in field and laboratory research, and imposed a redistribution of expertise among statisticians, experimental scientists and the farm staff. On the other hand the use of statistical methods in agricultural science called for a systematization of information management and made computing an activity integral to the experimental research done at Rothamsted, permanently integrating the statisticians' tools and expertise into the station research programme. Fisher's statistical methods did not remain confined within agricultural research and by the end of the 1950s they had come to stay in psychology, sociology, education, chemistry, medicine, engineering, economics, quality control, just to mention a few of the disciplines which adopted them.},
  langid = {english},
  pmid = {25311906},
  keywords = {Agriculture,Analysis of Variance,England,History 20th Century,Research Design,Statistics as Topic}
}

@article{paroliniPursuitScienceAgriculture2015,
  title = {In Pursuit of a Science of Agriculture: The Role of Statistics in Field Experiments},
  shorttitle = {In Pursuit of a Science of Agriculture},
  author = {Parolini, Giuditta},
  year = {2015},
  month = sep,
  journal = {History and Philosophy of the Life Sciences},
  volume = {37},
  number = {3},
  pages = {261--281},
  issn = {0391-9714},
  doi = {10.1007/s40656-015-0075-9},
  abstract = {Since the beginning of the twentieth century statistics has reshaped the experimental cultures of agricultural research taking part in the subtle dialectic between the epistemic and the material that is proper to experimental systems. This transformation has become especially relevant in field trials and the paper will examine the British agricultural institution, Rothamsted Experimental Station, where statistical methods nowadays popular in the planning and analysis of field experiments were developed in the 1920s. At Rothamsted statistics promoted randomisation over systematic arrangements, factorisation over one-question trials, and emphasised the importance of the experimental error in assessing field trials. These changes in methodology transformed also the material culture of agricultural science, and a new body, the Field Plots Committee, was created to manage the field research of the agricultural institution. Although successful, the vision of field experimentation proposed by the Rothamsted statisticians was not unproblematic. Experimental scientists closely linked to the farming community questioned it in favour of a field research that could be~more easily understood by farmers. The clash between the two agendas reveals how the role attributed to statistics in field experimentation defined different pursuits of agricultural research, alternately conceived of~as a scientists' science or as~a farmers' science.},
  langid = {english},
  pmid = {26149775},
  keywords = {Agriculture,Biostatistics,History 20th Century,Research Design,United Kingdom}
}

@misc{pattersonCarbonEmissionsLarge2021,
  title = {Carbon {{Emissions}} and {{Large Neural Network Training}}},
  author = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  year = {2021},
  month = apr,
  number = {arXiv:2104.10350},
  eprint = {2104.10350},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.10350},
  urldate = {2025-03-26},
  abstract = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume {$<$}1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary {\textasciitilde}5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be {\textasciitilde}1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be {\textasciitilde}2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to {\textasciitilde}100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning}
}

@misc{pattersonCarbonEmissionsLarge2021a,
  title = {Carbon {{Emissions}} and {{Large Neural Network Training}}},
  author = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  year = {2021},
  month = apr,
  journal = {arXiv e-prints},
  doi = {10.48550/arXiv.2104.10350},
  urldate = {2025-03-26},
  abstract = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume {$<$}1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary {\textasciitilde}5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be {\textasciitilde}1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be {\textasciitilde}2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to {\textasciitilde}100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning},
  annotation = {ADS Bibcode: 2021arXiv210410350P}
}

@misc{pattersonCarbonEmissionsLarge2021b,
  title = {Carbon {{Emissions}} and {{Large Neural Network Training}}},
  author = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  year = {2021},
  month = apr,
  number = {arXiv:2104.10350},
  eprint = {2104.10350},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.10350},
  urldate = {2025-03-26},
  abstract = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume {$<$}1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary {\textasciitilde}5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be {\textasciitilde}1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be {\textasciitilde}2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to {\textasciitilde}100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning}
}

@article{payneNewTraditionalMethods2006,
  title = {New and {{Traditional Methods}} for the {{Analysis}} of {{Unreplicated Experiments}}},
  author = {Payne, Roger W.},
  year = {2006},
  journal = {Crop Science},
  volume = {46},
  number = {6},
  pages = {2476--2481},
  issn = {1435-0653},
  doi = {10.2135/cropsci2006.04.0273},
  urldate = {2025-03-30},
  abstract = {This paper reviews some traditional and more recent methods for analyzing unreplicated experiments. Such experiments have presented a challenge to statisticians throughout their involvement in agricultural research. At Rothamsted this began in 1919, when R.A. Fisher was appointed to analyze the accumulated data from the classical field experiments. Fisher's experiences with the classicals, which had virtually no replication, must have contributed to his inclusion of replication as one of the key features of a well-designed experiment. Nevertheless, Fisher made good use of Rothamsted's data, for example in his study of the influence of rainfall on yields from the Broadbalk. He also devised the randomization test, which can be used to analyze unreplicated data. More recently, Broadbalk has also been used to study climate change and sustainability. Newer developments have been concerned to find alternatives to use, instead of blocking, to take account of the spatial variation within an experiment. The resulting methods for modeling spatial correlations have allowed experimenters to obtain more precise estimates of treatment effects---or to decrease numbers of replicates---and they can also provide reliable analyses of unreplicated treatments.},
  copyright = {{\copyright} Crop Science Society of America},
  langid = {english}
}

@misc{PDFContributionGeostatistics,
  title = {({{PDF}}) {{The Contribution}} of {{Geostatistics}} to {{Precision Agriculture}}},
  journal = {ResearchGate},
  urldate = {2025-03-29},
  abstract = {PDF {\textbar} On Nov 22, 2016, Gabriele Buttafuoco and others published The Contribution of Geostatistics to Precision Agriculture {\textbar} Find, read and cite all the research you need on ResearchGate},
  howpublished = {https://www.researchgate.net/publication/311307357\_The\_Contribution\_of\_Geostatistics\_to\_Precision\_Agriculture},
  langid = {english}
}

@article{piephoWhyRandomizeAgricultural2013,
  title = {Why {{Randomize Agricultural Experiments}}?},
  author = {Piepho, H. P. and M{\"o}hring, J. and Williams, E. R.},
  year = {2013},
  journal = {Journal of Agronomy and Crop Science},
  volume = {199},
  number = {5},
  pages = {374--383},
  issn = {1439-037X},
  doi = {10.1111/jac.12026},
  urldate = {2025-03-30},
  abstract = {This study illustrates the importance of randomization using two hypothetical field trials, one with a marked systematic trend and the other with a more erratic spatial pattern. The insights from these two examples are reinforced by analysis of a uniformity trial and a small simulation study. Results illustrate that both model-based spatial analysis and randomization-based analysis assuming independent errors are valid with full randomization but may be invalidated when randomization is lacking. It is concluded that randomization provides protection against different forms of spatial trend. The examples given in the study serve as a general reminder that agricultural experiments should be randomized whenever possible.},
  langid = {english},
  keywords = {experimental design,field trial,linear model,randomization,statistics,uniformity trial}
}

@article{polderHypeSpectralImaging2021,
  title = {The Hype in Spectral Imaging},
  author = {Polder, Gerrit and Gowen, Aoife},
  year = {2021},
  month = apr,
  journal = {Spectroscopy Europe},
  pages = {12},
  doi = {10.1255/sew.2021.a12}
}

@article{PP1333,
  title = {{{PP}} 1/333 (1) {{Adoption}} of Digital Technology for Data Generation for the Efficacy Evaluation of Plant Protection Products},
  journal = {EPPO Bulletin},
  volume = {n/a},
  number = {n/a},
  issn = {1365-2338},
  doi = {10.1111/epp.13037},
  urldate = {2025-03-17},
  copyright = {{\copyright} 2024 European and Mediterranean Plant Protection Organization.},
  langid = {english}
}

@article{PP13332024,
  title = {{{{\textsc{PP}}}} 1/333 (1) {{Adoption}} of Digital Technology for Data Generation for the Efficacy Evaluation of Plant Protection Products},
  year = {2024},
  month = nov,
  journal = {EPPO Bulletin},
  pages = {epp.13037},
  issn = {0250-8052, 1365-2338},
  doi = {10.1111/epp.13037},
  urldate = {2025-03-12},
  langid = {english}
}

@article{puntelLeveragingDigitalAgriculture2024,
  title = {Leveraging Digital Agriculture for On-Farm Testing of Technologies},
  author = {Puntel, Laila A. and Thompson, Laura J. and Mieno, Taro},
  year = {2024},
  month = mar,
  journal = {Frontiers in Agronomy},
  volume = {6},
  publisher = {Frontiers},
  issn = {2673-3218},
  doi = {10.3389/fagro.2024.1234232},
  urldate = {2025-03-29},
  abstract = {{$<$}p{$>$}The Precision Nitrogen Project (PNP) worked with more than 80 corn and winter wheat producers to inexpensively design and implement randomized, replicated field strip trials on whole commercial farm fields, and to provide site-specific testing of current nitrogen (N) technologies. This article proposes a conceptual framework and detailed procedure to select the N technology to be tested; design and implement field trials; generate, process, and manage field trial data; and automatically analyze, report, and share benefits from precision N technology. The selection of the N technology was farmer-driven to ensure a good fit and to increase the likelihood of future technology adoption. The technology selection method was called the ``N tiered approach'', which consisted of selecting a technology that progressively increases the level of complexity without exceeding the farmer's learning process or farm logistic constraints. The N tools were classified into (1) crop model-based, (2) remote sensing-based, (3) enhanced efficiency fertilizers, and (4) biologicals. Field strip trials comparing producers' traditional management and the selected N technology were combined with site-specific N rate blocks placed in contrasting areas of the fields. Yield data from the N rate blocks was utilized to derive the site-specific optimal N rate. The benefits of current N technologies were quantified by comparing their yield, profit, and N use efficiency (NUE) to growers' traditional management and to the estimated site-specific optimal N rate. Communication of the trial results back to the growers was crucial to ensure the promotion and adoption of these N technologies farm wide. The framework and overall benefits from N technologies was presented and discussed. The proposed framework allowed researchers, agronomists, and farmers to carry out on-farm precision N experimentation using novel technologies to quantify benefits of digital ag technology and promote adoption.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Data driven,Digital technology,Nitrogen,On farm,Trial design}
}

@article{qiModifiedSoilAdjusted1994,
  title = {A Modified Soil Adjusted Vegetation Index},
  author = {Qi, J. and Chehbouni, A. and Huete, A. R. and Kerr, Y. H. and Sorooshian, S.},
  year = {1994},
  month = may,
  journal = {Remote Sensing of Environment},
  volume = {48},
  number = {2},
  pages = {119--126},
  issn = {0034-4257},
  doi = {10.1016/0034-4257(94)90134-1},
  urldate = {2025-03-29},
  abstract = {There is currently a great deal of interest in the quantitative characterization of temporal and spatial vegetation patterns with remotely sensed data for the study of earth system science and global change. Spectral models and indices are being developed to improve vegetation sensitivity by accounting for atmosphere and soil effects. The soil-adjusted vegetation index (SAVI) was developed to minimize soil influences on canopy spectra by incorporating a soil adjustment factor L into the denominator of the normalized difference vegetation index (NDVI) equation. For optimal adjustment of the soil effect, however, the L factor should vary inversely with the amount of vegetation present. A modified SAVI (MSAVI) that replaces the constant L in the SAVI equation with a variable L function is presented in this article. The L function may be derived by induction or by using the product of the NDVI and weighted difference vegetation index (WDVI). Results based on ground and aircraft-measured cotton canopies are presented. The MSAVI is shown to increase the dynamic range of the vegetation signal while further minimizing the soil background influences, resulting in greater vegetation sensitivity as defined by a ``vegetation signal'' to ``soil noise'' ratio.}
}

@misc{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = feb,
  number = {arXiv:2103.00020},
  eprint = {2103.00020},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.00020},
  urldate = {2025-03-26},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{razavianCNNFeaturesOfftheshelf2014,
  title = {{{CNN Features}} Off-the-Shelf: An {{Astounding Baseline}} for {{Recognition}}},
  shorttitle = {{{CNN Features}} Off-the-Shelf},
  author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
  year = {2014},
  month = may,
  number = {arXiv:1403.6382},
  eprint = {1403.6382},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1403.6382},
  urldate = {2025-03-26},
  abstract = {Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the {\textbackslash}overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the {\textbackslash}overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the {\textbackslash}overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or \$L2\$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{richterGeostatisticalModelsAgricultural2012,
  title = {Geostatistical {{Models}} in {{Agricultural Field Experiments}}: {{Investigations Based}} on {{Uniformity Trials}}},
  shorttitle = {Geostatistical {{Models}} in {{Agricultural Field Experiments}}},
  author = {Richter, Christel and Kroschewski, B{\"a}rbel},
  year = {2012},
  journal = {Agronomy Journal},
  volume = {104},
  number = {1},
  pages = {91--105},
  issn = {1435-0645},
  doi = {10.2134/agronj2011.0100},
  urldate = {2025-03-30},
  abstract = {The probability of detecting treatment differences can often be increased by using geostatistical instead of classical statistical models. Geostatistical approaches require the selection of the best fitted model from a set of alternative models. This additional analysis effort could be reduced if the same model shows consistently the best fit for a given field or crop. To prove whether this reduction can be expected for designed on-station trials, we analyzed five uniformity trials conducted on the same field. We studied whether different layouts of randomized complete block designs, the positions on the field, and the randomized plans influenced the model decision and analyzed the precision achieved. For this, the designs were shifted across the field, and 1000 randomized plans were projected onto each position. The model fit was evaluated using the corrected Akaike information criterion (AICC). The ranked AICC values were used for assessing model preference. In the means of all crops, designs, and models, the variation of the ranks depended on an individual decision for the combination of position and randomized plan by 62.6\%. Therefore, the best fitted model was not predictable for a single experiment. As in the classical approach, the proper layout of a trial determines precision and unbiasedness of treatment differences. Randomization and blocking still should be the basic principles of experiment planning; however, their roles have partially changed. The detected bias of the Type I errors, both of the t-test and F test, needs further investigation. Basic findings are also valid for on-farm trials.},
  copyright = {{\copyright} 2012 The Authors.},
  langid = {english}
}

@misc{RobustDeepLearningBasedDetector,
  title = {A {{Robust Deep-Learning-Based Detector}} for {{Real-Time Tomato Plant Diseases}} and {{Pests Recognition}}},
  urldate = {2025-03-23},
  howpublished = {https://www.mdpi.com/1424-8220/17/9/2022}
}

@article{rodriguez-alvarezCorrectingSpatialHeterogeneity2018,
  title = {Correcting for Spatial Heterogeneity in Plant Breeding Experiments with {{P-splines}}},
  author = {{Rodr{\'i}guez-{\'A}lvarez}, Mar{\'i}a Xos{\'e} and Boer, Martin P. and {van Eeuwijk}, Fred A. and Eilers, Paul H. C.},
  year = {2018},
  month = mar,
  journal = {Spatial Statistics},
  volume = {23},
  pages = {52--71},
  issn = {2211-6753},
  doi = {10.1016/j.spasta.2017.10.003},
  urldate = {2025-03-29},
  abstract = {An important aim of the analysis of agricultural field experiments is to obtain good predictions for genotypic performance, by correcting for spatial effects. In practice these corrections turn out to be complicated, since there can be different types of spatial effects; those due to management interventions applied to the field plots and those due to various kinds of erratic spatial trends. This paper explores the use of two-dimensional smooth surfaces to model random spatial variation. We propose the use of anisotropic tensor product P-splines to explicitly model large-scale (global trend) and small-scale (local trend) spatial dependence. On top of this spatial field, effects of genotypes, blocks, replicates, and/or other sources of spatial variation are described by a mixed model in a standard way. Each component in the model is shown to have an effective dimension. They are closely related to variance estimation, and helpful for characterising the importance of model components. An important result of this paper is the formal proof of the relation between several definitions of heritability and the effective dimension associated with the genetic component. The practical value of our approach is illustrated by simulations and analyses of large-scale plant breeding experiments. An R-package, SpATS, is provided.},
  keywords = {Effective dimension,Field trials,Heritability,Linear mixed model,Spatial analysis,Tensor product P-splines}
}

@article{rodriguez-alvarezFastSmoothingParameter2015,
  title = {Fast Smoothing Parameter Separation in Multidimensional Generalized {{P-splines}}: The {{SAP}} Algorithm},
  shorttitle = {Fast Smoothing Parameter Separation in Multidimensional Generalized {{P-splines}}},
  author = {{Rodr{\'i}guez-{\'A}lvarez}, Mar{\'i}a Xos{\'e} and Lee, Dae-Jin and Kneib, Thomas and Durb{\'a}n, Mar{\'i}a and Eilers, Paul},
  year = {2015},
  month = sep,
  journal = {Statistics and Computing},
  volume = {25},
  number = {5},
  pages = {941--957},
  issn = {1573-1375},
  doi = {10.1007/s11222-014-9464-2},
  urldate = {2025-03-29},
  abstract = {A new computational algorithm for estimating the smoothing parameters of a multidimensional penalized spline generalized linear model with anisotropic penalty is presented. This new proposal is based on the mixed model representation of a multidimensional P-spline, in which the smoothing parameter for each covariate is expressed in terms of variance components. On the basis of penalized quasi-likelihood methods, closed-form expressions for the estimates of the variance components are obtained. This formulation leads to an efficient implementation that considerably reduces the computational burden. The proposed algorithm can be seen as a generalization of the algorithm by Schall (1991)---for variance components estimation---to deal with non-standard structures of the covariance matrix of the random effects. The practical performance of the proposed algorithm is evaluated by means of simulations, and comparisons with alternative methods are made on the basis of the mean square error criterion and the computing time. Finally, we illustrate our proposal with the analysis of two real datasets: a two dimensional example of historical records of monthly precipitation data in USA and a three dimensional one of mortality data from respiratory disease according to the age at death, the year of death and the month of death.},
  langid = {english},
  keywords = {Anisotropic penalty,Artificial Intelligence,Mixed models,P-splines,Smoothing,Tensor product}
}

@misc{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  month = may,
  number = {arXiv:1505.04597},
  eprint = {1505.04597},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1505.04597},
  urldate = {2025-03-26},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{rouseMonitoringVegetationSystems1974,
  title = {Monitoring Vegetation Systems in the {{Great Plains}} with {{ERTS}}},
  author = {Rouse, J. W. and Haas, R. H. and Schell, J. A. and Deering, D. W.},
  year = {1974},
  month = jan,
  urldate = {2025-03-29},
  abstract = {The Great Plains Corridor rangeland project utilizes natural vegetation systems as phenological indicators of seasonal development and climatic effects upon regional growth conditions. A method has been developed for quantitative measurement of vegetation conditions over broad regions using ERTS-1 MSS data. Radiance values recorded in ERTS-1 spectral bands 5 and 7, corrected for sun angle, are used to compute a band ratio parameter which is shown to be correlated with aboveground green biomass on rangelands.},
  keywords = {Geophysics},
  annotation = {NTRS Author Affiliations: Texas A\&M Univ.\\
NTRS Report/Patent Number: PAPER-A20\\
NTRS Document ID: 19740022614\\
NTRS Research Center: Legacy CDMS (CDMS)}
}

@inproceedings{rubleeORBEfficientAlternative2011,
  title = {{{ORB}}: {{An}} Efficient Alternative to {{SIFT}} or {{SURF}}},
  shorttitle = {{{ORB}}},
  booktitle = {2011 {{International Conference}} on {{Computer Vision}}},
  author = {Rublee, Ethan and Rabaud, Vincent and Konolige, Kurt and Bradski, Gary},
  year = {2011},
  month = nov,
  pages = {2564--2571},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2011.6126544},
  urldate = {2025-03-29},
  abstract = {Feature matching is at the base of many computer vision problems, such as object recognition or structure from motion. Current methods rely on costly descriptors for detection and matching. In this paper, we propose a very fast binary descriptor based on BRIEF, called ORB, which is rotation invariant and resistant to noise. We demonstrate through experiments how ORB is at two orders of magnitude faster than SIFT, while performing as well in many situations. The efficiency is tested on several real-world applications, including object detection and patch-tracking on a smart phone.},
  keywords = {Boats}
}

@article{ruffUnifyingReviewDeep2021,
  title = {A {{Unifying Review}} of {{Deep}} and {{Shallow Anomaly Detection}}},
  author = {Ruff, Lukas and Kauffmann, Jacob R. and Vandermeulen, Robert A. and Montavon, Gr{\'e}goire and Samek, Wojciech and Kloft, Marius and Dietterich, Thomas G. and M{\"u}ller, Klaus-Robert},
  year = {2021},
  month = may,
  journal = {Proceedings of the IEEE},
  volume = {109},
  number = {5},
  eprint = {2009.11732},
  primaryclass = {cs},
  pages = {756--795},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2021.3052449},
  urldate = {2025-03-23},
  abstract = {Deep learning approaches to anomaly detection have recently improved the state of the art in detection performance on complex datasets such as large collections of images or text. These results have sparked a renewed interest in the anomaly detection problem and led to the introduction of a great variety of new methods. With the emergence of numerous such methods, including approaches based on generative models, one-class classification, and reconstruction, there is a growing need to bring methods of this field into a systematic and unified perspective. In this review we aim to identify the common underlying principles as well as the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic 'shallow' and novel deep approaches and show how this relation might cross-fertilize or extend both directions. We further provide an empirical assessment of major existing methods that is enriched by the use of recent explainability techniques, and present specific worked-through examples together with practical advice. Finally, we outline critical open challenges and identify specific paths for future research in anomaly detection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{sajithaDeepLearningApproach2024,
  title = {A Deep Learning Approach to Detect Diseases in Pomegranate Fruits via Hybrid Optimal Attention Capsule Network},
  author = {Sajitha, P. and Diana Andrushia, A. and Anand, N. and Naser, M. Z. and Lubloy, Eva},
  year = {2024},
  month = dec,
  journal = {Ecological Informatics},
  volume = {84},
  pages = {102859},
  issn = {1574-9541},
  doi = {10.1016/j.ecoinf.2024.102859},
  urldate = {2025-03-23},
  abstract = {In 2022, the production rate of pomegranate is estimated at approximately 4.8 million metric tons. Unfortunately, these fruits are susceptible to many different kinds of diseases caused by bacterial, viral, and fungal infections. Such diseases can have a major negative impact on fruit quality, production, and the profitability of pomegranate cultivation. Nowadays, several machine learning and deep learning methods are used to identify pomegranate fruit diseases automatically and effectively. In post-harvest pomegranate fruit disease detection, deep learning has great potential to extract complex patterns and features from large datasets. This can improve disease identification accuracy, enabling more efficient disease control, lower crop losses, and better resource management. The proposed work introduces an intelligent deep learning-based approach for accurately detecting pomegranate diseases, begins with Improved Guided Image Filtering (Improved GIF) and resizing to pre-process fruit images, followed by feature extraction (shape, color, texture) using GLCM and GLRLM to streamline classification. Extracted features are then fed into a novel Hybrid Optimal Attention Capsule Network (Hybrid OACapsNet), which classifies the images as normal or diseased, conditions such as bacterial blight, heart rot, and scab. Our analysis indicates that the proposed classifier has a classification accuracy of 99.19~\%, precision of 98.45~\%, recall of 98.41~\%, F1-score of 98.43~\%, and specificity of 99.45~\% compared to other techniques. So this approach offers a framework, which is a feasible solution for automated detection of diseases in fruits, thereby benefiting farmers and supporting their farming operations.},
  keywords = {Deep learning,Fruit disease detection,Hybrid OACapsNet,Pomegranate,Post-harvest technique}
}

@article{sajithaDeepLearningApproach2024a,
  title = {A Deep Learning Approach to Detect Diseases in Pomegranate Fruits via Hybrid Optimal Attention Capsule Network},
  author = {Sajitha, P. and Diana Andrushia, A. and Anand, N. and Naser, M. Z. and Lubloy, Eva},
  year = {2024},
  month = dec,
  journal = {Ecological Informatics},
  volume = {84},
  pages = {102859},
  issn = {1574-9541},
  doi = {10.1016/j.ecoinf.2024.102859},
  urldate = {2025-03-23},
  abstract = {In 2022, the production rate of pomegranate is estimated at approximately 4.8 million metric tons. Unfortunately, these fruits are susceptible to many different kinds of diseases caused by bacterial, viral, and fungal infections. Such diseases can have a major negative impact on fruit quality, production, and the profitability of pomegranate cultivation. Nowadays, several machine learning and deep learning methods are used to identify pomegranate fruit diseases automatically and effectively. In post-harvest pomegranate fruit disease detection, deep learning has great potential to extract complex patterns and features from large datasets. This can improve disease identification accuracy, enabling more efficient disease control, lower crop losses, and better resource management. The proposed work introduces an intelligent deep learning-based approach for accurately detecting pomegranate diseases, begins with Improved Guided Image Filtering (Improved GIF) and resizing to pre-process fruit images, followed by feature extraction (shape, color, texture) using GLCM and GLRLM to streamline classification. Extracted features are then fed into a novel Hybrid Optimal Attention Capsule Network (Hybrid OACapsNet), which classifies the images as normal or diseased, conditions such as bacterial blight, heart rot, and scab. Our analysis indicates that the proposed classifier has a classification accuracy of 99.19~\%, precision of 98.45~\%, recall of 98.41~\%, F1-score of 98.43~\%, and specificity of 99.45~\% compared to other techniques. So this approach offers a framework, which is a feasible solution for automated detection of diseases in fruits, thereby benefiting farmers and supporting their farming operations.},
  keywords = {Deep learning,Fruit disease detection,Hybrid OACapsNet,Pomegranate,Post-harvest technique}
}

@book{salinasruizGeneralizedLinearMixed2023,
  title = {Generalized {{Linear Mixed Models}} with {{Applications}} in {{Agriculture}} and {{Biology}}},
  author = {Salinas Ru{\'i}z, Josafhat and Montesinos L{\'o}pez, Osval Antonio and Hern{\'a}ndez Ram{\'i}rez, Gabriela and Crossa Hiriart, Jose},
  year = {2023},
  publisher = {Springer International Publishing AG},
  address = {Cham, SWITZERLAND},
  urldate = {2025-03-14},
  isbn = {978-3-031-32800-8}
}

@article{sarviaGeometricVsSpectral2024,
  title = {Geometric vs Spectral Content of {{Remotely Piloted Aircraft Systems}} Images in the {{Precision}} Agriculture Context},
  author = {Sarvia, Filippo and De Petris, Samuele and Farbo, Alessandro and {Borgogno-Mondino}, Enrico},
  year = {2024},
  month = sep,
  journal = {The Egyptian Journal of Remote Sensing and Space Sciences},
  volume = {27},
  number = {3},
  pages = {524--531},
  issn = {1110-9823},
  doi = {10.1016/j.ejrs.2024.06.003},
  urldate = {2025-03-29},
  abstract = {In the last years the agricultural sector has been evolving and new technologies, like Unmanned Aerial Vehicles (UAV) and satellites, were introduced to increase crop management efficiency, reducing environmental costs and improving farmers' income. MAIA-S2 sensor is presently one of the most performing optical sensors operating on a Remotely Piloted Aircraft Systems (RPAS); given its spectral features, it aims at supporting a scaling process where monoscopic satellite data (namely Copernicus S2) with high temporal and limited geometric resolution can be integrated with stereoscopic data from RPAS having a very high spatial resolution. In this work, data from MAIA-S2 sensor were used to detect the effects of different fertilization types on corn with reference to a test field located in Carignano (Piemonte region, NW-Italy). Different amounts of top dressing fertilization were applied on corn and an RPAS acquisition operated on 14th June 2021 (corresponding date to the corn stem elongation stage) to explore if any effects could be detectable. Three spectral indices, namely Normalized Difference Vegetation Index, Normalized Difference Red Edge index and Canopy Height Model, computed from at-the-ground reflectance calibrated MAIA-S2 data, were compared to evaluate the correspondent response to the different fertilization rates. Results show that: (i) NDVI poorly detect N-related differences zones; (ii) NDRE and CHM reasonably reflect the different N fertilization doses; (iii) Only CHM proved to be able to detect crop height and, consequently, biomass differences that are known to be induced by different rates of fertilization.},
  keywords = {Canopy height model,Precision farming,RPAS,Spectral index,UAV}
}

@article{sarviaGeometricVsSpectral2024a,
  title = {Geometric vs Spectral Content of {{Remotely Piloted Aircraft Systems}} Images in the {{Precision}} Agriculture Context},
  author = {Sarvia, Filippo and De Petris, Samuele and Farbo, Alessandro and {Borgogno-Mondino}, Enrico},
  year = {2024},
  month = sep,
  journal = {The Egyptian Journal of Remote Sensing and Space Sciences},
  volume = {27},
  number = {3},
  pages = {524--531},
  issn = {1110-9823},
  doi = {10.1016/j.ejrs.2024.06.003},
  urldate = {2025-03-29},
  abstract = {In the last years the agricultural sector has been evolving and new technologies, like Unmanned Aerial Vehicles (UAV) and satellites, were introduced to increase crop management efficiency, reducing environmental costs and improving farmers' income. MAIA-S2 sensor is presently one of the most performing optical sensors operating on a Remotely Piloted Aircraft Systems (RPAS); given its spectral features, it aims at supporting a scaling process where monoscopic satellite data (namely Copernicus S2) with high temporal and limited geometric resolution can be integrated with stereoscopic data from RPAS having a very high spatial resolution. In this work, data from MAIA-S2 sensor were used to detect the effects of different fertilization types on corn with reference to a test field located in Carignano (Piemonte region, NW-Italy). Different amounts of top dressing fertilization were applied on corn and an RPAS acquisition operated on 14th June 2021 (corresponding date to the corn stem elongation stage) to explore if any effects could be detectable. Three spectral indices, namely Normalized Difference Vegetation Index, Normalized Difference Red Edge index and Canopy Height Model, computed from at-the-ground reflectance calibrated MAIA-S2 data, were compared to evaluate the correspondent response to the different fertilization rates. Results show that: (i) NDVI poorly detect N-related differences zones; (ii) NDRE and CHM reasonably reflect the different N fertilization doses; (iii) Only CHM proved to be able to detect crop height and, consequently, biomass differences that are known to be induced by different rates of fertilization.},
  keywords = {Canopy height model,Precision farming,RPAS,Spectral index,UAV}
}

@article{savaryGlobalBurdenPathogens2019,
  title = {The Global Burden of Pathogens and Pests on Major Food Crops},
  author = {Savary, Serge and Willocquet, Laetitia and Pethybridge, Sarah Jane and Esker, Paul and McRoberts, Neil and Nelson, Andy},
  year = {2019},
  month = mar,
  journal = {Nature Ecology \& Evolution},
  volume = {3},
  number = {3},
  pages = {430--439},
  issn = {2397-334X},
  doi = {10.1038/s41559-018-0793-y},
  abstract = {Crop pathogens and pests reduce the yield and quality of agricultural production. They cause substantial economic losses and reduce food security at household, national and global levels. Quantitative, standardized information on crop losses is difficult to compile and compare across crops, agroecosystems and regions. Here, we report on an expert-based assessment of crop health, and provide numerical estimates of yield losses on an individual pathogen and pest basis for five major crops globally and in food security hotspots. Our results document losses associated with 137 pathogens and pests associated with wheat, rice, maize, potato and soybean worldwide. Our yield loss (range) estimates at a global level and per hotspot for wheat (21.5\% (10.1-28.1\%)), rice (30.0\% (24.6-40.9\%)), maize (22.5\% (19.5-41.1\%)), potato (17.2\% (8.1-21.0\%)) and soybean (21.4\% (11.0-32.4\%)) suggest that the highest losses are associated with food-deficit regions with fast-growing populations, and frequently with emerging or re-emerging pests and diseases. Our assessment highlights differences in impacts among crop pathogens and pests and among food security hotspots. This analysis contributes critical information to prioritize crop health management to improve the sustainability of agroecosystems in delivering services to societies.},
  langid = {english},
  pmid = {30718852},
  keywords = {Agriculture,Animals,Climate Change,Crops Agricultural,Food Supply,Host-Pathogen Interactions,Insecta,Mites,Plant Weeds}
}

@book{schabenbergerStatisticalMethodsSpatial2004,
  title = {Statistical {{Methods}} for {{Spatial Data Analysis}}},
  author = {Schabenberger, Oliver and Gotway, Carol A.},
  year = {2004},
  month = dec,
  publisher = {CRC Press},
  abstract = {Understanding spatial statistics requires tools from applied and mathematical statistics, linear model theory, regression, time series, and stochastic processes. It also requires a mindset that focuses on the unique characteristics of spatial data and the development of specialized analytical tools designed explicitly for spatial data analysis. Statistical Methods for Spatial Data Analysis answers the demand for a text that incorporates all of these factors by presenting a balanced exposition that explores both the theoretical foundations of the field of spatial statistics as well as practical methods for the analysis of spatial data. This book is a comprehensive and illustrative treatment of basic statistical theory and methods for spatial data analysis, employing a model-based and frequentist approach that emphasizes the spatial domain. It introduces essential tools and approaches including: measures of autocorrelation and their role in data analysis; the background and theoretical framework supporting random fields; the analysis of mapped spatial point patterns; estimation and modeling of the covariance function and semivariogram; a comprehensive treatment of spatial analysis in the spectral domain; and spatial prediction and kriging. The volume also delivers a thorough analysis of spatial regression, providing a detailed development of linear models with uncorrelated errors, linear models with spatially-correlated errors and generalized linear mixed models for spatial data. It succinctly discusses Bayesian hierarchical models and concludes with reviews on simulating random fields, non-stationary covariance, and spatio-temporal processes.Additional material on the CRC Press website supplements the content of this book. The site provides data sets used as examples in the text, software code that can be used to implement many of the principal methods described and illustrated, and updates to the text itself.},
  googlebooks = {iVJuVLArmZcC},
  isbn = {978-0-203-49198-0},
  langid = {english},
  keywords = {Business & Economics / Industries / Agribusiness,Mathematics / Probability & Statistics / General}
}

@article{schramowskiMakingDeepNeural2020,
  title = {Making Deep Neural Networks Right for the Right Scientific Reasons by Interacting with Their Explanations},
  author = {Schramowski, Patrick and Stammer, Wolfgang and Teso, Stefano and Brugger, Anna and Herbert, Franziska and Shao, Xiaoting and Luigs, Hans-Georg and Mahlein, Anne-Katrin and Kersting, Kristian},
  year = {2020},
  month = aug,
  journal = {Nature Machine Intelligence},
  volume = {2},
  number = {8},
  pages = {476--486},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-0212-3},
  urldate = {2025-03-26},
  abstract = {Deep neural networks have demonstrated excellent performances in many real-world applications. Unfortunately, they may show Clever Hans-like behaviour (making use of confounding factors within datasets) to achieve high performance. In this work we introduce the novel learning setting of explanatory interactive learning and illustrate its benefits on a plant phenotyping research task. Explanatory interactive learning adds the scientist into the training loop, who interactively revises the original model by providing feedback on its explanations. Our experimental results demonstrate that explanatory interactive learning can help to avoid Clever Hans moments in machine learning and encourages (or discourages, if appropriate) trust in the underlying model.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Machine learning,Plant sciences}
}

@misc{schramowskiMakingDeepNeural2024,
  title = {Making Deep Neural Networks Right for the Right Scientific Reasons by Interacting with Their Explanations},
  author = {Schramowski, Patrick and Stammer, Wolfgang and Teso, Stefano and Brugger, Anna and Shao, Xiaoting and Luigs, Hans-Georg and Mahlein, Anne-Katrin and Kersting, Kristian},
  year = {2024},
  month = mar,
  number = {arXiv:2001.05371},
  eprint = {2001.05371},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2001.05371},
  urldate = {2025-03-26},
  abstract = {Deep neural networks have shown excellent performances in many real-world applications. Unfortunately, they may show "Clever Hans"-like behavior -- making use of confounding factors within datasets -- to achieve high performance. In this work, we introduce the novel learning setting of "explanatory interactive learning" (XIL) and illustrate its benefits on a plant phenotyping research task. XIL adds the scientist into the training loop such that she interactively revises the original model via providing feedback on its explanations. Our experimental results demonstrate that XIL can help avoiding Clever Hans moments in machine learning and encourages (or discourages, if appropriate) trust into the underlying model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{seitzComparisonEvaluationMultiView2006,
  title = {A {{Comparison}} and {{Evaluation}} of {{Multi-View Stereo Reconstruction Algorithms}}},
  booktitle = {2006 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'06)},
  author = {Seitz, S.M. and Curless, B. and Diebel, J. and Scharstein, D. and Szeliski, R.},
  year = {2006},
  month = jun,
  volume = {1},
  pages = {519--528},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2006.19},
  urldate = {2025-03-29},
  abstract = {This paper presents a quantitative comparison of several multi-view stereo reconstruction algorithms. Until now, the lack of suitable calibrated multi-view image datasets with known ground truth (3D shape models) has prevented such direct comparisons. In this paper, we first survey multi-view stereo algorithms and compare them qualitatively using a taxonomy that differentiates their key properties. We then describe our process for acquiring and calibrating multiview image datasets with high-accuracy ground truth and introduce our evaluation methodology. Finally, we present the results of our quantitative comparison of state-of-the-art multi-view stereo reconstruction algorithms on six benchmark datasets. The datasets, evaluation details, and instructions for submitting new models are available online at http://vision.middlebury.edu/mview.},
  keywords = {Cameras,Educational institutions,Image databases,Image reconstruction,Layout,Reconstruction algorithms,Shape measurement,Stereo image processing,Stereo vision,Taxonomy}
}

@misc{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2015},
  month = apr,
  number = {arXiv:1409.1556},
  eprint = {1409.1556},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1409.1556},
  urldate = {2025-03-23},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{singhEffectivePlantDisease2024,
  title = {Effective Plant Disease Diagnosis Using {{Vision Transformer}} Trained with Leafy-Generative Adversarial Network-Generated Images},
  author = {Singh, Aadarsh Kumar and Rao, Akhil and Chattopadhyay, Pratik and Maurya, Rahul and Singh, Lokesh},
  year = {2024},
  month = nov,
  journal = {Expert Systems with Applications},
  volume = {254},
  pages = {124387},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2024.124387},
  urldate = {2025-03-23},
  abstract = {Agriculture, as the foundation of human civilization, is critical to the global economy, providing food for billions. Plant diseases, caused by factors such as bacteria, fungi, viruses, and others, loom large over crop yields, jeopardizing farmers' livelihoods worldwide. Rapid and accurate identification of these diseases is critical for agricultural productivity protection and to date, several automated plant disease diagnosis methods have been developed by researchers worldwide. However, the issue of having limited labeled datasets for certain plant leaf diseases poses a significant challenge in training classification models effectively. This scarcity often results in class imbalance which adversely affects a model's ability to accurately predict all the disease classes. It appears there is a need to explore synthetic data generation techniques to train the model for making a better prediction. Further, the disease prediction model should be lightweight so that it can be conveniently integrated with low-end devices with less computational power that farmers can afford to purchase. In this work, we aim to develop an effective neural augmentation model that can render synthetic disease patterns on uninfected leaf images thereby enhancing the leaf disease dataset by adding artificial samples corresponding to those disease classes for which only minor ground truth information is available. Our work extends the state-of-the-art by introducing a new model for leaf disease augmentation, termed ``LeafyGAN'', that comprises two key elements: a segmentation model and a disease translation model, both of which are GAN-based. The segmentation model is a pix2pix GAN that is trained to separate foreground leaf images from the background and is trained using a combination of L1 loss and standard GAN loss. The disease translation model is a CycleGAN which is trained using a combination of adversarial loss and cycle consistency loss, which uses the generated segmented mask to render synthetic disease patterns to the extracted leaf regions. A lightweight MobileViT model trained using this augmented data has been seen to perform disease diagnosis with a remarkable accuracy of 99.92\% on the PlantVillage dataset and 75.72\% on the PlantDoc dataset. Notably, our model achieves an accuracy that is comparable with the recent CNN and Transformer-based models with a significantly lesser number of parameters.},
  keywords = {Disease pattern generation,Generative adversarial networks,Lightweight Vision Transformers,Plant disease diagnosis}
}

@article{slaetsLinearMixedModels2021,
  title = {Linear Mixed Models and Geostatistics for Designed Experiments in Soil Science: {{Two}} Entirely Different Methods or Two Sides of the Same Coin?},
  shorttitle = {Linear Mixed Models and Geostatistics for Designed Experiments in Soil Science},
  author = {Slaets, Johanna I. F. and Boeddinghaus, Runa S. and Piepho, Hans-Peter},
  year = {2021},
  journal = {European Journal of Soil Science},
  volume = {72},
  number = {1},
  pages = {47--68},
  issn = {1365-2389},
  doi = {10.1111/ejss.12976},
  urldate = {2025-03-30},
  abstract = {Soil scientists are accustomed to geostatistical methods and tools such as semivariograms and kriging for analysis of observational data. Such methods assume and exploit that observations are spatially correlated. Conversely, analysis of variance (ANOVA) of designed experiments assumes that observations from different experimental units are independent, an assumption that is justified based on randomization. It may be beneficial, however, to perform an ANOVA assuming a geostatistical covariance model. Also, it is increasingly common to have multiple observations per experimental unit. Simple ANOVA assuming independence of observations is not appropriate for such data. Instead, a linear mixed model accounting for correlation among observations made on the same plot is required for proper analysis. The purpose of this paper is to demonstrate the benefits of integrating geostatistical covariance structures and ANOVA procedures into a linear mixed modelling framework. Two examples from designed experiments are considered in detail, making a link between terminologies and jargon used in geostatistical analysis on the one hand and linear mixed modelling on the other hand. We provide code in R and SAS for both examples in two supporting companion documents. Highlights Analysis of variance and geostatistical analysis can be joined in a mixed model. Randomization justifies the independence assumption in analysis of variance. Geostatistical models imply a correlation of errors and can improve efficiency. Lacking randomization, spatial correlation can be accounted for in a mixed model.},
  copyright = {{\copyright} 2020 The Authors. European Journal of Soil Science published by John Wiley \& Sons Ltd on behalf of British Society of Soil Science.},
  langid = {english},
  keywords = {analysis of variance,designed experiment,kriging,mean comparison,nugget,polynomial regression,random effect,randomization,range,semivariogram,sill,spatial analysis}
}

@article{stroupRethinkingAnalysisNonNormal2015,
  title = {Rethinking the {{Analysis}} of {{Non-Normal Data}} in {{Plant}} and {{Soil Science}}},
  author = {Stroup, Walter W.},
  year = {2015},
  journal = {Agronomy Journal},
  volume = {107},
  number = {2},
  pages = {811--827},
  issn = {1435-0645},
  doi = {10.2134/agronj2013.0342},
  urldate = {2025-03-30},
  abstract = {The introduction of high-quality, useable generalized linear mixed model (GLMM) software in the mid-2000s changed the conversation regarding the analysis of non-normal data from designed experiments. For well over half a century, the reigning paradigm called for using analysis of variance (ANOVA), either assuming approximate normality of the original data or applying a variance-stabilizing transformation. The appearance of GLMMs creates a dilemma. The ANOVA-based analyses and GLMM-based analyses often yield mutually contradictory results. What results should a researcher report, and how should the choice be justified? If GLMM-based analysis is preferred---and there is increasing evidence that this is the case---approaches to data analysis ingrained while learning ANOVA must be unlearned and relearned. The basic issues associated with the analysis of non-normal data are reviewed here, the thought processes required for GLMMs and how they differ from traditional ANOVA are introduced, and three examples are presented, giving an overview of GLMM-based analysis. The three examples include discussions of what is known to date about the relative merits of GLMM- and ANOVA-based analysis of non-normal data.},
  copyright = {{\copyright} 2015 The Authors.},
  langid = {english}
}

@misc{strubellEnergyPolicyConsiderations2019,
  title = {Energy and {{Policy Considerations}} for {{Deep Learning}} in {{NLP}}},
  author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  year = {2019},
  month = jun,
  number = {arXiv:1906.02243},
  eprint = {1906.02243},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.02243},
  urldate = {2025-03-26},
  abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@article{studentProbableErrorMean1908,
  title = {The {{Probable Error}} of a {{Mean}}},
  author = {{Student}},
  year = {1908},
  journal = {Biometrika},
  volume = {6},
  number = {1},
  eprint = {2331554},
  eprinttype = {jstor},
  pages = {1--25},
  publisher = {[Oxford University Press, Biometrika Trust]},
  issn = {0006-3444},
  doi = {10.2307/2331554},
  urldate = {2025-03-14}
}

@book{szeliskiComputerVisionAlgorithms2022,
  title = {Computer {{Vision}}: {{Algorithms}} and {{Applications}}},
  shorttitle = {Computer {{Vision}}},
  author = {Szeliski, Richard},
  year = {2022},
  series = {Texts in {{Computer Science}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  issn = {1868-0941, 1868-095X},
  doi = {10.1007/978-3-030-34372-9},
  urldate = {2025-03-29},
  copyright = {https://www.springer.com/tdm},
  isbn = {978-3-030-34371-2 978-3-030-34372-9},
  langid = {english},
  keywords = {3D Reconstruction,Computational Photography,Computer Vision,Deep Learning,Feature Detection and Matching,Image Processing,Image Segmentation,Image Stitching,Image-Based Rendering,Motion Estimation,Scene Recognition,Structure from Motion}
}

@misc{tanEfficientNetRethinkingModel2020,
  title = {{{EfficientNet}}: {{Rethinking Model Scaling}} for {{Convolutional Neural Networks}}},
  shorttitle = {{{EfficientNet}}},
  author = {Tan, Mingxing and Le, Quoc V.},
  year = {2020},
  month = sep,
  number = {arXiv:1905.11946},
  eprint = {1905.11946},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1905.11946},
  urldate = {2025-03-23},
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{tanEfficientNetRethinkingModel2020a,
  title = {{{EfficientNet}}: {{Rethinking Model Scaling}} for {{Convolutional Neural Networks}}},
  shorttitle = {{{EfficientNet}}},
  author = {Tan, Mingxing and Le, Quoc V.},
  year = {2020},
  month = sep,
  number = {arXiv:1905.11946},
  eprint = {1905.11946},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1905.11946},
  urldate = {2025-03-26},
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{thapaPlantPathology20202020,
  title = {The {{Plant Pathology}} 2020 Challenge Dataset to Classify Foliar Disease of Apples},
  author = {Thapa, Ranjita and Snavely, Noah and Belongie, Serge and Khan, Awais},
  year = {2020},
  month = apr,
  number = {arXiv:2004.11958},
  eprint = {2004.11958},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.11958},
  urldate = {2025-03-23},
  abstract = {Apple orchards in the U.S. are under constant threat from a large number of pathogens and insects. Appropriate and timely deployment of disease management depends on early disease detection. Incorrect and delayed diagnosis can result in either excessive or inadequate use of chemicals, with increased production costs, environmental, and health impacts. We have manually captured 3,651 high-quality, real-life symptom images of multiple apple foliar diseases, with variable illumination, angles, surfaces, and noise. A subset, expert-annotated to create a pilot dataset for apple scab, cedar apple rust, and healthy leaves, was made available to the Kaggle community for 'Plant Pathology Challenge'; part of the Fine-Grained Visual Categorization (FGVC) workshop at CVPR 2020 (Computer Vision and Pattern Recognition). We also trained an off-the-shelf convolutional neural network (CNN) on this data for disease classification and achieved 97\% accuracy on a held-out test set. This dataset will contribute towards development and deployment of machine learning-based automated plant disease classification algorithms to ultimately realize fast and accurate disease detection. We will continue to add images to the pilot dataset for a larger, more comprehensive expert-annotated dataset for future Kaggle competitions and to explore more advanced methods for disease classification and quantification.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing}
}

@book{thenkabailHyperspectralRemoteSensing2016,
  title = {Hyperspectral {{Remote Sensing}} of {{Vegetation}}},
  editor = {Thenkabail, Prasad S. and Lyon, John G.},
  year = {2016},
  month = apr,
  publisher = {CRC Press},
  address = {Boca Raton},
  doi = {10.1201/b11222},
  abstract = {Hyperspectral narrow-band (or imaging spectroscopy) spectral data are fast emerging as practical solutions in modeling and mapping vegetation. Recent research has demonstrated the advances in and merit of hyperspectral data in a range of applications including quantifying agricultural crops, modeling forest canopy biochemical properties, detecting},
  isbn = {978-0-429-19218-0}
}

@article{tocherDesignAnalysisBlock1952,
  title = {The {{Design}} and {{Analysis}} of {{Block Experiments}}},
  author = {Tocher, K. D.},
  year = {1952},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {14},
  number = {1},
  pages = {45--91},
  issn = {2517-6161},
  doi = {10.1111/j.2517-6161.1952.tb00101.x},
  urldate = {2025-03-30},
  abstract = {This paper attempts to give a systematic derivation of the principal results of the analysis of block experiments using matrix notation. It also shows how the problem of design can be formulated in an analytical form and illustrates the type of mathematical problem that arises in such a formulation. The systematic approach to the analysis of experiments enables methods to be evolved for performing these analyses on automatic calculating machines and to facilitate this the paper discusses the analysis of spoilt experiments and the iterative solution of the equations arising in the recovery of inter-block information.},
  copyright = {{\copyright} 1952 The Authors},
  langid = {english}
}

@article{todaHowConvolutionalNeural2019,
  title = {How {{Convolutional Neural Networks Diagnose Plant Disease}}},
  author = {Toda, Yosuke and Okura, Fumio},
  year = {2019},
  journal = {Plant Phenomics (Washington, D.C.)},
  volume = {2019},
  pages = {9237136},
  issn = {2643-6515},
  doi = {10.34133/2019/9237136},
  abstract = {Deep learning with convolutional neural networks (CNNs) has achieved great success in the classification of various plant diseases. However, a limited number of studies have elucidated the process of inference, leaving it as an untouchable black box. Revealing the CNN to extract the learned feature as an interpretable form not only ensures its reliability but also enables the validation of the model authenticity and the training dataset by human intervention. In this study, a variety of neuron-wise and layer-wise visualization methods were applied using a CNN, trained with a publicly available plant disease image dataset. We showed that neural networks can capture the colors and textures of lesions specific to respective diseases upon diagnosis, which resembles human decision-making. While several visualization methods were used as they are, others had to be optimized to target a specific layer that fully captures the features to generate consequential outputs. Moreover, by interpreting the generated attention maps, we identified several layers that were not contributing to inference and removed such layers inside the network, decreasing the number of parameters by 75\% without affecting the classification accuracy. The results provide an impetus for the CNN black box users in the field of plant science to better understand the diagnosis process and lead to further efficient use of deep learning for plant disease diagnosis.},
  langid = {english},
  pmcid = {PMC7706313},
  pmid = {33313540}
}

@article{tomislavhenglPracticalGuideGeostatistical2007,
  title = {A {{Practical Guide}} to {{Geostatistical}}  {{Mapping}} of {{Environmental Variables}}},
  author = {{Tomislav Hengl}},
  year = {2007},
  month = sep,
  issn = {1018-5593},
  urldate = {2025-03-29}
}

@inproceedings{torralbaUnbiasedLookDataset2011,
  title = {Unbiased Look at Dataset Bias},
  booktitle = {{{CVPR}} 2011},
  author = {Torralba, Antonio and Efros, Alexei A.},
  year = {2011},
  month = jun,
  pages = {1521--1528},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2011.5995347},
  urldate = {2025-03-26},
  abstract = {Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.},
  keywords = {Communities,Internet,Object recognition,Support vector machines,Testing,Training,Visualization}
}

@article{trevisanSpatialVariabilityCrop2021,
  title = {Spatial Variability of Crop Responses to Agronomic Inputs in On-Farm Precision Experimentation},
  author = {Trevisan, R. G. and Bullock, D. S. and Martin, N. F.},
  year = {2021},
  month = apr,
  journal = {Precision Agriculture},
  volume = {22},
  number = {2},
  pages = {342--363},
  issn = {1573-1618},
  doi = {10.1007/s11119-020-09720-8},
  urldate = {2025-03-29},
  abstract = {Within-field variability of crop yield levels has been extensively investigated, but the spatial variability of crop yield responses to agronomic treatments is less understood. On-farm precision experimentation (OFPE) can be a valuable tool for the estimation of in-field variation of optimal input rates and thus improve agronomic decisions. Therefore, the objectives of this study were to investigate the spatial variability of optimal input rates in OFPE and the potential economic benefit of site-specific input management. Mixed geographically weighted regression (GWR) models were used to estimate local yield response functions. The methodology was applied to investigate the spatial variability in corn response to nitrogen and seed rates in four cornfields in Illinois, USA. The results showed that spatial heterogeneity of model parameters was significant in all four fields evaluated. On average, the RMSE of the fitted yield decreased from 1.2~Mg~ha-1 in the non-spatial global model to 0.7~Mg~ha-1 in the GWR model, and the r-squared increased from 10 to 68\%. The average potential gain of using optimized uniform rates of seed and nitrogen was US\$ 65.00~ha-1, while the added potential gain of the site-specific application was US\$ 58.00~ha-1. The combination of OFPE and GWR proved to be an effective tool for testing precision agriculture's central hypothesis of whether optimal input application rates display adequate spatial variability to justify the costs of the variable rate technology itself. The reported results encourage more research on response-based input management recommendations instead of the still widespread focus on yield-based algorithms.},
  langid = {english},
  keywords = {Geographically weighted regression,Nitrogen,Seed,Variable rate application,Yield response functions,Zea mays L.}
}

@inproceedings{triggsBundleAdjustmentModern2000,
  title = {Bundle {{Adjustment}} --- {{A Modern Synthesis}}},
  booktitle = {Vision {{Algorithms}}: {{Theory}} and {{Practice}}},
  author = {Triggs, Bill and McLauchlan, Philip F. and Hartley, Richard I. and Fitzgibbon, Andrew W.},
  editor = {Triggs, Bill and Zisserman, Andrew and Szeliski, Richard},
  year = {2000},
  pages = {298--372},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-44480-7_21},
  abstract = {This paper is a survey of the theory and methods of photogrammetric bundle adjustment, aimed at potential implementors in the computer vision community. Bundle adjustment is the problem of refining a visual reconstruction to produce jointly optimal structure and viewing parameter estimates. Topics covered include: the choice of cost function and robustness; numerical optimization including sparse Newton methods, linearly convergent approximations, updating and recursive methods; gauge (datum) invariance; and quality control. The theory is developed for general robust cost functions rather than restricting attention to traditional nonlinear least squares.},
  isbn = {978-3-540-44480-0},
  langid = {english},
  keywords = {Bundle Adjustment,Gauge Freedom,Optimization,Scene Reconstruction,Sparse Matrices}
}

@article{tuckerRedPhotographicInfrared1979,
  title = {Red and Photographic Infrared Linear Combinations for Monitoring Vegetation},
  author = {Tucker, Compton J.},
  year = {1979},
  month = may,
  journal = {Remote Sensing of Environment},
  volume = {8},
  number = {2},
  pages = {127--150},
  issn = {0034-4257},
  doi = {10.1016/0034-4257(79)90013-0},
  urldate = {2025-03-29},
  abstract = {In situ collected spectrometer data were used to evaluate and quantify the relationships between various linear combinations of red and photographic infrared radiances and experimental plot biomass, leaf water content, and chlorophyll content. The radiance variables evaluated included the red and photographic infrared (IR) radiance and the linear combinations of the IR/red ratio, the square root of the IR/red ratio, the IR-red difference, the vegetation index, and the transformed vegetation index. In addition, the corresponding green and red linear combinations were evaluated for comparative purposes. Three data sets were used from June, September, and October sampling periods. Regression analysis showed the increased utility of the IR and red linear combinations vis-{\`a}-vis the same green and red linear combinations. The red and IR linear combinations had 7\% and 14\% greater regression significance than the green and red linear combinations for the June and September sampling periods, respectively. The vegetation index, transformed vegetation index, and square root of the IR/red ratio were the most significant, followed closely by the IR/red ratio. Less than a 6\% difference separated the highest and lowest of these four ER and red linear combinations. The use of these linear combinations was shown to be sensitive primarily to the green leaf area or green leaf biomass. As such, these linear combinations of the red and photographic IR radiances can be employed to monitor the photosynthetically active biomass of plant canopies.}
}

@article{vallabhajosyulaNovelHierarchicalFramework2024,
  title = {A Novel Hierarchical Framework for Plant Leaf Disease Detection Using Residual Vision Transformer},
  author = {Vallabhajosyula, Sasikala and Sistla, Venkatramaphanikumar and Kolli, Venkata Krishna Kishore},
  year = {2024},
  month = may,
  journal = {Heliyon},
  volume = {10},
  number = {9},
  pages = {e29912},
  issn = {2405-8440},
  doi = {10.1016/j.heliyon.2024.e29912},
  urldate = {2025-03-23},
  abstract = {Early detection of plant leaf diseases accurately and promptly is very crucial for safeguarding agricultural crop productivity and ensuring food security. During their life cycle, plant leaves get diseased because of multiple factors like bacteria, fungi, weather conditions, etc. In this work, the authors propose a model that aids in the early detection of leaf diseases using a novel hierarchical residual vision transformer using improved Vision Transformer and ResNet9 models. The proposed model can extract more meaningful and discriminating details by reducing the number of trainable parameters with a smaller number of computations. The proposed method is evaluated on the Local Crop dataset, Plant Village dataset, and Extended Plant Village Dataset with 13, 38, and 51 different leaf disease classes. The proposed model is trained using the best trail parameters of Improved Vision Transformer and classified the features using ResNet 9. Performance evaluation is carried out on a wide aspects over the aforementioned datasets and results revealed that the proposed model outperforms other models such as InceptionV3, MobileNetV2, and ResNet50.},
  keywords = {Deep leaning,Inception V3,MobileNetV2,Plant leaf disease detection,Vision transformer}
}

@article{vanesSpatialNatureRandomization1993,
  title = {Spatial {{Nature}} of {{Randomization}} and {{Its Effect}} on the {{Outcome}} of {{Field Experiments}}},
  author = {{van Es}, H. M. and {van Es}, C. L.},
  year = {1993},
  journal = {Agronomy Journal},
  volume = {85},
  number = {2},
  pages = {420--428},
  issn = {1435-0645},
  doi = {10.2134/agronj1993.00021962008500020046x},
  urldate = {2025-03-30},
  abstract = {Many sites used for field trials exhibit a spatially-dependent variance structure in that nearby observations are autocorrelated. This may affect treatment comparisons made at unequal distances. This study investigated (i) the probabilistic nature of randomization with respect to spatial distances in randomized complete block (RCB) designs and (ii) the effect of spatially unbalanced designs on the outcome of field experiments. The mean distance of treatment comparison increases linearly with the number of treatments in the experiment. The lack of spatial balance also increases and is inversely related to the number of replications. This may result in increases in Type II errors for short-distance treatment contrasts and increases in Type I errors for long-distance contrasts. Evaluation of two experimental RCB designs involving four and nine treatments and four replications showed that the randomization process does not ensure spatial balance. Two simulated experiments, each using 50 randomizations based on the Mercer and Hall data showed no adverse effect from spatially unbalanced treatment comparisons if the underlying variance structure is random, but significant effects if spatial autocorrelation is present. Expected variances for long-distance comparisons (5.6\% of all comparisons) were 51\% higher than the error term resulting in tests of treatment differences at ɑ = 0.05 to actually be conducted at the 0.09 error level. Alternative design and analysis methods that ensure spatially balanced treatment comparisons are discussed.},
  copyright = {Copyright {\copyright} American Society of Agronomy},
  langid = {english}
}

@article{wadouxSamplingDesignOptimization2019,
  title = {Sampling Design Optimization for Soil Mapping with Random Forest},
  author = {Wadoux, Alexandre M. J-C. and Brus, Dick J. and Heuvelink, Gerard B. M.},
  year = {2019},
  month = dec,
  journal = {Geoderma},
  volume = {355},
  pages = {113913},
  issn = {0016-7061},
  doi = {10.1016/j.geoderma.2019.113913},
  urldate = {2025-03-29},
  abstract = {Machine learning techniques are widely employed to generate digital soil maps. The map accuracy is partly determined by the number and spatial locations of the measurements used to calibrate the machine learning model. However, determining the optimal sampling design for mapping with machine learning techniques has not yet been considered in detail in digital soil mapping studies. In this paper, we investigate sampling design optimization for soil mapping with random forest. A design is optimized using spatial simulated annealing by minimizing the mean squared prediction error (MSE). We applied this approach to mapping soil organic carbon for a part of Europe using subsamples of the LUCAS dataset. The optimized subsamples are used as input for the random forest machine learning model, using a large set of readily available environmental data as covariates. We also predicted the same soil property using subsamples selected by simple random sampling, conditioned Latin Hypercube sampling (cLHS), spatial coverage sampling and feature space coverage sampling. Distributions of the estimated population MSEs are obtained through repeated random splitting of the LUCAS dataset, serving as the population of interest, into subsets used for validation, testing and selection of calibration samples, and repeated selection of calibration samples with the various sampling designs. The differences between the medians of the MSE distributions were tested for significance using the non-parametric Mann-Whitney test. The process was repeated for different sample sizes. We also analyzed the spread of the optimized designs in both geographic and feature space to reveal their characteristics. Results show that optimization of the sampling design by minimizing the MSE is worthwhile for small sample sizes. However, an important disadvantage of sampling design optimization using MSE is that it requires known values of the soil property at all locations and as a consequence is only feasible for subsampling an existing dataset. For larger sample sizes, the effect of using an MSE optimized design diminishes. In this case, we recommend to use a sample spread uniformly in the feature (i.e. covariate) space of the most important random forest covariates. The results also show that for our case study, cLHS sampling performs worse than the other sampling designs for mapping with random forest. We stress that comparison of sampling designs for calibration by splitting the data just once is very sensitive to the data split that one happens to use if the validation set is small.},
  keywords = {-means,Conditioned Latin Hypercube,LUCAS,Optimal design,Pedometrics,Random forest,Spatial coverage,Spatial simulated annealing,Uncertainty assessment}
}

@article{wangControllableDataGeneration2024,
  title = {Controllable {{Data Generation}} by {{Deep Learning}}: {{A Review}}},
  shorttitle = {Controllable {{Data Generation}} by {{Deep Learning}}},
  author = {Wang, Shiyu and Du, Yuanqi and Guo, Xiaojie and Pan, Bo and Qin, Zhaohui and Zhao, Liang},
  year = {2024},
  month = oct,
  journal = {ACM Computing Surveys},
  volume = {56},
  number = {9},
  eprint = {2207.09542},
  primaryclass = {cs},
  pages = {1--38},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3648609},
  urldate = {2025-03-26},
  abstract = {Designing and generating new data under targeted properties has been attracting various critical applications such as molecule design, image editing and speech synthesis. Traditional hand-crafted approaches heavily rely on expertise experience and intensive human efforts, yet still suffer from the insufficiency of scientific knowledge and low throughput to support effective and efficient data generation. Recently, the advancement of deep learning has created the opportunity for expressive methods to learn the underlying representation and properties of data. Such capability provides new ways of determining the mutual relationship between the structural patterns and functional properties of the data and leveraging such relationships to generate structural data, given the desired properties. This article is a systematic review that explains this promising research area, commonly known as controllable deep data generation. First, the article raises the potential challenges and provides preliminaries. Then the article formally defines controllable deep data generation, proposes a taxonomy on various techniques and summarizes the evaluation metrics in this specific domain. After that, the article introduces exciting applications of controllable deep data generation, experimentally analyzes and compares existing works. Finally, this article highlights the promising future directions of controllable deep data generation and identifies five potential challenges.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@article{wanOptimizingUAVbasedUncooled2024,
  title = {Optimizing {{UAV-based}} Uncooled Thermal Cameras in Field Conditions for Precision Agriculture},
  author = {Wan, Quanxing and Smigaj, Magdalena and Brede, Benjamin and Kooistra, Lammert},
  year = {2024},
  month = nov,
  journal = {International Journal of applied Earth Observation and Geoinformation},
  volume = {134},
  pages = {104184},
  publisher = {Elsevier},
  issn = {1569-8432},
  doi = {10.1016/j.jag.2024.104184},
  urldate = {2025-03-29},
  langid = {english}
}

@book{websterGeostatisticsEnvironmentalScientists2007,
  title = {Geostatistics for {{Environmental Scientists}}},
  author = {Webster, Richard and Oliver, Margaret A.},
  year = {2007},
  month = oct,
  publisher = {John Wiley \& Sons},
  abstract = {Geostatistics is essential for environmental scientists. Weather and climate vary from place to place, soil varies at every scale at which it is examined, and even man-made attributes -- such as the distribution of pollution -- vary. The techniques used in geostatistics are ideally suited to the needs of environmental scientists, who use them to make the best of sparse data for prediction, and top plan future surveys when resources are limited. Geostatistical technology has advanced much in the last few years and many of these developments are being incorporated into the practitioner's repertoire. This second edition describes these techniques for environmental scientists. Topics such as stochastic simulation, sampling, data screening, spatial covariances, the variogram and its modeling, and spatial prediction by kriging are described in rich detail. At each stage the underlying theory is fully explained, and the rationale behind the choices given, allowing the reader to appreciate the assumptions and constraints involved.},
  googlebooks = {WBwSyvIvNY8C},
  isbn = {978-0-470-51726-0},
  langid = {english},
  keywords = {Mathematics / General,Mathematics / Probability & Statistics / General,Mathematics / Probability & Statistics / Stochastic Processes}
}

@article{westobyStructurefromMotionPhotogrammetryLowcost2012,
  title = {`{{Structure-from-Motion}}' Photogrammetry: {{A}} Low-Cost, Effective Tool for Geoscience Applications},
  shorttitle = {`{{Structure-from-Motion}}' Photogrammetry},
  author = {Westoby, M. J. and Brasington, J. and Glasser, N. F. and Hambrey, M. J. and Reynolds, J. M.},
  year = {2012},
  month = dec,
  journal = {Geomorphology},
  volume = {179},
  pages = {300--314},
  issn = {0169-555X},
  doi = {10.1016/j.geomorph.2012.08.021},
  urldate = {2025-03-29},
  abstract = {High-resolution topographic surveying is traditionally associated with high capital and logistical costs, so that data acquisition is often passed on to specialist third party organisations. The high costs of data collection are, for many applications in the earth sciences, exacerbated by the remoteness and inaccessibility of many field sites, rendering cheaper, more portable surveying platforms (i.e. terrestrial laser scanning or GPS) impractical. This paper outlines a revolutionary, low-cost, user-friendly photogrammetric technique for obtaining high-resolution datasets at a range of scales, termed `Structure-from-Motion' (SfM). Traditional softcopy photogrammetric methods require the 3-D location and pose of the camera(s), or the 3-D location of ground control points to be known to facilitate scene triangulation and reconstruction. In contrast, the SfM method solves the camera pose and scene geometry simultaneously and automatically, using a highly redundant bundle adjustment based on matching features in multiple overlapping, offset images. A comprehensive introduction to the technique is presented, followed by an outline of the methods used to create high-resolution digital elevation models (DEMs) from extensive photosets obtained using a consumer-grade digital camera. As an initial appraisal of the technique, an SfM-derived DEM is compared directly with a similar model obtained using terrestrial laser scanning. This intercomparison reveals that decimetre-scale vertical accuracy can be achieved using SfM even for sites with complex topography and a range of land-covers. Example applications of SfM are presented for three contrasting landforms across a range of scales including; an exposed rocky coastal cliff; a breached moraine-dam complex; and a glacially-sculpted bedrock ridge. The SfM technique represents a major advancement in the field of photogrammetry for geoscience applications. Our results and experiences indicate SfM is an inexpensive, effective, and flexible approach to capturing complex topography.},
  keywords = {Close-range photogrammetry,Digital elevation model (DEM),SFMToolkit,Structure-from-Motion (SfM),Terrestrial laser scanning (TLS)}
}

@article{westobyStructurefromMotionPhotogrammetryLowcost2012a,
  title = {`{{Structure-from-Motion}}' Photogrammetry: {{A}} Low-Cost, Effective Tool for Geoscience Applications},
  shorttitle = {`{{Structure-from-Motion}}' Photogrammetry},
  author = {Westoby, M. J. and Brasington, J. and Glasser, N. F. and Hambrey, M. J. and Reynolds, J. M.},
  year = {2012},
  month = dec,
  journal = {Geomorphology},
  volume = {179},
  pages = {300--314},
  issn = {0169-555X},
  doi = {10.1016/j.geomorph.2012.08.021},
  urldate = {2025-03-30},
  abstract = {High-resolution topographic surveying is traditionally associated with high capital and logistical costs, so that data acquisition is often passed on to specialist third party organisations. The high costs of data collection are, for many applications in the earth sciences, exacerbated by the remoteness and inaccessibility of many field sites, rendering cheaper, more portable surveying platforms (i.e. terrestrial laser scanning or GPS) impractical. This paper outlines a revolutionary, low-cost, user-friendly photogrammetric technique for obtaining high-resolution datasets at a range of scales, termed `Structure-from-Motion' (SfM). Traditional softcopy photogrammetric methods require the 3-D location and pose of the camera(s), or the 3-D location of ground control points to be known to facilitate scene triangulation and reconstruction. In contrast, the SfM method solves the camera pose and scene geometry simultaneously and automatically, using a highly redundant bundle adjustment based on matching features in multiple overlapping, offset images. A comprehensive introduction to the technique is presented, followed by an outline of the methods used to create high-resolution digital elevation models (DEMs) from extensive photosets obtained using a consumer-grade digital camera. As an initial appraisal of the technique, an SfM-derived DEM is compared directly with a similar model obtained using terrestrial laser scanning. This intercomparison reveals that decimetre-scale vertical accuracy can be achieved using SfM even for sites with complex topography and a range of land-covers. Example applications of SfM are presented for three contrasting landforms across a range of scales including; an exposed rocky coastal cliff; a breached moraine-dam complex; and a glacially-sculpted bedrock ridge. The SfM technique represents a major advancement in the field of photogrammetry for geoscience applications. Our results and experiences indicate SfM is an inexpensive, effective, and flexible approach to capturing complex topography.},
  keywords = {Close-range photogrammetry,Digital elevation model (DEM),SFMToolkit,Structure-from-Motion (SfM),Terrestrial laser scanning (TLS)}
}

@article{williamsOptimalityContrastsBlock2015,
  title = {Optimality and {{Contrasts}} in {{Block Designs}} with {{Unequal Treatment Replication}}},
  author = {Williams, Emlyn and Piepho, Hans-Peter},
  year = {2015},
  journal = {Australian \& New Zealand Journal of Statistics},
  volume = {57},
  number = {2},
  pages = {203--209},
  issn = {1467-842X},
  doi = {10.1111/anzs.12116},
  urldate = {2025-03-30},
  abstract = {The computer construction of optimal or near-optimal experimental designs is common in practice. Search procedures are often based on the non-zero eigenvalues of the information matrix of the design. Minimising the average of the pairwise treatment variances can also be used as a search criterion. For equal treatment replication these approaches are equivalent to maximising the harmonic mean of the design's canonical efficiency factors, but differ when treatments are unequally replicated. This paper investigates the extent of these differences and discusses some apparent inconsistencies previously observed when comparing the optimality of equally and unequally replicated designs.},
  copyright = {{\copyright} 2015 Australian Statistical Publishing Association Inc. Published by Wiley Publishing Asia Pty Ltd.},
  langid = {english},
  keywords = {block designs,efficiency factors,optimality,treatment contrasts}
}

@book{wolfElementsPhotogrammetryApplication2013,
  title = {Elements of {{Photogrammetry}} with {{Application}} in {{GIS}}, {{Fourth Edition}}},
  author = {Wolf, Paul R. and DeWitt, Bon A. and Wilkinson, Benjamin E.},
  year = {2013},
  month = oct,
  publisher = {McGraw Hill Professional},
  abstract = {The definitive guide to photogrammetry--fully updated Thoroughly revised to cover the latest technological advances in the field, Elements of Photogrammetry with Applications in GIS, Fourth Edition, provides complete details on the foundational principles of photogrammetry as well as important advanced concepts. Significant changes in the instruments and procedures used in modern photogrammetry, including laser scanning, are discussed. Example problems clarify computational procedures and extensive photographs and diagrams illustrate the material presented in this comprehensive resource. Coverage includes:  Principles of photography and imaging Cameras and other imaging devices Image measurements and refinements Object space coordinate systems Vertical photographs Stereoscopic viewing Stereoscopic parallax Stereoscopic plotting instruments Laser scanning systems Elementary methods of planimetric mapping for GIS Titled and oblique photographs Introduction to analytical photogrammetry Topographic mapping and spatial data collection Fundamental principles of digital image processing Photogrammetric applications in GIS Control for aerial photogrammetry Aerotriangulation Project planning Terrestrial and close-range photogrammetry},
  googlebooks = {bCx5rmWMHyAC},
  isbn = {978-0-07-176111-6},
  langid = {english},
  keywords = {Technology & Engineering / Civil / General}
}

@article{WTO_SPS_Agreement,
  title = {The {{WTO}} Agreement on the Application of Sanitary and Phytosanitary Measures ({{SPS}} Agreement)},
  author = {{World Trade Organization}},
  year = {1995},
  journal = {World Trade Organization},
  urldate = {2025-03-12}
}

@misc{zophRethinkingPretrainingSelftraining2020,
  title = {Rethinking {{Pre-training}} and {{Self-training}}},
  author = {Zoph, Barret and Ghiasi, Golnaz and Lin, Tsung-Yi and Cui, Yin and Liu, Hanxiao and Cubuk, Ekin D. and Le, Quoc V.},
  year = {2020},
  month = nov,
  number = {arXiv:2006.06882},
  eprint = {2006.06882},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.06882},
  urldate = {2025-03-26},
  abstract = {Pre-training is a dominant paradigm in computer vision. For example, supervised ImageNet pre-training is commonly used to initialize the backbones of object detection and segmentation models. He et al., however, show a surprising result that ImageNet pre-training has limited impact on COCO object detection. Here we investigate self-training as another method to utilize additional data on the same setup and contrast it against ImageNet pre-training. Our study reveals the generality and flexibility of self-training with three additional insights: 1) stronger data augmentation and more labeled data further diminish the value of pre-training, 2) unlike pre-training, self-training is always helpful when using stronger data augmentation, in both low-data and high-data regimes, and 3) in the case that pre-training is helpful, self-training improves upon pre-training. For example, on the COCO object detection dataset, pre-training benefits when we use one fifth of the labeled data, and hurts accuracy when we use all labeled data. Self-training, on the other hand, shows positive improvements from +1.3 to +3.4AP across all dataset sizes. In other words, self-training works well exactly on the same setup that pre-training does not work (using ImageNet to help COCO). On the PASCAL segmentation dataset, which is a much smaller dataset than COCO, though pre-training does help significantly, self-training improves upon the pre-trained model. On COCO object detection, we achieve 54.3AP, an improvement of +1.5AP over the strongest SpineNet model. On PASCAL segmentation, we achieve 90.5 mIOU, an improvement of +1.5\% mIOU over the previous state-of-the-art result by DeepLabv3+.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{zotero-1243,
  urldate = {2025-03-12},
  howpublished = {https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=OJ:L:1997:265:FULL}
}

@misc{zotero-1245,
  urldate = {2025-03-12},
  howpublished = {https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=OJ:L:1997:265:FULL}
}

@misc{zotero-1449,
  urldate = {2025-03-23},
  howpublished = {https://openaccess.thecvf.com/content/CVPR2021/papers/Salehi\_Multiresolution\_Knowledge\_Distillation\_for\_Anomaly\_Detection\_CVPR\_2021\_paper.pdf}
}

@misc{zotero-1450,
  urldate = {2025-03-23},
  howpublished = {https://people.csail.mit.edu/wrvb/files/publications/liu2018deep.pdf}
}

@misc{zotero-1464,
  urldate = {2025-03-23},
  howpublished = {https://pdf.sciencedirectassets.com/273474/1-s2.0-S1574954124X00059/1-s2.0-S1574954124004011/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEIX\%2F\%2F\%2F\%2F\%2F\%2F\%2F\%2F\%2F\%2FwEaCXVzLWVhc3QtMSJHMEUCIQD\%2ByF0jnLEk0ShVLoli4s9AldcT06iPuHBM8qFj0cfjtwIgLW\%2F\%2FNp9t397jt6H8hS5kBxtbODHHdeYA8yYZy2ZwolMqvAUI3f\%2F\%2F\%2F\%2F\%2F\%2F\%2F\%2F\%2F\%2FARAFGgwwNTkwMDM1NDY4NjUiDNUjJMeWp\%2FPXgAOLyCqQBWcMZqqOlvNO9PsQWrdcK8lsqoG31Jot1jl3Ze7ddmb9NvvaklQeeDbaMrwkCF9\%2BZmYPyr3XWVbd5edLuNjde5EeaxDMRUOUcEZX2xSUQAXtUAfp8WKYZlzWiDkRto0GC612yI5f9FXt57fmLe6LLK\%2FiKZWNdhfK1lTqAbxas2uLr22sj5M9bfvZ5FaYMmZ0IwtaL1jLokfQG0hpIPTuLRzHmBhoURaOPfI9RdtzELeBENPOrC06P2IndVSq7KqbF1DxgjdimuNgNUisTBR5a5CtXkfw1dnRqNb8LceojxmKp9bjNKIDzIxu6ZVFSl1x6eF\%2Fsnbq4e3\%2BPMjRHC2lEp3xy\%2B5vZN9n0rCA5\%2Biiuc\%2F60iGHKlv4qGPMZ84pi4ngD4alImmp\%2F8xYb5WSS\%2FGnbaG1GcgcFawPR5LXGJdfrWRc6WO9LQuP84WN1OGlvcFwUWKK\%2FIS8xQPIwkESDEMYEQJqsIRDjqdAOBe1hI9s9RihqzwLHdU0QhWf0E6QbGtSHE5miN95sVp7OEHYAnfwp\%2BIf\%2B\%2BcuxTAotvXIRjLV7iwEKx6n8F\%2BQ4\%2BfnddzXFEzaDCrGxfy7NLXWdUV69CYvxCtN2wUI\%2F7yNH7vwPcI90aAtiISFFrxn\%2BwlDpHpjE41ZalORoJWx6AhEYNdtTj\%2FlA0ORklOF4HmDvTJStmXKJSrrytxVzJkbKjE8jcWG6R9tF5KZA2i253cGqN\%2BKMDYCSbv8aYMcH3G03Xs37DpqCFswzRo6d8xi\%2FncvVIoERrqxK\%2FFHbXlpDI6oV5wsCmp0\%2BYJWEp1pns5hM57GeoHifG\%2Ftehry4w5IKETUfNxS9so2pI5ox\%2BfB5WNDf30l2os2XGfBmN\%2FUxj54Zi3Ab7YJKYTcMNPVgb8GOrEBdNlrJaodVDo2iSMhiUGufqOnXYqV2XMBUOzZPybMiSZtt8iKWkpQ\%2Fx8v3ugBXkO21XSuSxgyNYC03nC6T09UddwBD62lpJWU\%2BtOiMQus7bzrJMY4gWRixjM8KZUpRxrFqHmAGcjWDjVDRX\%2B7swxL3URJqjWbBGdm3HILY8qt2hNjjxaXUfhBkKgbxUdP010F1W2fWHtq3Ul\%2F8FXDSvwPn1GPVxF0wZwtRy9xlVKzdjtr\&X-Amz-Algorithm=AWS4-HMAC-SHA256\&X-Amz-Date=20250323T211138Z\&X-Amz-SignedHeaders=host\&X-Amz-Expires=300\&X-Amz-Credential=ASIAQ3PHCVTY3T3Y7MGC\%2F20250323\%2Fus-east-1\%2Fs3\%2Faws4\_request\&X-Amz-Signature=14c743ad138e9da44a7d502dab6e56386653ec2b43218faa05df686323637671\&hash=df35effb639e24a37a09e14a32f5724aa94d054b70eae3791e1070efde6d7f31\&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61\&pii=S1574954124004011\&tid=spdf-5b9f07a9-b7b7-464a-a50d-1cbf2f221732\&sid=b0c16a9e643f20470e4954013ab4a852191agxrqb\&type=client\&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t\&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t\&ua=13115c5155550b065052\&rr=9250ec2368e10e0f\&cc=it}
}

@book{zuurGAMZeroinflatedModels2019,
  title = {{{GAM}} and Zero-Inflated {{Models}}},
  author = {Zuur, Alain F. and Ieno, Elena N. and Savel'ev, Anatolij A. and Zuur, Alain F.},
  year = {2019},
  series = {Beginner's Guide to Spatial, Temporal and Spatial-Temporal Ecological Data Analysis with {{R-INLA}}},
  number = {volume 2},
  publisher = {Highland Statistics Ltd},
  address = {Newburgh, United Kingdom},
  isbn = {978-0-9571741-9-1},
  langid = {english}
}

@article{chiang_quantitative_2020,
	title = {Quantitative {Ordinal} {Scale} {Estimates} of {Plant} {Disease} {Severity}: {Comparing} {Treatments} {Using} a {Proportional} {Odds} {Model}},
	volume = {110},
	issn = {0031-949X, 1943-7684},
	shorttitle = {Quantitative {Ordinal} {Scale} {Estimates} of {Plant} {Disease} {Severity}},
	url = {https://apsjournals.apsnet.org/doi/10.1094/PHYTO-10-18-0372-R},
	doi = {10.1094/PHYTO-10-18-0372-R},
	abstract = {Studies in plant pathology, agronomy, and plant breeding requiring disease severity assessment often use quantitative ordinal scales (i.e., a special type of ordinal scale that uses defined numeric ranges); a frequently used example of such a scale is the Horsfall-Barratt scale. Parametric proportional odds models (POMs) may be used to analyze the ratings obtained from quantitative ordinal scales directly, without converting ratings to percent area affected using range midpoints of such scales (currently a standard procedure). Our aim was to evaluate the performance of the POM for comparing treatments using ordinal estimates of disease severity relative to two alternatives, the midpoint conversions (MCs) and nearest percent estimates (NPEs). A simulation method was implemented and the parameters of the simulation estimated using actual disease severity data from the field. The criterion for comparison of the three approaches was the power of the hypothesis test (the probability to reject the null hypothesis when it is false). Most often, NPEs had superior performance. The performance of the POM was never inferior to using the MC at severity {\textless}40\%. Especially at low disease severity (≤10\%), the POM was superior to using the MC method. Thus, for early onset of disease or for comparing treatments with severities {\textless}40\%, the POM is preferable for analyzing disease severity data based on quantitative ordinal scales when comparing treatments and at severities {\textgreater}40\% is equivalent to other methods.},
	language = {en},
	number = {4},
	urldate = {2025-06-29},
	journal = {Phytopathology®},
	author = {Chiang, K. S. and Liu, H. I. and Chen, Y. L. and El Jarroudi, M. and Bock, C. H.},
	month = apr,
	year = {2020},
	pages = {734--743},
}

@article{moraes_characterizing_2022,
	title = {Characterizing {Heterogeneity} and {Determining} {Sample} {Sizes} for {Accurately} {Estimating} {Wheat} {Fusarium} {Head} {Blight} {Index} in {Research} {Plots}},
	volume = {112},
	issn = {0031-949X, 1943-7684},
	url = {https://apsjournals.apsnet.org/doi/10.1094/PHYTO-04-21-0157-R},
	doi = {10.1094/PHYTO-04-21-0157-R},
	abstract = {Because Fusarium head blight (FHB) intensity is usually highly variable within a plot, the number of spikes rated for FHB index (IND) quantification must be considered when designing experiments. In addition, quantification of sources of IND heterogeneity is crucial for defining sampling protocols. Field experiments were conducted to quantify the variability of IND (“field severity”) at different spatial scales and to investigate the effects of sample size on estimated plot-level mean IND and its accuracy. A total of 216 7-row × 6-m-long plots of a moderately resistant and a susceptible cultivar were spray-inoculated with different Fusarium graminearum spore concentrations at anthesis to generate a range of IND levels. A one-stage cluster sampling approach was used to estimate IND, with an average of 32 spikes rated at each of 10 equally spaced points per plot. Plot-level mean IND ranged from 0.9 to 37.9\%. Heterogeneity of IND, quantified by fitting unconditional hierarchical linear models, was higher among spikes within clusters than among clusters within plots or among plots. The projected relative error of mean IND increased as mean IND decreased, and as sample size decreased to {\textless}100 spikes per plot. Simple random samples were drawn with replacement 50,000 times from the original dataset for each plot and used to estimate the effects of sample sizes on mean IND. Samples of 100 or more spikes resulted in more precise estimates of mean IND than smaller samples. Poor sampling may result in inaccurate estimates of IND and poor interpretation of results.},
	language = {en},
	number = {2},
	urldate = {2025-06-29},
	journal = {Phytopathology®},
	author = {Moraes, Wanderson Bucker and Madden, Laurence V. and Paul, Pierce A.},
	month = feb,
	year = {2022},
	pages = {315--334},
}

@article{chiang_what_2014,
	title = {What {Interval} {Characteristics} {Make} a {Good} {Categorical} {Disease} {Assessment} {Scale}?},
	volume = {104},
	issn = {0031-949X, 1943-7684},
	url = {https://apsjournals.apsnet.org/doi/10.1094/PHYTO-10-13-0279-R},
	doi = {10.1094/PHYTO-10-13-0279-R},
	abstract = {Plant pathologists most often obtain quantitative information on disease severity using visual assessments. Category scales have been used for assessing plant disease severity in field experiments, epidemiological studies, and for screening germplasm. The most widely used category scale is the Horsfall-Barratt (H-B) scale, but reports show that estimates of disease severity using the H-B scale are less precise compared with nearest percent estimates (NPEs) using the 0 to 100\% ratio scale. Few studies have compared different category scales. The objective of this study was to compare NPEs, the H-B midpoint converted data, and four different linear category scales (5 and 10\% increments, with and without additional grades at low severity [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 15.0, 20.0…100\%, and 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 30.0…100\%, respectively]). Results of simulations based on known distributions of disease estimation using the type II error rate (the risk of failing to reject H
              0
              when H
              0
              is false) showed that at disease severity ≤5\%, a 10\% category scale had a greater probability of failing to reject H
              0
              when H
              0
              is false compared with all other methods, while the H-B scale performed least well at 20 to 50\% severity. The 5\% category scale performed as well as NPEs except when disease severity was ≤1\%. Both the 5 and 10\% category scales with the additional grades included performed as well as NPEs. These results were confirmed with a mixed model analysis and bootstrap analysis of the original rater assessment data. A better knowledge of the advantages and disadvantages of category scale types will provide a basis for plant pathologists and plant breeders seeking to maximize accuracy and reliability of assessments to make an informed decision when choosing a disease assessment method.},
	language = {en},
	number = {6},
	urldate = {2025-06-29},
	journal = {Phytopathology®},
	author = {Chiang, Kuo-Szu and Liu, Shih-Chia and Bock, Clive H. and Gottwald, Tim R.},
	month = jun,
	year = {2014},
	pages = {575--585},
}

@article{stevenson_overview_2001,
	title = {An overview of the injury severity score and the new injury severity score},
	volume = {7},
	issn = {1353-8047},
	doi = {10.1136/ip.7.1.10},
	abstract = {OBJECTIVE: The research was undertaken to describe the injury severity score (ISS) and the new injury severity score (NISS) and to illustrate their statistical properties.
DESIGN: Descriptive analysis and assessment of the distribution of these scales.
METHODS: Three data sources--the National Pediatric Trauma Registry; the Massachusetts Uniform Hospital Discharge Data Set; and a trauma registry from an urban level I trauma center in Massachusetts--were used to describe the distribution of the ISS and NISS among injured patients.
RESULTS: The ISS/NISS was found to have a positively skewed distribution and transformation did not improve their skewness.
CONCLUSION: The findings suggest that for statistical or analytical purposes the ISS/ NISS should not be considered a continuous variable, particularly if ISS/NISS is treated as a continuous variable for correlation with an outcome measure.},
	language = {eng},
	number = {1},
	journal = {Injury Prevention: Journal of the International Society for Child and Adolescent Injury Prevention},
	author = {Stevenson, M. and Segui-Gomez, M. and Lescohier, I. and Di Scala, C. and McDonald-Smith, G.},
	month = mar,
	year = {2001},
	pmid = {11289527},
	pmcid = {PMC1730702},
	keywords = {Adolescent, Adult, Aged, Child, Child, Preschool, Cross-Sectional Studies, Female, Humans, Infant, Infant, Newborn, Injury Severity Score, Male, Middle Aged, Prognosis, Reproducibility of Results, United States, Wounds and Injuries},
	pages = {10--13},
}

@article{acutis_perfunctory_2012,
	title = {Perfunctory analysis of variance in agronomy, and its consequences in experimental results interpretation},
	volume = {43},
	issn = {1161-0301},
	url = {https://www.sciencedirect.com/science/article/pii/S1161030112000883},
	doi = {10.1016/j.eja.2012.06.006},
	abstract = {Analysis of variance (ANOVA) is based on two main assumptions, i.e., normality and homogeneity of the variances of the populations samples are collected from. In order to verify the correct application of ANOVA in agronomic research, we revised the two most recent years of two high ranked journals concerning agronomy: European Journal of Agronomy and Field Crops Research. The main issues considered were: presence of tests for normality and homogeneity of variance, and eventually the possibility of identifying problems due to analysis carried out on data not matching these assumptions and to incorrect applications of multiple comparisons. Forty-six percent of the reviewed papers uses ANOVA and, in 60\% of these papers, assumptions are not verified at all (and frequently there are evidences that assumptions are not met), or there is a misuse of multiple comparison tests. We also pointed out that the more relevant risk of transmitting erroneous information to the scientific community comes from the use of wrong techniques for multiple comparisons, in particular when protected least significant difference (LSD) test is used. This was demonstrated through exemplifications carried out using Monte Carlo simulations that showed an unacceptable rate of type-III errors found with the protected LSD methods for means separation. We think this study could represent a useful warning on how to avoid misleading conclusions from agronomic experiments due to the incorrect application of classical statistical techniques (i.e., procedures not fully controlling the type-I error rate at experimentwise level).},
	urldate = {2025-06-29},
	journal = {European Journal of Agronomy},
	author = {Acutis, Marco and Scaglia, Barbara and Confalonieri, Roberto},
	month = nov,
	year = {2012},
	keywords = {Homoscedasticity, LSD, Multiple comparisons, Type-I error rate, Type-III error rate},
	pages = {129--135},
}

@article{aquiles_e_effect_2024,
	title = {The effect of spatial lag on modeling geomatic covariates using analysis of variance},
	volume = {16},
	issn = {1866-928X},
	url = {https://doi.org/10.1007/s12518-024-00579-2},
	doi = {10.1007/s12518-024-00579-2},
	abstract = {In recent years, statistical methods have been developed that include spatial considerations, for example, those that incorporate data with georeferencing. The descriptive part of geographical information systems currently provides many visualization and analysis tools; however, in terms of analysis, these systems are still quite limited, therefore, ignorance of these limitations may result in data with spatial effects being treated with conventional statistical methods for non-spatial use, which can certainly invalidate the excellent work of data capture with advanced tools such as those that are used daily in the geomatic context. This prompted the current document, drawing attention to how geomatic information analyzed with statistical methods that imply independence in modeled observations can be invalid. The Moran index is compared with a proposal for a spatial lag coefficient in the context of experimental design so that users of variance analysis do not apply this well-known procedure in a ritualistic way, perhaps revising some assumptions and perhaps ignoring more important ones. The distortion of the p value generated from the analysis of variance is clear in the presence of spatial dependence. In this case, it is associated with the lag or spatial overlap. The methodology is easy to apply in other designs with the development of the design matrix, its reparameterization and the choice of the respective weight matrix. This may cause users to reconsider the traditional method of analysis and incorporate some appropriate analysis methodology to address spatial effects present in data or in outputs from the modeling process.},
	number = {3},
	journal = {Applied Geomatics},
	author = {Aquiles E., Darghan C. and Darlley S., Taborda L. and Nair J., González S. and Carlos A., Rivera M. and Jesús E., Ospina N.},
	month = sep,
	year = {2024},
	pages = {779--788},
}

@article{slaets_linear_2021,
	title = {Linear mixed models and geostatistics for designed experiments in soil science: {Two} entirely different methods or two sides of the same coin?},
	volume = {72},
	issn = {1365-2389},
	shorttitle = {Linear mixed models and geostatistics for designed experiments in soil science},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ejss.12976},
	doi = {10.1111/ejss.12976},
	abstract = {Soil scientists are accustomed to geostatistical methods and tools such as semivariograms and kriging for analysis of observational data. Such methods assume and exploit that observations are spatially correlated. Conversely, analysis of variance (ANOVA) of designed experiments assumes that observations from different experimental units are independent, an assumption that is justified based on randomization. It may be beneficial, however, to perform an ANOVA assuming a geostatistical covariance model. Also, it is increasingly common to have multiple observations per experimental unit. Simple ANOVA assuming independence of observations is not appropriate for such data. Instead, a linear mixed model accounting for correlation among observations made on the same plot is required for proper analysis. The purpose of this paper is to demonstrate the benefits of integrating geostatistical covariance structures and ANOVA procedures into a linear mixed modelling framework. Two examples from designed experiments are considered in detail, making a link between terminologies and jargon used in geostatistical analysis on the one hand and linear mixed modelling on the other hand. We provide code in R and SAS for both examples in two supporting companion documents. Highlights Analysis of variance and geostatistical analysis can be joined in a mixed model. Randomization justifies the independence assumption in analysis of variance. Geostatistical models imply a correlation of errors and can improve efficiency. Lacking randomization, spatial correlation can be accounted for in a mixed model.},
	language = {en},
	number = {1},
	urldate = {2025-06-29},
	journal = {European Journal of Soil Science},
	author = {Slaets, Johanna I. F. and Boeddinghaus, Runa S. and Piepho, Hans-Peter},
	year = {2021},
	note = {\_eprint: https://bsssjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/ejss.12976},
	keywords = {analysis of variance, designed experiment, kriging, mean comparison, nugget, polynomial regression, random effect, randomization, range, semivariogram, sill, spatial analysis},
	pages = {47--68},
}

@article{millar_remedies_2004,
	series = {Models in {Fisheries} {Research}: {GLMs}, {GAMS} and {GLMMs}},
	title = {Remedies for pseudoreplication},
	volume = {70},
	issn = {0165-7836},
	url = {https://www.sciencedirect.com/science/article/pii/S016578360400181X},
	doi = {10.1016/j.fishres.2004.08.016},
	abstract = {Pseudoreplication is the failure of a statistical analysis to properly incorporate the true structure of randomness present in the data. It has been well documented and studied in the ecological literature but has received little attention in the fisheries literature. Avoiding pseudoreplication in analyses of fisheries data can be difficult due to the complexity of the statistical procedures required. However, recent developments in statistical methodology are decreasing the extent to which pseudoreplication has to be tolerated. Seven examples are given here, beginning with simple design-based remedies and progressing to more challenging examples including the model-based remedies of mixed-effects modelling, generalized linear mixed models, state-space models, and geostatistics.},
	number = {2},
	urldate = {2025-06-29},
	journal = {Fisheries Research},
	author = {Millar, Russell B. and Anderson, Marti J.},
	month = dec,
	year = {2004},
	keywords = {Generalized linear mixed models, Geostatistics, Maximum likelihood, Mixed-effects, Nonparametric MANOVA, Overdispersion, Pseudoreplication, Random effects, Sequential population analysis, State-space models},
	pages = {397--407},
}

@article{Piepho_2011,
author = {Piepho, Hans-Peter and Richter, Christel and Spilke, Joachim and Hartung, Karin and Kunick, Arndt and Thöle, Heinrich},
year = {2011},
month = {11},
pages = {721-735},
title = {Statistical aspects of on-farm experimentation},
volume = {62},
journal = {Crop and Pasture Science},
doi = {10.1071/CP11175}
}

@article{alexopoulos_complementary_2023,
	title = {Complementary {Use} of {Ground}-{Based} {Proximal} {Sensing} and {Airborne}/{Spaceborne} {Remote} {Sensing} {Techniques} in {Precision} {Agriculture}: {A} {Systematic} {Review}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-4395},
	shorttitle = {Complementary {Use} of {Ground}-{Based} {Proximal} {Sensing} and {Airborne}/{Spaceborne} {Remote} {Sensing} {Techniques} in {Precision} {Agriculture}},
	url = {https://www.mdpi.com/2073-4395/13/7/1942},
	doi = {10.3390/agronomy13071942},
	abstract = {As the global population continues to increase, projected to reach an estimated 9.7 billion people by 2050, there will be a growing demand for food production and agricultural resources. Transition toward Agriculture 4.0 is expected to enhance agricultural productivity through the integration of advanced technologies, increase resource efficiency, ensure long-term food security by applying more sustainable farming practices, and enhance resilience and climate change adaptation. By integrating technologies such as ground IoT sensing and remote sensing, via both satellite and Unmanned Aerial Vehicles (UAVs), and exploiting data fusion and data analytics, farming can make the transition to a more efficient, productive, and sustainable paradigm. The present work performs a systematic literature review (SLR), identifying the challenges associated with UAV, Satellite, and Ground Sensing in their application in agriculture, comparing them and discussing their complementary use to facilitate Precision Agriculture (PA) and transition to Agriculture 4.0.},
	language = {en},
	number = {7},
	urldate = {2025-06-29},
	journal = {Agronomy},
	author = {Alexopoulos, Angelos and Koutras, Konstantinos and Ali, Sihem Ben and Puccio, Stefano and Carella, Alessandro and Ottaviano, Roberta and Kalogeras, Athanasios},
	month = jul,
	year = {2023},
	note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {challenges, IoT, Precision Agriculture, proximal sensing, remote sensing, satellite, UAV},
	pages = {1942},
}


@incollection{antoniou_chapter_2023,
	series = {Earth {Observation}},
	title = {Chapter 8 - {On} volunteered geographic information quality: a framework for sharing data quality information},
	isbn = {978-0-323-98983-1},
	shorttitle = {Chapter 8 - {On} volunteered geographic information quality},
	url = {https://www.sciencedirect.com/science/article/pii/B9780323989831000090},
	abstract = {One of the most challenging issues regarding the use of volunteered geographic information (VGI) is data quality. The usually loose, nonhierarchical, and uncontrolled production process of VGI is based on contributions from non-GI professionals which raises concerns about the overall quality and fitness-for-purpose of such spatial data. At the same time, new sources of uncertainty have surfaced, which mainly relate to the underlying socio-economic factors of the areas mapped. In this context, established quality evaluation methods are hard to implement. The chapter describes a new quality evaluation framework that will facilitate VGI data acceptance. The chapter argues that the main obstacle to VGI acceptance is that there is imbalanced information regarding the overall VGI data quality between VGI producers and VGI consumers. A new framework for VGI quality evaluation is suggested, which is based on Economic theories. The work of Akerlof, Spence, and Stiglitz on the analysis of markets with asymmetric information, for which they were awarded the Nobel Prize in Economics, is used as the backbone for the suggested VGI quality evaluation framework. In the course of the chapter, the concepts and the positions of the three Nobel Prize laureates are discussed and ways to be implemented in addressing the issue of VGI data quality evaluation are presented.},
	urldate = {2025-06-29},
	booktitle = {Geoinformatics for {Geosciences}},
	publisher = {Elsevier},
	author = {Antoniou, Vyron},
	editor = {Stathopoulos, Nikolaos and Tsatsaris, Andreas and Kalogeropoulos, Kleomenis},
	month = jan,
	year = {2023},
	doi = {10.1016/B978-0-323-98983-1.00009-0},
	keywords = {quality evaluation, quality framework, Volunteered geographic information},
	pages = {149--160},
}


@article{zhao_grain_2011,
	title = {Grain separation loss monitoring system in combine harvester},
	volume = {76},
	issn = {0168-1699},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169911000354},
	doi = {https://doi.org/10.1016/j.compag.2011.01.016},
	abstract = {Based on laboratory experimental results obtained with an axial threshing test-rig with tangential feeding, cumulative distribution functions of separated grain in axial and radial directions of threshing rotor were built. Based on the analysis of the relationship between grain separation loss and grain separation flux in an area under the concave, an indirect grain separation loss monitoring method is presented in this paper. Piezo-electric polyvinylidene fluoride (PVDF) film was selected as sensitive material to design a grain flux sensor. While grain and material-other-than-grain (MOG) separated in the monitoring area impact on piezo-electric PVDF films, different electric charges are generated. After signal progressing with a charge amplifier, frequency discrimination and wave shaping, the number of grain can be counted by a microcontroller (MCU) and the grain separation loss of combine harvester can be measured in real-time. Field test results indicated that the measurement errors of grain separation loss recorded by the monitoring system relative to the loss checked manually were less than 12\%.},
	number = {2},
	journal = {Computers and Electronics in Agriculture},
	author = {Zhao, Zhan and Li, Yaoming and Chen, Jin and Xu, Jiaojiao},
	year = {2011},
	keywords = {Combine harvester, Grain flux sensor, Grain separation loss, Piezo-electric PVDF film, Real-time monitoring},
	pages = {183--188},
}

@article{cisternas_systematic_2020,
	title = {Systematic literature review of implementations of precision agriculture},
	volume = {176},
	issn = {0168-1699},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169920312357},
	doi = {10.1016/j.compag.2020.105626},
	abstract = {Agriculture production highly depends on water and soil factors which increasingly need to be utilized efficiently. Precision agriculture, through the set of information technologies that it uses, allows to effectively manage these resources. This work aims to gather the existing knowledge on technologies used in precision agriculture and ways to discern the most appropriate one for different contexts in agricultural processes. A systematic literature review is performed to identify precision agriculture implementations and to answer questions such as the type of technologies used, criteria for their comparison and selection, and the existence of frameworks that help to decide what technologies to implement. A total of 3,949 publications were reviewed, of which 259 addressed the posed research questions. The findings are that remote sensors are the most used technology, the required knowledge is an important criterion for deciding to implement precision agriculture, and no framework was found that guides its implementation.},
	urldate = {2025-06-29},
	journal = {Computers and Electronics in Agriculture},
	author = {Cisternas, Isabel and Velásquez, Ignacio and Caro, Angélica and Rodríguez, Alfonso},
	month = sep,
	year = {2020},
	keywords = {Information technologies, Precision agriculture, Precision agriculture implementations, Systematic literature review},
	pages = {105626},
}


@article{mondino_considerazioni_2017,
	title = {Considerazioni su costi e mercato potenziali del telerilevamento da {SAPR} in {Italia} nel settore vitivinicolo},
	url = {https://iris.unito.it/handle/2318/1643737},
	abstract = {I sistemi SAPR (Sistemi Aeromobili a Pilotaggio Remoto) si stanno configurando sempre più come un nuovo strumento di lavoro per molte professioni. Le caratteristiche che ne guidano la diffusione sono la relativa economicità e la sostanziale indipendenza dell’operatore da parti terze. All’accoppiata favorevole “SAPR + sensori ultraleggeri a basso costo” il merito di aver sdoganato e proposto al grande pubblico discipline tecniche come la fotogrammetria e il telerilevamento, introducendole in professioni ed ambiti produttivi più tradizionali. Tuttavia, benché ne sia stata ormai dimostrata l’utilità per applicazioni metriche, meno dimostrabile, allo stato attuale, è la loro efficacia nell’ambito del telerilevamento, dove l’alto grado di automazione, che pure gli algoritmi di trattamento delle immagini hanno raggiunto, non riesce a garantire una analoga semplicità di utilizzo e di lettura dell’informazione. In particolare nel settore agronomico, dove questi sistemi sono immaginati per la conduzione “mirata” delle pratiche colturali (agricoltura di precisione), il trasferimento tecnologico deve essere guidato in modo rigoroso, evitando le improvvisazioni che potrebbero portare, prima che il settore parta, ad un suo affossamento. E’ infatti vero che le tecniche di telerilevamento devono ancora dimostrare agli operatori di settore di poter generare informazioni in grado di indirizzare le loro pratiche agronomiche meglio di quanto possibile con approcci più tradizionali. Soprattutto, devono ancora dimostrare che i costi sono compatibili con quelli di conduzione ordinari, o che comunque la valenza economica (o ambientale) dei benefici prodotti copra almeno i costi sostenuti.},
	language = {ita},
	urldate = {2025-06-29},
	journal = {AIT Conference 11° Workshop tematico di Telerilevamento},
	author = {Mondino, Borgogno and Corrado, Enrico},
	year = {2017},
	note = {Accepted: 2017-06-30T09:06:36Z},
}
