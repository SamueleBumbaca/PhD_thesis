
@misc{zotero-1243,
	url = {https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=OJ:L:1997:265:FULL},
	urldate = {2025-03-12},
}

@misc{EURLex1997265a,
	title = {{EUR}-{Lex} - {L}:1997:265:{TOC} - {EN} - {EUR}-{Lex}},
	shorttitle = {{EUR}-{Lex} - {L}},
	url = {https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=oj:JOL_1997_265_R_TOC},
	language = {en},
	urldate = {2025-03-12},
	note = {Doc ID: L:1997:265:TOC
Doc Sector: other
Doc Title: Official Journal of the European Communities, L 265, 27 September 1997
Doc Type: other
Usr\_lan: en},
}

@misc{zotero-1245,
	url = {https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=OJ:L:1997:265:FULL},
	urldate = {2025-03-12},
}

@article{WTO_SPS_Agreement,
	title = {The {WTO} agreement on the application of sanitary and phytosanitary measures ({SPS} agreement)},
	url = {https://www.wto.org/english/tratop_e/sps_e/spsagr_e.htm},
	urldate = {2025-03-12},
	journal = {World Trade Organization},
	author = {{World Trade Organization}},
	year = {1995},
}

@misc{IPPC,
	title = {International standards for phytosanitary measures (ispms)},
	url = {https://www.ippc.int/en/core-activities/standards-setting/ispms/},
	urldate = {2025-03-12},
	author = {{International Plant Protection Convention}},
	year = {2022},
}

@misc{EC_Regulation_1107_2009,
	title = {Regulation ({EC}) {No} 1107/2009 of the {European} {Parliament} and of the {Council} of 21 {October} 2009 concerning the placing of plant protection products on the market},
	url = {https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32009R1107},
	urldate = {2025-03-12},
	author = {{European Parliament and Council}},
	year = {2009},
	note = {Pages: 1–50
Volume: L 309},
}

@techreport{EPPO_PP1_181,
	title = {{PP} 1/181(5) {Conduct} and reporting of efficacy evaluation trials, including good experimental practice},
	url = {https://pp1.eppo.int/standards/PP1-181-5},
	urldate = {2025-03-12},
	institution = {European and Mediterranean Plant Protection Organization},
	author = {{EPPO}},
	year = {2021},
}

@techreport{EPPO_PP1_135,
	title = {{PP} 1/135(4) phytotoxicity assessment},
	url = {https://pp1.eppo.int/standards/PP1-135-4},
	urldate = {2025-03-12},
	institution = {European and Mediterranean Plant Protection Organization},
	author = {{EPPO}},
	year = {2014},
}

@techreport{EPPO_PP1_152,
	title = {{PP} 1/152 {Design} and analysis of efficacy evaluation trials},
	url = {https://pp1.eppo.int/standards/PP1-152-4},
	urldate = {2025-03-12},
	institution = {European and Mediterranean Plant Protection Organization},
	author = {{EPPO}},
	year = {2012},
}

@techreport{EPPO_PP1_93,
	title = {{PP} 1/93(3) weeds in cereals},
	url = {https://pp1.eppo.int/standards/PP1-093-3},
	urldate = {2025-03-12},
	institution = {European and Mediterranean Plant Protection Organization},
	author = {{EPPO}},
	year = {2015},
}

@article{EWRS_score,
	title = {Einheitliche codierung der phänologischen entwicklungsstadien mono- und dikotyler pflanzen - erweiterte {BBCH}-skala, allgemein},
	volume = {43},
	journal = {Nachrichtenblatt des Deutschen Pflanzenschutzdienstes},
	author = {Bleiholder, H. and van den Boom, T. and Langelüddeke, P. and Stauss, R.},
	year = {1991},
	pages = {265--270},
}

@article{other_scores,
	title = {Nonlinear regression analysis and its applications},
	volume = {1},
	number = {1},
	journal = {Journal of Agricultural, Biological, and Environmental Statistics},
	author = {Bates, D. M. and Watts, D. G.},
	year = {1988},
	pages = {120--135},
}

@misc{EURLex1997265,
	title = {Uniform {Principles} for evaluation and authorisation of plant protection products},
	url = {https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=oj:JOL_1997_265_R_TOC},
	urldate = {2025-03-12},
	author = {{European Commission}},
	year = {1997},
	note = {Pages: 87–109
Volume: L 265},
}

@misc{directive_91_414_EEC,
	title = {Council {Directive} 91/414/{EEC} of 15 {July} 1991 concerning the placing of plant protection products on the market},
	url = {https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX%3A31991L0414},
	urldate = {2025-03-12},
	author = {{Council of the European Communities}},
	year = {1991},
	note = {Pages: 1–32
Volume: L 230},
}

@misc{EURLex1997265,
	title = {Council directive 97/57/{EC} of 22 september 1997},
	year = {1997},
	note = {tex.howpublished: {\textless}a href="\{\vphantom{\}}"{\textgreater}\{\vphantom{\}}{\textless}/a{\textgreater}https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX},
}

@misc{EURLex1997265,
	title = {Council directive 97/57/{EC}},
	url = {https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:31997L0057},
	author = {{European Commission}},
	year = {1997},
}

@article{PP13332024,
	title = {{\textless}span style="font-variant:small-caps;"{\textgreater}{PP}{\textless}/span{\textgreater} 1/333 (1) {Adoption} of digital technology for data generation for the efficacy evaluation of plant protection products},
	issn = {0250-8052, 1365-2338},
	shorttitle = {{\textless}span style="font-variant},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/epp.13037},
	doi = {10.1111/epp.13037},
	language = {en},
	urldate = {2025-03-12},
	journal = {EPPO Bulletin},
	month = nov,
	year = {2024},
	pages = {epp.13037},
}

@article{kumleEstimatingPowerGeneralized2021,
	title = {Estimating power in (generalized) linear mixed models: {An} open introduction and tutorial in {R}},
	volume = {53},
	issn = {1554-3528},
	shorttitle = {Estimating power in (generalized) linear mixed models},
	url = {https://link.springer.com/10.3758/s13428-021-01546-0},
	doi = {10.3758/s13428-021-01546-0},
	abstract = {Abstract
            
              Mixed-effects models are a powerful tool for modeling fixed and random effects simultaneously, but do not offer a feasible analytic solution for estimating the probability that a test correctly rejects the null hypothesis. Being able to estimate this probability, however, is critical for sample size planning, as power is closely linked to the reliability and replicability of empirical findings. A flexible and very intuitive alternative to
              analytic
              power solutions are
              simulation-based
              power analyses. Although various tools for conducting simulation-based power analyses for mixed-effects models are available, there is lack of guidance on how to appropriately use them. In this tutorial, we discuss how to estimate power for mixed-effects models in different use cases: first, how to use models that were fit on available (e.g. published) data to determine sample size; second, how to determine the number of stimuli required for sufficient power; and finally, how to conduct sample size planning without available data. Our examples cover both linear and generalized linear models and we provide code and resources for performing simulation-based power analyses on openly accessible data sets. The present work therefore helps researchers to navigate sound research design when using mixed-effects models, by summarizing resources, collating available knowledge, providing solutions and tools, and applying them to real-world problems in sample sizing planning when sophisticated analysis procedures like mixed-effects models are outlined as inferential procedures.},
	language = {en},
	number = {6},
	urldate = {2025-03-13},
	journal = {Behavior Research Methods},
	author = {Kumle, Levi and Võ, Melissa L.-H. and Draschkow, Dejan},
	month = dec,
	year = {2021},
	pages = {2528--2543},
}

@book{salinasruizGeneralizedLinearMixed2023,
	address = {Cham, SWITZERLAND},
	title = {Generalized {Linear} {Mixed} {Models} with {Applications} in {Agriculture} and {Biology}},
	isbn = {978-3-031-32800-8},
	url = {http://ebookcentral.proquest.com/lib/unitoit/detail.action?docID=30702995},
	urldate = {2025-03-14},
	publisher = {Springer International Publishing AG},
	author = {Salinas Ruíz, Josafhat and Montesinos López, Osval Antonio and Hernández Ramírez, Gabriela and Crossa Hiriart, Jose},
	year = {2023},
}

@book{gburAnalysisGeneralizedLinear2020,
	title = {Analysis of {Generalized} {Linear} {Mixed} {Models} in the {Agricultural} and {Natural} {Resources} {Sciences}},
	isbn = {978-0-89118-182-8},
	abstract = {Generalized Linear Mixed Models in the Agricultural and Natural Resources Sciences provides readers with an understanding and appreciation for the design and analysis of mixed models for non-normally distributed data. It is the only publication of its kind directed specifically toward the agricultural and natural resources sciences audience. Readers will especially benefit from the numerous worked examples based on actual experimental data and the discussion of pitfalls associated with incorrect analyses.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Gbur, Edward E. and Stroup, Walter W. and McCarter, Kevin S. and Durham, Susan and Young, Linda J. and Christman, Mary and West, Mark and Kramer, Matthew},
	month = jan,
	year = {2020},
	note = {Google-Books-ID: BgnMDwAAQBAJ},
	keywords = {Science / Life Sciences / Horticulture, Technology \& Engineering / Agriculture / Agronomy / Crop Science, Technology \& Engineering / Agriculture / General},
}

@article{studentProbableErrorMean1908,
	title = {The {Probable} {Error} of a {Mean}},
	volume = {6},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2331554},
	doi = {10.2307/2331554},
	number = {1},
	urldate = {2025-03-14},
	journal = {Biometrika},
	author = {{Student}},
	year = {1908},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {1--25},
}

@incollection{fisherStatisticalMethodsResearch1992,
	address = {New York, NY},
	title = {Statistical {Methods} for {Research} {Workers}},
	isbn = {978-1-4612-4380-9},
	url = {https://doi.org/10.1007/978-1-4612-4380-9_6},
	abstract = {The prime object of this book is to put into the hands of research workers, and especially of biologists, the means of applying statistical tests accurately to numerical data accumulated in their own laboratories or available in the literature.},
	language = {en},
	urldate = {2025-03-14},
	booktitle = {Breakthroughs in {Statistics}: {Methodology} and {Distribution}},
	publisher = {Springer},
	author = {Fisher, R. A.},
	editor = {Kotz, Samuel and Johnson, Norman L.},
	year = {1992},
	doi = {10.1007/978-1-4612-4380-9_6},
	pages = {66--70},
}

@book{zuurGAMZeroinflatedModels2019,
	address = {Newburgh, United Kingdom},
	series = {Beginner's guide to spatial, temporal and spatial-temporal ecological data analysis with {R}-{INLA}},
	title = {{GAM} and zero-inflated {Models}},
	isbn = {978-0-9571741-9-1},
	language = {eng},
	number = {volume 2},
	publisher = {Highland Statistics Ltd},
	author = {Zuur, Alain F. and Ieno, Elena N. and Savelʹev, Anatolij A. and Zuur, Alain F.},
	year = {2019},
}

@article{kumleEstimatingPowerGeneralized2021a,
	title = {Estimating power in (generalized) linear mixed models: {An} open introduction and tutorial in {R}},
	volume = {53},
	issn = {1554-3528},
	shorttitle = {Estimating power in (generalized) linear mixed models},
	doi = {10.3758/s13428-021-01546-0},
	abstract = {Mixed-effects models are a powerful tool for modeling fixed and random effects simultaneously, but do not offer a feasible analytic solution for estimating the probability that a test correctly rejects the null hypothesis. Being able to estimate this probability, however, is critical for sample size planning, as power is closely linked to the reliability and replicability of empirical findings. A flexible and very intuitive alternative to analytic power solutions are simulation-based power analyses. Although various tools for conducting simulation-based power analyses for mixed-effects models are available, there is lack of guidance on how to appropriately use them. In this tutorial, we discuss how to estimate power for mixed-effects models in different use cases: first, how to use models that were fit on available (e.g. published) data to determine sample size; second, how to determine the number of stimuli required for sufficient power; and finally, how to conduct sample size planning without available data. Our examples cover both linear and generalized linear models and we provide code and resources for performing simulation-based power analyses on openly accessible data sets. The present work therefore helps researchers to navigate sound research design when using mixed-effects models, by summarizing resources, collating available knowledge, providing solutions and tools, and applying them to real-world problems in sample sizing planning when sophisticated analysis procedures like mixed-effects models are outlined as inferential procedures.},
	language = {eng},
	number = {6},
	journal = {Behavior Research Methods},
	author = {Kumle, Levi and Võ, Melissa L.-H. and Draschkow, Dejan},
	month = dec,
	year = {2021},
	pmid = {33954914},
	pmcid = {PMC8613146},
	keywords = {Simulation, Humans, Computer Simulation, Linear Models, lme4, Mixed models, mixedpower, Power, R, Reproducibility of Results, Sample Size},
	pages = {2528--2543},
}

@book{gburAnalysisGeneralizedLinear2020a,
	title = {Analysis of {Generalized} {Linear} {Mixed} {Models} in the {Agricultural} and {Natural} {Resources} {Sciences}},
	isbn = {978-0-89118-182-8},
	abstract = {Generalized Linear Mixed Models in the Agricultural and Natural Resources Sciences provides readers with an understanding and appreciation for the design and analysis of mixed models for non-normally distributed data. It is the only publication of its kind directed specifically toward the agricultural and natural resources sciences audience. Readers will especially benefit from the numerous worked examples based on actual experimental data and the discussion of pitfalls associated with incorrect analyses.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Gbur, Edward E. and Stroup, Walter W. and McCarter, Kevin S. and Durham, Susan and Young, Linda J. and Christman, Mary and West, Mark and Kramer, Matthew},
	month = jan,
	year = {2020},
	note = {Google-Books-ID: BgnMDwAAQBAJ},
	keywords = {Science / Life Sciences / Horticulture, Technology \& Engineering / Agriculture / Agronomy / Crop Science, Technology \& Engineering / Agriculture / General},
}

@article{PP1333,
	title = {{PP} 1/333 (1) {Adoption} of digital technology for data generation for the efficacy evaluation of plant protection products},
	volume = {n/a},
	copyright = {© 2024 European and Mediterranean Plant Protection Organization.},
	issn = {1365-2338},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/epp.13037},
	doi = {10.1111/epp.13037},
	language = {en},
	number = {n/a},
	urldate = {2025-03-17},
	journal = {EPPO Bulletin},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/epp.13037},
}

@incollection{fisherStatisticalMethodsResearch1992a,
	address = {New York, NY},
	title = {Statistical {Methods} for {Research} {Workers}},
	isbn = {978-1-4612-4380-9},
	url = {https://doi.org/10.1007/978-1-4612-4380-9_6},
	abstract = {The prime object of this book is to put into the hands of research workers, and especially of biologists, the means of applying statistical tests accurately to numerical data accumulated in their own laboratories or available in the literature.},
	language = {en},
	urldate = {2025-03-17},
	booktitle = {Breakthroughs in {Statistics}: {Methodology} and {Distribution}},
	publisher = {Springer},
	author = {Fisher, R. A.},
	editor = {Kotz, Samuel and Johnson, Norman L.},
	year = {1992},
	doi = {10.1007/978-1-4612-4380-9_6},
	pages = {66--70},
}

@misc{simonyanVeryDeepConvolutional2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	doi = {10.48550/arXiv.1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv:1409.1556 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{hughesOpenAccessRepository2016,
	title = {An open access repository of images on plant health to enable the development of mobile disease diagnostics},
	url = {http://arxiv.org/abs/1511.08060},
	doi = {10.48550/arXiv.1511.08060},
	abstract = {Human society needs to increase food production by an estimated 70\% by 2050 to feed an expected population size that is predicted to be over 9 billion people. Currently, infectious diseases reduce the potential yield by an average of 40\% with many farmers in the developing world experiencing yield losses as high as 100\%. The widespread distribution of smartphones among crop growers around the world with an expected 5 billion smartphones by 2020 offers the potential of turning the smartphone into a valuable tool for diverse communities growing food. One potential application is the development of mobile disease diagnostics through machine learning and crowdsourcing. Here we announce the release of over 50,000 expertly curated images on healthy and infected leaves of crops plants through the existing online platform PlantVillage. We describe both the data and the platform. These data are the beginning of an on-going, crowdsourcing effort to enable computer vision approaches to help solve the problem of yield losses in crop plants due to infectious diseases.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Hughes, David P. and Salathe, Marcel},
	month = apr,
	year = {2016},
	note = {arXiv:1511.08060 [cs]},
	keywords = {Computer Science - Computers and Society},
}

@misc{thapaPlantPathology20202020,
	title = {The {Plant} {Pathology} 2020 challenge dataset to classify foliar disease of apples},
	url = {http://arxiv.org/abs/2004.11958},
	doi = {10.48550/arXiv.2004.11958},
	abstract = {Apple orchards in the U.S. are under constant threat from a large number of pathogens and insects. Appropriate and timely deployment of disease management depends on early disease detection. Incorrect and delayed diagnosis can result in either excessive or inadequate use of chemicals, with increased production costs, environmental, and health impacts. We have manually captured 3,651 high-quality, real-life symptom images of multiple apple foliar diseases, with variable illumination, angles, surfaces, and noise. A subset, expert-annotated to create a pilot dataset for apple scab, cedar apple rust, and healthy leaves, was made available to the Kaggle community for 'Plant Pathology Challenge'; part of the Fine-Grained Visual Categorization (FGVC) workshop at CVPR 2020 (Computer Vision and Pattern Recognition). We also trained an off-the-shelf convolutional neural network (CNN) on this data for disease classification and achieved 97\% accuracy on a held-out test set. This dataset will contribute towards development and deployment of machine learning-based automated plant disease classification algorithms to ultimately realize fast and accurate disease detection. We will continue to add images to the pilot dataset for a larger, more comprehensive expert-annotated dataset for future Kaggle competitions and to explore more advanced methods for disease classification and quantification.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Thapa, Ranjita and Snavely, Noah and Belongie, Serge and Khan, Awais},
	month = apr,
	year = {2020},
	note = {arXiv:2004.11958 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{heDeepResidualLearning2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	doi = {10.48550/arXiv.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{huangDenselyConnectedConvolutional2017,
	title = {Densely {Connected} {Convolutional} {Networks}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.html},
	urldate = {2025-03-23},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
	year = {2017},
	pages = {4700--4708},
}

@misc{tanEfficientNetRethinkingModel2020,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {http://arxiv.org/abs/1905.11946},
	doi = {10.48550/arXiv.1905.11946},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Tan, Mingxing and Le, Quoc V.},
	month = sep,
	year = {2020},
	note = {arXiv:1905.11946 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{dosovitskiyImageWorth16x162021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{liuSwinTransformerHierarchical2021,
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}},
	shorttitle = {Swin {Transformer}},
	url = {http://arxiv.org/abs/2103.14030},
	doi = {10.48550/arXiv.2103.14030},
	abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with {\textbackslash}textbf\{S\}hifted {\textbackslash}textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	month = aug,
	year = {2021},
	note = {arXiv:2103.14030 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{oquabDINOv2LearningRobust2024,
	title = {{DINOv2}: {Learning} {Robust} {Visual} {Features} without {Supervision}},
	shorttitle = {{DINOv2}},
	url = {http://arxiv.org/abs/2304.07193},
	doi = {10.48550/arXiv.2304.07193},
	abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
	month = feb,
	year = {2024},
	note = {arXiv:2304.07193 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{FacebookresearchDinov22025,
	title = {facebookresearch/dinov2},
	copyright = {Apache-2.0},
	url = {https://github.com/facebookresearch/dinov2},
	abstract = {PyTorch code and models for the DINOv2 self-supervised learning method.},
	urldate = {2025-03-23},
	publisher = {Meta Research},
	month = mar,
	year = {2025},
	note = {original-date: 2023-03-29T16:00:37Z},
}

@misc{ModelsPretrainedWeights,
	title = {Models and pre-trained weights — {Torchvision} main documentation},
	url = {https://pytorch.org/vision/master/models.html},
	urldate = {2025-03-23},
}

@misc{RobustDeepLearningBasedDetector,
	title = {A {Robust} {Deep}-{Learning}-{Based} {Detector} for {Real}-{Time} {Tomato} {Plant} {Diseases} and {Pests} {Recognition}},
	url = {https://www.mdpi.com/1424-8220/17/9/2022},
	urldate = {2025-03-23},
}

@article{todaHowConvolutionalNeural2019,
	title = {How {Convolutional} {Neural} {Networks} {Diagnose} {Plant} {Disease}},
	volume = {2019},
	issn = {2643-6515},
	doi = {10.34133/2019/9237136},
	abstract = {Deep learning with convolutional neural networks (CNNs) has achieved great success in the classification of various plant diseases. However, a limited number of studies have elucidated the process of inference, leaving it as an untouchable black box. Revealing the CNN to extract the learned feature as an interpretable form not only ensures its reliability but also enables the validation of the model authenticity and the training dataset by human intervention. In this study, a variety of neuron-wise and layer-wise visualization methods were applied using a CNN, trained with a publicly available plant disease image dataset. We showed that neural networks can capture the colors and textures of lesions specific to respective diseases upon diagnosis, which resembles human decision-making. While several visualization methods were used as they are, others had to be optimized to target a specific layer that fully captures the features to generate consequential outputs. Moreover, by interpreting the generated attention maps, we identified several layers that were not contributing to inference and removed such layers inside the network, decreasing the number of parameters by 75\% without affecting the classification accuracy. The results provide an impetus for the CNN black box users in the field of plant science to better understand the diagnosis process and lead to further efficient use of deep learning for plant disease diagnosis.},
	language = {eng},
	journal = {Plant Phenomics (Washington, D.C.)},
	author = {Toda, Yosuke and Okura, Fumio},
	year = {2019},
	pmid = {33313540},
	pmcid = {PMC7706313},
	pages = {9237136},
}

@misc{zotero-1449,
	url = {https://openaccess.thecvf.com/content/CVPR2021/papers/Salehi_Multiresolution_Knowledge_Distillation_for_Anomaly_Detection_CVPR_2021_paper.pdf},
	urldate = {2025-03-23},
}

@misc{zotero-1450,
	url = {https://people.csail.mit.edu/wrvb/files/publications/liu2018deep.pdf},
	urldate = {2025-03-23},
}

@misc{chalapathyDeepLearningAnomaly2019,
	title = {Deep {Learning} for {Anomaly} {Detection}: {A} {Survey}},
	shorttitle = {Deep {Learning} for {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/1901.03407},
	doi = {10.48550/arXiv.1901.03407},
	abstract = {Anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. The aim of this survey is two-fold, firstly we present a structured and comprehensive overview of research methods in deep learning-based anomaly detection. Furthermore, we review the adoption of these methods for anomaly across various application domains and assess their effectiveness. We have grouped state-of-the-art research techniques into different categories based on the underlying assumptions and approach adopted. Within each category we outline the basic anomaly detection technique, along with its variants and present key assumptions, to differentiate between normal and anomalous behavior. For each category, we present we also present the advantages and limitations and discuss the computational complexity of the techniques in real application domains. Finally, we outline open issues in research and challenges faced while adopting these techniques.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Chalapathy, Raghavendra and Chawla, Sanjay},
	month = jan,
	year = {2019},
	note = {arXiv:1901.03407 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ruffUnifyingReviewDeep2021,
	title = {A {Unifying} {Review} of {Deep} and {Shallow} {Anomaly} {Detection}},
	volume = {109},
	issn = {0018-9219, 1558-2256},
	url = {http://arxiv.org/abs/2009.11732},
	doi = {10.1109/JPROC.2021.3052449},
	abstract = {Deep learning approaches to anomaly detection have recently improved the state of the art in detection performance on complex datasets such as large collections of images or text. These results have sparked a renewed interest in the anomaly detection problem and led to the introduction of a great variety of new methods. With the emergence of numerous such methods, including approaches based on generative models, one-class classification, and reconstruction, there is a growing need to bring methods of this field into a systematic and unified perspective. In this review we aim to identify the common underlying principles as well as the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic 'shallow' and novel deep approaches and show how this relation might cross-fertilize or extend both directions. We further provide an empirical assessment of major existing methods that is enriched by the use of recent explainability techniques, and present specific worked-through examples together with practical advice. Finally, we outline critical open challenges and identify specific paths for future research in anomaly detection.},
	number = {5},
	urldate = {2025-03-23},
	journal = {Proceedings of the IEEE},
	author = {Ruff, Lukas and Kauffmann, Jacob R. and Vandermeulen, Robert A. and Montavon, Grégoire and Samek, Wojciech and Kloft, Marius and Dietterich, Thomas G. and Müller, Klaus-Robert},
	month = may,
	year = {2021},
	note = {arXiv:2009.11732 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {756--795},
}

@article{sajithaDeepLearningApproach2024,
	title = {A deep learning approach to detect diseases in pomegranate fruits via hybrid optimal attention capsule network},
	volume = {84},
	issn = {1574-9541},
	url = {https://www.sciencedirect.com/science/article/pii/S1574954124004011},
	doi = {10.1016/j.ecoinf.2024.102859},
	abstract = {In 2022, the production rate of pomegranate is estimated at approximately 4.8 million metric tons. Unfortunately, these fruits are susceptible to many different kinds of diseases caused by bacterial, viral, and fungal infections. Such diseases can have a major negative impact on fruit quality, production, and the profitability of pomegranate cultivation. Nowadays, several machine learning and deep learning methods are used to identify pomegranate fruit diseases automatically and effectively. In post-harvest pomegranate fruit disease detection, deep learning has great potential to extract complex patterns and features from large datasets. This can improve disease identification accuracy, enabling more efficient disease control, lower crop losses, and better resource management. The proposed work introduces an intelligent deep learning-based approach for accurately detecting pomegranate diseases, begins with Improved Guided Image Filtering (Improved GIF) and resizing to pre-process fruit images, followed by feature extraction (shape, color, texture) using GLCM and GLRLM to streamline classification. Extracted features are then fed into a novel Hybrid Optimal Attention Capsule Network (Hybrid OACapsNet), which classifies the images as normal or diseased, conditions such as bacterial blight, heart rot, and scab. Our analysis indicates that the proposed classifier has a classification accuracy of 99.19 \%, precision of 98.45 \%, recall of 98.41 \%, F1-score of 98.43 \%, and specificity of 99.45 \% compared to other techniques. So this approach offers a framework, which is a feasible solution for automated detection of diseases in fruits, thereby benefiting farmers and supporting their farming operations.},
	urldate = {2025-03-23},
	journal = {Ecological Informatics},
	author = {Sajitha, P. and Diana Andrushia, A. and Anand, N. and Naser, M. Z. and Lubloy, Eva},
	month = dec,
	year = {2024},
	keywords = {Deep learning, Fruit disease detection, Hybrid OACapsNet, Pomegranate, Post-harvest technique},
	pages = {102859},
}

@article{mohantyUsingDeepLearning2016,
	title = {Using {Deep} {Learning} for {Image}-{Based} {Plant} {Disease} {Detection}},
	volume = {7},
	issn = {1664-462X},
	url = {https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2016.01419/full},
	doi = {10.3389/fpls.2016.01419},
	abstract = {{\textless}p{\textgreater}Crop diseases are a major threat to food security, but their rapid identification remains difficult in many parts of the world due to the lack of the necessary infrastructure. The combination of increasing global smartphone penetration and recent advances in computer vision made possible by deep learning has paved the way for smartphone-assisted disease diagnosis. Using a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions, we train a deep convolutional neural network to identify 14 crop species and 26 diseases (or absence thereof). The trained model achieves an accuracy of 99.35\% on a held-out test set, demonstrating the feasibility of this approach. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a massive global scale.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-03-23},
	journal = {Frontiers in Plant Science},
	author = {Mohanty, Sharada P. and Hughes, David P. and Salathé, Marcel},
	month = sep,
	year = {2016},
	note = {Publisher: Frontiers},
	keywords = {machine learning, deep learning, Disease diagnosis, Crop diseases, digital epidemiology},
}

@article{ferentinosDeepLearningModels2018,
	title = {Deep learning models for plant disease detection and diagnosis},
	volume = {145},
	issn = {0168-1699},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169917311742},
	doi = {10.1016/j.compag.2018.01.009},
	abstract = {In this paper, convolutional neural network models were developed to perform plant disease detection and diagnosis using simple leaves images of healthy and diseased plants, through deep learning methodologies. Training of the models was performed with the use of an open database of 87,848 images, containing 25 different plants in a set of 58 distinct classes of [plant, disease] combinations, including healthy plants. Several model architectures were trained, with the best performance reaching a 99.53\% success rate in identifying the corresponding [plant, disease] combination (or healthy plant). The significantly high success rate makes the model a very useful advisory or early warning tool, and an approach that could be further expanded to support an integrated plant disease identification system to operate in real cultivation conditions.},
	urldate = {2025-03-23},
	journal = {Computers and Electronics in Agriculture},
	author = {Ferentinos, Konstantinos P.},
	month = feb,
	year = {2018},
	keywords = {Machine learning, Artificial intelligence, Convolutional neural networks, Pattern recognition, Plant disease identification},
	pages = {311--318},
}

@article{barbedoFactorsInfluencingUse2018,
	title = {Factors influencing the use of deep learning for plant disease recognition},
	volume = {172},
	issn = {1537-5110},
	url = {https://www.sciencedirect.com/science/article/pii/S1537511018303027},
	doi = {10.1016/j.biosystemseng.2018.05.013},
	abstract = {Deep learning is quickly becoming one of the most important tools for image classification. This technology is now beginning to be applied to the tasks of plant disease classification and recognition. The positive results that are being obtained using this approach hide some issues that are seldom taken into account in the respective experiments. This article presents an investigation into the main factors that affect the design and effectiveness of deep neural nets applied to plant pathology. An in-depth analysis of the subject, in which advantages and shortcomings are highlighted, should lead to more realistic conclusions on the subject. The arguments used throughout the text are built upon both studies found in the literature and experiments carried out using an image database carefully built to reflect and reproduce many of the conditions expected to be found in practice. This database, which contains almost 50,000 images, is being made freely available for academic purposes.},
	urldate = {2025-03-23},
	journal = {Biosystems Engineering},
	author = {Barbedo, Jayme G. A.},
	month = aug,
	year = {2018},
	keywords = {Image processing, Deep neural nets, Disease classification, Image database, Transfer learning},
	pages = {84--91},
}

@article{martinelliAdvancedMethodsPlant2015,
	title = {Advanced methods of plant disease detection. {A} review},
	volume = {35},
	url = {https://hal.science/hal-01284270},
	doi = {10.1007/s13593-014-0246-1},
	abstract = {Plant diseases are responsible for major economic losses in the agricultural industry worldwide. Monitoring plant health and detecting pathogen early are essential to reduce disease spread and facilitate effective management practices. DNA-based and serological methods now provide essential tools for accurate plant disease diagnosis, in addition to the traditional visual scouting for symptoms. Although DNA-based and serological methods have revolutionized plant disease detection, they are not very reliable at asymptomatic stage, especially in case of pathogen with systemic diffusion. They need at least 1–2 days for sample harvest, processing, and analysis. Here, we describe modern methods based on nucleic acid and protein analysis. Then, we review innovative approaches currently under development. Our main findings are the following: (1) novel sensors based on the analysis of host responses, e.g., differential mobility spectrometer and lateral flow devices, deliver instantaneous results and can effectively detect early infections directly in the field; (2) biosensors based on phage display and biophotonics can also detect instantaneously infections although they can be integrated with other systems; and (3) remote sensing techniques coupled with spectroscopy-based methods allow high spatialization of results, these techniques may be very useful as a rapid preliminary identification of primary infections. We explain how these tools will help plant disease management and complement serological and DNA-based methods. While serological and PCR-based methods are the most available and effective to confirm disease diagnosis, volatile and biophotonic sensors provide instantaneous results and may be used to identify infections at asymptomatic stages. Remote sensing technologies will be extremely helpful to greatly spatialize diagnostic results. These innovative techniques represent unprecedented tools to render agriculture more sustainable and safe, avoiding expensive use of pesticides in crop protection.},
	number = {1},
	urldate = {2025-03-23},
	journal = {Agronomy for Sustainable Development},
	author = {Martinelli, Federico and Scalenghe, Riccardo and Davino, Salvatore and Panno, Stefano and Scuderi, Giuseppe and Ruisi, Paolo and Villa, Paolo and Stroppiana, Daniela and Boschetti, Mirco and Goulart, Luiz R. and Davis, Cristina E. and Dandekar, Abhaya M.},
	year = {2015},
	note = {Publisher: Springer Verlag/EDP Sciences/INRA},
	keywords = {Remote sensing, Spectroscopy, Plant disease, Biophotonics, Commercial kits, DNA-based methods, Immunological assays, Volatile organic compounds},
	pages = {1--25},
}

@article{savaryGlobalBurdenPathogens2019,
	title = {The global burden of pathogens and pests on major food crops},
	volume = {3},
	issn = {2397-334X},
	doi = {10.1038/s41559-018-0793-y},
	abstract = {Crop pathogens and pests reduce the yield and quality of agricultural production. They cause substantial economic losses and reduce food security at household, national and global levels. Quantitative, standardized information on crop losses is difficult to compile and compare across crops, agroecosystems and regions. Here, we report on an expert-based assessment of crop health, and provide numerical estimates of yield losses on an individual pathogen and pest basis for five major crops globally and in food security hotspots. Our results document losses associated with 137 pathogens and pests associated with wheat, rice, maize, potato and soybean worldwide. Our yield loss (range) estimates at a global level and per hotspot for wheat (21.5\% (10.1-28.1\%)), rice (30.0\% (24.6-40.9\%)), maize (22.5\% (19.5-41.1\%)), potato (17.2\% (8.1-21.0\%)) and soybean (21.4\% (11.0-32.4\%)) suggest that the highest losses are associated with food-deficit regions with fast-growing populations, and frequently with emerging or re-emerging pests and diseases. Our assessment highlights differences in impacts among crop pathogens and pests and among food security hotspots. This analysis contributes critical information to prioritize crop health management to improve the sustainability of agroecosystems in delivering services to societies.},
	language = {eng},
	number = {3},
	journal = {Nature Ecology \& Evolution},
	author = {Savary, Serge and Willocquet, Laetitia and Pethybridge, Sarah Jane and Esker, Paul and McRoberts, Neil and Nelson, Andy},
	month = mar,
	year = {2019},
	pmid = {30718852},
	keywords = {Agriculture, Crops, Agricultural, Animals, Climate Change, Food Supply, Host-Pathogen Interactions, Insecta, Mites, Plant Weeds},
	pages = {430--439},
}

@article{sajithaDeepLearningApproach2024a,
	title = {A deep learning approach to detect diseases in pomegranate fruits via hybrid optimal attention capsule network},
	volume = {84},
	issn = {1574-9541},
	url = {https://www.sciencedirect.com/science/article/pii/S1574954124004011},
	doi = {10.1016/j.ecoinf.2024.102859},
	abstract = {In 2022, the production rate of pomegranate is estimated at approximately 4.8 million metric tons. Unfortunately, these fruits are susceptible to many different kinds of diseases caused by bacterial, viral, and fungal infections. Such diseases can have a major negative impact on fruit quality, production, and the profitability of pomegranate cultivation. Nowadays, several machine learning and deep learning methods are used to identify pomegranate fruit diseases automatically and effectively. In post-harvest pomegranate fruit disease detection, deep learning has great potential to extract complex patterns and features from large datasets. This can improve disease identification accuracy, enabling more efficient disease control, lower crop losses, and better resource management. The proposed work introduces an intelligent deep learning-based approach for accurately detecting pomegranate diseases, begins with Improved Guided Image Filtering (Improved GIF) and resizing to pre-process fruit images, followed by feature extraction (shape, color, texture) using GLCM and GLRLM to streamline classification. Extracted features are then fed into a novel Hybrid Optimal Attention Capsule Network (Hybrid OACapsNet), which classifies the images as normal or diseased, conditions such as bacterial blight, heart rot, and scab. Our analysis indicates that the proposed classifier has a classification accuracy of 99.19 \%, precision of 98.45 \%, recall of 98.41 \%, F1-score of 98.43 \%, and specificity of 99.45 \% compared to other techniques. So this approach offers a framework, which is a feasible solution for automated detection of diseases in fruits, thereby benefiting farmers and supporting their farming operations.},
	urldate = {2025-03-23},
	journal = {Ecological Informatics},
	author = {Sajitha, P. and Diana Andrushia, A. and Anand, N. and Naser, M. Z. and Lubloy, Eva},
	month = dec,
	year = {2024},
	keywords = {Deep learning, Fruit disease detection, Hybrid OACapsNet, Pomegranate, Post-harvest technique},
	pages = {102859},
}

@misc{zotero-1464,
	url = {https://pdf.sciencedirectassets.com/273474/1-s2.0-S1574954124X00059/1-s2.0-S1574954124004011/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEIX%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQD%2ByF0jnLEk0ShVLoli4s9AldcT06iPuHBM8qFj0cfjtwIgLW%2F%2FNp9t397jt6H8hS5kBxtbODHHdeYA8yYZy2ZwolMqvAUI3f%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDNUjJMeWp%2FPXgAOLyCqQBWcMZqqOlvNO9PsQWrdcK8lsqoG31Jot1jl3Ze7ddmb9NvvaklQeeDbaMrwkCF9%2BZmYPyr3XWVbd5edLuNjde5EeaxDMRUOUcEZX2xSUQAXtUAfp8WKYZlzWiDkRto0GC612yI5f9FXt57fmLe6LLK%2FiKZWNdhfK1lTqAbxas2uLr22sj5M9bfvZ5FaYMmZ0IwtaL1jLokfQG0hpIPTuLRzHmBhoURaOPfI9RdtzELeBENPOrC06P2IndVSq7KqbF1DxgjdimuNgNUisTBR5a5CtXkfw1dnRqNb8LceojxmKp9bjNKIDzIxu6ZVFSl1x6eF%2Fsnbq4e3%2BPMjRHC2lEp3xy%2B5vZN9n0rCA5%2Biiuc%2F60iGHKlv4qGPMZ84pi4ngD4alImmp%2F8xYb5WSS%2FGnbaG1GcgcFawPR5LXGJdfrWRc6WO9LQuP84WN1OGlvcFwUWKK%2FIS8xQPIwkESDEMYEQJqsIRDjqdAOBe1hI9s9RihqzwLHdU0QhWf0E6QbGtSHE5miN95sVp7OEHYAnfwp%2BIf%2B%2BcuxTAotvXIRjLV7iwEKx6n8F%2BQ4%2BfnddzXFEzaDCrGxfy7NLXWdUV69CYvxCtN2wUI%2F7yNH7vwPcI90aAtiISFFrxn%2BwlDpHpjE41ZalORoJWx6AhEYNdtTj%2FlA0ORklOF4HmDvTJStmXKJSrrytxVzJkbKjE8jcWG6R9tF5KZA2i253cGqN%2BKMDYCSbv8aYMcH3G03Xs37DpqCFswzRo6d8xi%2FncvVIoERrqxK%2FFHbXlpDI6oV5wsCmp0%2BYJWEp1pns5hM57GeoHifG%2Ftehry4w5IKETUfNxS9so2pI5ox%2BfB5WNDf30l2os2XGfBmN%2FUxj54Zi3Ab7YJKYTcMNPVgb8GOrEBdNlrJaodVDo2iSMhiUGufqOnXYqV2XMBUOzZPybMiSZtt8iKWkpQ%2Fx8v3ugBXkO21XSuSxgyNYC03nC6T09UddwBD62lpJWU%2BtOiMQus7bzrJMY4gWRixjM8KZUpRxrFqHmAGcjWDjVDRX%2B7swxL3URJqjWbBGdm3HILY8qt2hNjjxaXUfhBkKgbxUdP010F1W2fWHtq3Ul%2F8FXDSvwPn1GPVxF0wZwtRy9xlVKzdjtr&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250323T211138Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY3T3Y7MGC%2F20250323%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=14c743ad138e9da44a7d502dab6e56386653ec2b43218faa05df686323637671&hash=df35effb639e24a37a09e14a32f5724aa94d054b70eae3791e1070efde6d7f31&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1574954124004011&tid=spdf-5b9f07a9-b7b7-464a-a50d-1cbf2f221732&sid=b0c16a9e643f20470e4954013ab4a852191agxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=13115c5155550b065052&rr=9250ec2368e10e0f&cc=it},
	urldate = {2025-03-23},
}

@article{singhEffectivePlantDisease2024,
	title = {Effective plant disease diagnosis using {Vision} {Transformer} trained with leafy-generative adversarial network-generated images},
	volume = {254},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417424012533},
	doi = {10.1016/j.eswa.2024.124387},
	abstract = {Agriculture, as the foundation of human civilization, is critical to the global economy, providing food for billions. Plant diseases, caused by factors such as bacteria, fungi, viruses, and others, loom large over crop yields, jeopardizing farmers’ livelihoods worldwide. Rapid and accurate identification of these diseases is critical for agricultural productivity protection and to date, several automated plant disease diagnosis methods have been developed by researchers worldwide. However, the issue of having limited labeled datasets for certain plant leaf diseases poses a significant challenge in training classification models effectively. This scarcity often results in class imbalance which adversely affects a model’s ability to accurately predict all the disease classes. It appears there is a need to explore synthetic data generation techniques to train the model for making a better prediction. Further, the disease prediction model should be lightweight so that it can be conveniently integrated with low-end devices with less computational power that farmers can afford to purchase. In this work, we aim to develop an effective neural augmentation model that can render synthetic disease patterns on uninfected leaf images thereby enhancing the leaf disease dataset by adding artificial samples corresponding to those disease classes for which only minor ground truth information is available. Our work extends the state-of-the-art by introducing a new model for leaf disease augmentation, termed “LeafyGAN”, that comprises two key elements: a segmentation model and a disease translation model, both of which are GAN-based. The segmentation model is a pix2pix GAN that is trained to separate foreground leaf images from the background and is trained using a combination of L1 loss and standard GAN loss. The disease translation model is a CycleGAN which is trained using a combination of adversarial loss and cycle consistency loss, which uses the generated segmented mask to render synthetic disease patterns to the extracted leaf regions. A lightweight MobileViT model trained using this augmented data has been seen to perform disease diagnosis with a remarkable accuracy of 99.92\% on the PlantVillage dataset and 75.72\% on the PlantDoc dataset. Notably, our model achieves an accuracy that is comparable with the recent CNN and Transformer-based models with a significantly lesser number of parameters.},
	urldate = {2025-03-23},
	journal = {Expert Systems with Applications},
	author = {Singh, Aadarsh Kumar and Rao, Akhil and Chattopadhyay, Pratik and Maurya, Rahul and Singh, Lokesh},
	month = nov,
	year = {2024},
	keywords = {Disease pattern generation, Generative adversarial networks, Lightweight Vision Transformers, Plant disease diagnosis},
	pages = {124387},
}

@article{vallabhajosyulaNovelHierarchicalFramework2024,
	title = {A novel hierarchical framework for plant leaf disease detection using residual vision transformer},
	volume = {10},
	issn = {2405-8440},
	url = {https://www.sciencedirect.com/science/article/pii/S2405844024059437},
	doi = {10.1016/j.heliyon.2024.e29912},
	abstract = {Early detection of plant leaf diseases accurately and promptly is very crucial for safeguarding agricultural crop productivity and ensuring food security. During their life cycle, plant leaves get diseased because of multiple factors like bacteria, fungi, weather conditions, etc. In this work, the authors propose a model that aids in the early detection of leaf diseases using a novel hierarchical residual vision transformer using improved Vision Transformer and ResNet9 models. The proposed model can extract more meaningful and discriminating details by reducing the number of trainable parameters with a smaller number of computations. The proposed method is evaluated on the Local Crop dataset, Plant Village dataset, and Extended Plant Village Dataset with 13, 38, and 51 different leaf disease classes. The proposed model is trained using the best trail parameters of Improved Vision Transformer and classified the features using ResNet 9. Performance evaluation is carried out on a wide aspects over the aforementioned datasets and results revealed that the proposed model outperforms other models such as InceptionV3, MobileNetV2, and ResNet50.},
	number = {9},
	urldate = {2025-03-23},
	journal = {Heliyon},
	author = {Vallabhajosyula, Sasikala and Sistla, Venkatramaphanikumar and Kolli, Venkata Krishna Kishore},
	month = may,
	year = {2024},
	keywords = {Deep leaning, Inception V3, MobileNetV2, Plant leaf disease detection, Vision transformer},
	pages = {e29912},
}

@misc{katafuchiImagebasedPlantDisease2021,
	title = {Image-based {Plant} {Disease} {Diagnosis} with {Unsupervised} {Anomaly} {Detection} {Based} on {Reconstructability} of {Colors}},
	url = {http://arxiv.org/abs/2011.14306},
	doi = {10.48550/arXiv.2011.14306},
	abstract = {This paper proposes an unsupervised anomaly detection technique for image-based plant disease diagnosis. The construction of large and publicly available datasets containing labeled images of healthy and diseased crop plants led to growing interest in computer vision techniques for automatic plant disease diagnosis. Although supervised image classifiers based on deep learning can be a powerful tool for plant disease diagnosis, they require a huge amount of labeled data. The data mining technique of anomaly detection includes unsupervised approaches that do not require rare samples for training classifiers. We propose an unsupervised anomaly detection technique for image-based plant disease diagnosis that is based on the reconstructability of colors; a deep encoder-decoder network trained to reconstruct the colors of {\textbackslash}textit\{healthy\} plant images should fail to reconstruct colors of symptomatic regions. Our proposed method includes a new image-based framework for plant disease detection that utilizes a conditional adversarial network called pix2pix and a new anomaly score based on CIEDE2000 color difference. Experiments with PlantVillage dataset demonstrated the superiority of our proposed method compared to an existing anomaly detector at identifying diseased crop images in terms of accuracy, interpretability and computational efficiency.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Katafuchi, Ryoya and Tokunaga, Terumasa},
	month = sep,
	year = {2021},
	note = {arXiv:2011.14306 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{bumbacaSupportingScreeningNew2024,
	title = {Supporting {Screening} of {New} {Plant} {Protection} {Products} through a {Multispectral} {Photogrammetric} {Approach} {Integrated} with {AI}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-4395},
	url = {https://www.mdpi.com/2073-4395/14/2/306},
	doi = {10.3390/agronomy14020306},
	abstract = {This work was aimed at developing a prototype system based on multispectral digital photogrammetry to support tests required by international regulations for new Plant Protection Products (PPPs). In particular, the goal was to provide a system addressing the challenges of a new PPP evaluation with a higher degree of objectivity with respect to the current one, which relies on expert evaluations. The system uses Digital Photogrammetry, which is applied to multispectral acquisitions and Artificial Intelligence (AI). The goal of this paper is also to simplify the present screening process, moving it towards more objective and quantitative scores about phytotoxicity. The implementation of an opportunely trained AI model for phytotoxicity prediction aims to convert ordinary human visual observations, which are presently provided with a discrete scale (forbidding a variance analysis), into a continuous variable. The technical design addresses the need for a reduced dataset for training the AI model and relating discrete observations, as usually performed, to some proxy variables derived from the photogrammetric multispectral 3D model. To achieve this task, an appropriate photogrammetric multispectral system was designed. The system operates in multi-nadiral-view mode over a bench within a greenhouse exploiting an active system for lighting providing uniform and diffuse illumination. The whole system is intended to reduce the environmental variability of acquisitions tending to a standard situation. The methodology combines advanced image processing, image radiometric calibration, and machine learning techniques to predict the General Phytotoxicity percentage index (PHYGEN), a crucial measure of phytotoxicity. Results show that the system can generate reliable estimates of PHYGEN, compliant with existing accuracy standards (even from previous PPPs symptom severity models), using limited training datasets. The proposed solution addressing this challenge is the adoption of the Logistic Function with LASSO model regularization that has been shown to overcome the limitations of a small sample size (typical of new PPP trials). Additionally, it provides the estimate of a numerical continuous index (a percentage), which makes it possible to tackle the objectivity problem related to human visual evaluation that is presently based on an ordinal discrete scale. In our opinion, the proposed prototype system could have significant potential in improving the screening process for new PPPs. In fact, it works specifically for new PPPs screening and, despite this, it has an accuracy consistent with the one ordinarily accepted for human visual approaches. Additionally, it provides a higher degree of objectivity and repeatability.},
	language = {en},
	number = {2},
	urldate = {2025-03-23},
	journal = {Agronomy},
	author = {Bumbaca, Samuele and Borgogno-Mondino, Enrico},
	month = feb,
	year = {2024},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {machine learning, computer vision, diagnostic, digitalization, plant protection product},
	pages = {306},
}

@misc{tanEfficientNetRethinkingModel2020a,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {http://arxiv.org/abs/1905.11946},
	doi = {10.48550/arXiv.1905.11946},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
	urldate = {2025-03-26},
	publisher = {arXiv},
	author = {Tan, Mingxing and Le, Quoc V.},
	month = sep,
	year = {2020},
	note = {arXiv:1905.11946 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{zophRethinkingPretrainingSelftraining2020,
	title = {Rethinking {Pre}-training and {Self}-training},
	url = {http://arxiv.org/abs/2006.06882},
	doi = {10.48550/arXiv.2006.06882},
	abstract = {Pre-training is a dominant paradigm in computer vision. For example, supervised ImageNet pre-training is commonly used to initialize the backbones of object detection and segmentation models. He et al., however, show a surprising result that ImageNet pre-training has limited impact on COCO object detection. Here we investigate self-training as another method to utilize additional data on the same setup and contrast it against ImageNet pre-training. Our study reveals the generality and flexibility of self-training with three additional insights: 1) stronger data augmentation and more labeled data further diminish the value of pre-training, 2) unlike pre-training, self-training is always helpful when using stronger data augmentation, in both low-data and high-data regimes, and 3) in the case that pre-training is helpful, self-training improves upon pre-training. For example, on the COCO object detection dataset, pre-training benefits when we use one fifth of the labeled data, and hurts accuracy when we use all labeled data. Self-training, on the other hand, shows positive improvements from +1.3 to +3.4AP across all dataset sizes. In other words, self-training works well exactly on the same setup that pre-training does not work (using ImageNet to help COCO). On the PASCAL segmentation dataset, which is a much smaller dataset than COCO, though pre-training does help significantly, self-training improves upon the pre-trained model. On COCO object detection, we achieve 54.3AP, an improvement of +1.5AP over the strongest SpineNet model. On PASCAL segmentation, we achieve 90.5 mIOU, an improvement of +1.5\% mIOU over the previous state-of-the-art result by DeepLabv3+.},
	urldate = {2025-03-26},
	publisher = {arXiv},
	author = {Zoph, Barret and Ghiasi, Golnaz and Lin, Tsung-Yi and Cui, Yin and Liu, Hanxiao and Cubuk, Ekin D. and Le, Quoc V.},
	month = nov,
	year = {2020},
	note = {arXiv:2006.06882 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{DeepTransferLearning,
	title = {Deep transfer learning for image classification: a survey},
	shorttitle = {Deep transfer learning for image classification},
	url = {https://ar5iv.labs.arxiv.org/html/2205.09904},
	abstract = {Deep neural networks such as convolutional neural networks (CNNs) and transformers have achieved many successes
in image classification in recent years. It has been consistently
demonstrated that best practice for imag…},
	language = {en},
	urldate = {2025-03-26},
	journal = {ar5iv},
}

@misc{razavianCNNFeaturesOfftheshelf2014,
	title = {{CNN} {Features} off-the-shelf: an {Astounding} {Baseline} for {Recognition}},
	shorttitle = {{CNN} {Features} off-the-shelf},
	url = {http://arxiv.org/abs/1403.6382},
	doi = {10.48550/arXiv.1403.6382},
	abstract = {Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the {\textbackslash}overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the {\textbackslash}overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the {\textbackslash}overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or \$L2\$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
	urldate = {2025-03-26},
	publisher = {arXiv},
	author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
	month = may,
	year = {2014},
	note = {arXiv:1403.6382 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{kornblithBetterImageNetModels2019,
	title = {Do {Better} {ImageNet} {Models} {Transfer} {Better}?},
	url = {http://arxiv.org/abs/1805.08974},
	doi = {10.48550/arXiv.1805.08974},
	abstract = {Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classification networks on 12 image classification datasets. We find that, when networks are used as fixed feature extractors or fine-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy (\$r = 0.99\$ and \$0.96\$, respectively). In the former setting, we find that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield penultimate layer features that are much worse for transfer learning. Additionally, we find that, on two small fine-grained image classification datasets, pretraining on ImageNet provides minimal benefits, indicating the learned features from ImageNet do not transfer well to fine-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.},
	urldate = {2025-03-26},
	publisher = {arXiv},
	author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
	month = jun,
	year = {2019},
	note = {arXiv:1805.08974 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{devlinBERTPretrainingDeep2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2025-03-26},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{chenSimpleFrameworkContrastive2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	doi = {10.48550/arXiv.2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	urldate = {2025-03-26},
	publisher = {arXiv},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jul,
	year = {2020},
	note = {arXiv:2002.05709 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{schramowskiMakingDeepNeural2024,
	title = {Making deep neural networks right for the right scientific reasons by interacting with their explanations},
	url = {http://arxiv.org/abs/2001.05371},
	doi = {10.48550/arXiv.2001.05371},
	abstract = {Deep neural networks have shown excellent performances in many real-world applications. Unfortunately, they may show "Clever Hans"-like behavior -- making use of confounding factors within datasets -- to achieve high performance. In this work, we introduce the novel learning setting of "explanatory interactive learning" (XIL) and illustrate its benefits on a plant phenotyping research task. XIL adds the scientist into the training loop such that she interactively revises the original model via providing feedback on its explanations. Our experimental results demonstrate that XIL can help avoiding Clever Hans moments in machine learning and encourages (or discourages, if appropriate) trust into the underlying model.},
	urldate = {2025-03-26},
	publisher = {arXiv},
	author = {Schramowski, Patrick and Stammer, Wolfgang and Teso, Stefano and Brugger, Anna and Shao, Xiaoting and Luigs, Hans-Georg and Mahlein, Anne-Katrin and Kersting, Kristian},
	month = mar,
	year = {2024},
	note = {arXiv:2001.05371 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{strubellEnergyPolicyConsiderations2019,
	title = {Energy and {Policy} {Considerations} for {Deep} {Learning} in {NLP}},
	url = {http://arxiv.org/abs/1906.02243},
	doi = {10.48550/arXiv.1906.02243},
	abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
	urldate = {2025-03-26},
	publisher = {arXiv},
	author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
	month = jun,
	year = {2019},
	note = {arXiv:1906.02243 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{pattersonCarbonEmissionsLarge2021,
	title = {Carbon {Emissions} and {Large} {Neural} {Network} {Training}},
	url = {http://arxiv.org/abs/2104.10350},
	doi = {10.48550/arXiv.2104.10350},
	abstract = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume {\textless}1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary {\textasciitilde}5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be {\textasciitilde}1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be {\textasciitilde}2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to {\textasciitilde}100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
	urldate = {2025-03-26},
	publisher = {arXiv},
	author = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
	month = apr,
	year = {2021},
	note = {arXiv:2104.10350 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computers and Society},
}

@misc{heMomentumContrastUnsupervised2020,
	title = {Momentum {Contrast} for {Unsupervised} {Visual} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1911.05722},
	doi = {10.48550/arXiv.1911.05722},
	abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
	urldate = {2025-03-26},
	publisher = {arXiv},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	month = mar,
	year = {2020},
	note = {arXiv:1911.05722 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{bommasaniOpportunitiesRisksFoundation2022,
	title = {On the {Opportunities} and {Risks} of {Foundation} {Models}},
	url = {http://arxiv.org/abs/2108.07258},
	doi = {10.48550/arXiv.2108.07258},
	abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	urldate = {2025-03-26},
	publisher = {arXiv},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and Arx, Sydney von and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	month = jul,
	year = {2022},
	note = {arXiv:2108.07258 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
}

@misc{brownLanguageModelsAre2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2025-03-26},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{radfordLearningTransferableVisual2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {http://arxiv.org/abs/2103.00020},
	doi = {10.48550/arXiv.2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	urldate = {2025-03-26},
	publisher = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv:2103.00020 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{pattersonCarbonEmissionsLarge2021a,
	title = {Carbon {Emissions} and {Large} {Neural} {Network} {Training}},
	url = {https://ui.adsabs.harvard.edu/abs/2021arXiv210410350P},
	doi = {10.48550/arXiv.2104.10350},
	abstract = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume {\textless}1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary {\textasciitilde}5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be {\textasciitilde}1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be {\textasciitilde}2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to {\textasciitilde}100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
	urldate = {2025-03-26},
	author = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
	month = apr,
	year = {2021},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2021arXiv210410350P},
	keywords = {Computer Science - Machine Learning, Computer Science - Computers and Society},
}

@misc{pattersonCarbonEmissionsLarge2021b,
	title = {Carbon {Emissions} and {Large} {Neural} {Network} {Training}},
	url = {http://arxiv.org/abs/2104.10350},
	doi = {10.48550/arXiv.2104.10350},
	abstract = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume {\textless}1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary {\textasciitilde}5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be {\textasciitilde}1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be {\textasciitilde}2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to {\textasciitilde}100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
	urldate = {2025-03-26},
	publisher = {arXiv},
	author = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
	month = apr,
	year = {2021},
	note = {arXiv:2104.10350 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computers and Society},
}

@article{wangControllableDataGeneration2024,
	title = {Controllable {Data} {Generation} by {Deep} {Learning}: {A} {Review}},
	volume = {56},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Controllable {Data} {Generation} by {Deep} {Learning}},
	url = {http://arxiv.org/abs/2207.09542},
	doi = {10.1145/3648609},
	abstract = {Designing and generating new data under targeted properties has been attracting various critical applications such as molecule design, image editing and speech synthesis. Traditional hand-crafted approaches heavily rely on expertise experience and intensive human efforts, yet still suffer from the insufficiency of scientific knowledge and low throughput to support effective and efficient data generation. Recently, the advancement of deep learning has created the opportunity for expressive methods to learn the underlying representation and properties of data. Such capability provides new ways of determining the mutual relationship between the structural patterns and functional properties of the data and leveraging such relationships to generate structural data, given the desired properties. This article is a systematic review that explains this promising research area, commonly known as controllable deep data generation. First, the article raises the potential challenges and provides preliminaries. Then the article formally defines controllable deep data generation, proposes a taxonomy on various techniques and summarizes the evaluation metrics in this specific domain. After that, the article introduces exciting applications of controllable deep data generation, experimentally analyzes and compares existing works. Finally, this article highlights the promising future directions of controllable deep data generation and identifies five potential challenges.},
	number = {9},
	urldate = {2025-03-26},
	journal = {ACM Computing Surveys},
	author = {Wang, Shiyu and Du, Yuanqi and Guo, Xiaojie and Pan, Bo and Qin, Zhaohui and Zhao, Liang},
	month = oct,
	year = {2024},
	note = {arXiv:2207.09542 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {1--38},
}

@misc{ControllableDataGeneration,
	title = {Controllable {Data} {Generation} by {Deep} {Learning}: {A} {Review}},
	url = {https://arxiv.org/html/2207.09542},
	urldate = {2025-03-26},
}

@misc{ControllableDataGenerationa,
	title = {Controllable {Data} {Generation} by {Deep} {Learning}: {A} {Review}},
	url = {https://arxiv.org/html/2207.09542},
	urldate = {2025-03-26},
}

@inproceedings{torralbaUnbiasedLookDataset2011,
	title = {Unbiased look at dataset bias},
	url = {https://ieeexplore.ieee.org/abstract/document/5995347},
	doi = {10.1109/CVPR.2011.5995347},
	abstract = {Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.},
	urldate = {2025-03-26},
	booktitle = {{CVPR} 2011},
	author = {Torralba, Antonio and Efros, Alexei A.},
	month = jun,
	year = {2011},
	note = {ISSN: 1063-6919},
	keywords = {Support vector machines, Visualization, Testing, Training, Communities, Internet, Object recognition},
	pages = {1521--1528},
}

@article{schramowskiMakingDeepNeural2020,
	title = {Making deep neural networks right for the right scientific reasons by interacting with their explanations},
	volume = {2},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-020-0212-3},
	doi = {10.1038/s42256-020-0212-3},
	abstract = {Deep neural networks have demonstrated excellent performances in many real-world applications. Unfortunately, they may show Clever Hans-like behaviour (making use of confounding factors within datasets) to achieve high performance. In this work we introduce the novel learning setting of explanatory interactive learning and illustrate its benefits on a plant phenotyping research task. Explanatory interactive learning adds the scientist into the training loop, who interactively revises the original model by providing feedback on its explanations. Our experimental results demonstrate that explanatory interactive learning can help to avoid Clever Hans moments in machine learning and encourages (or discourages, if appropriate) trust in the underlying model.},
	language = {en},
	number = {8},
	urldate = {2025-03-26},
	journal = {Nature Machine Intelligence},
	author = {Schramowski, Patrick and Stammer, Wolfgang and Teso, Stefano and Brugger, Anna and Herbert, Franziska and Shao, Xiaoting and Luigs, Hans-Georg and Mahlein, Anne-Katrin and Kersting, Kristian},
	month = aug,
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	keywords = {Machine learning, Plant sciences},
	pages = {476--486},
}

@article{bockSpecialIssuePhytopathometry2022,
	title = {A special issue on phytopathometry — visual assessment, remote sensing, and artificial intelligence in the twenty-first century},
	volume = {47},
	issn = {1983-2052},
	url = {https://doi.org/10.1007/s40858-022-00498-w},
	doi = {10.1007/s40858-022-00498-w},
	language = {en},
	number = {1},
	urldate = {2025-03-26},
	journal = {Tropical Plant Pathology},
	author = {Bock, Clive H. and Barbedo, Jayme G. A. and Mahlein, Anne-Katrin and Del Ponte, Emerson M.},
	month = feb,
	year = {2022},
	pages = {1--4},
}

@book{goodfellowDeepLearning2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
}

@article{koldasbayevaChallengesDatadrivenGeospatial2024,
	title = {Challenges in data-driven geospatial modeling for environmental research and practice},
	volume = {15},
	copyright = {2024 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-024-55240-8},
	doi = {10.1038/s41467-024-55240-8},
	abstract = {Machine learning-based geospatial applications offer unique opportunities for environmental monitoring due to domains and scales adaptability and computational efficiency. However, the specificity of environmental data introduces biases in straightforward implementations. We identify a streamlined pipeline to enhance model accuracy, addressing issues like imbalanced data, spatial autocorrelation, prediction errors, and the nuances of model generalization and uncertainty estimation. We examine tools and techniques for overcoming these obstacles and provide insights into future geospatial AI developments. A big picture of the field is completed from advances in data processing in general, including the demands of industry-related solutions relevant to outcomes of applied sciences.},
	language = {en},
	number = {1},
	urldate = {2025-03-26},
	journal = {Nature Communications},
	author = {Koldasbayeva, Diana and Tregubova, Polina and Gasanov, Mikhail and Zaytsev, Alexey and Petrovskaia, Anna and Burnaev, Evgeny},
	month = dec,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Biogeography, Environmental impact},
	pages = {10700},
}

@misc{ronnebergerUNetConvolutionalNetworks2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	doi = {10.48550/arXiv.1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2025-03-26},
	publisher = {arXiv},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv:1505.04597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{He2016deep,
	title = {he2016deep},
	url = {https://www.bing.com/search?q=he2016deep&cvid=0410f7e82dca4d7c967613a4434fb341&gs_lcrp=EgRlZGdlKgYIABBFGDkyBggAEEUYOTIICAEQ6QcY_FXSAQczNzlqMGo5qAIIsAIB&FORM=ANAB01&PC=U531},
	abstract = {Intelligent search from Bing makes it easier to quickly find what you’re looking for and rewards you.},
	language = {en},
	urldate = {2025-03-26},
	journal = {Bing},
}

@article{kamilarisDeepLearningAgriculture2018,
	title = {Deep learning in agriculture: {A} survey},
	volume = {147},
	issn = {01681699},
	shorttitle = {Deep learning in agriculture},
	url = {http://arxiv.org/abs/1807.11809},
	doi = {10.1016/j.compag.2018.02.016},
	abstract = {Deep learning constitutes a recent, modern technique for image processing and data analysis, with promising results and large potential. As deep learning has been successfully applied in various domains, it has recently entered also the domain of agriculture. In this paper, we perform a survey of 40 research efforts that employ deep learning techniques, applied to various agricultural and food production challenges. We examine the particular agricultural problems under study, the specific models and frameworks employed, the sources, nature and pre-processing of data used, and the overall performance achieved according to the metrics used at each work under study. Moreover, we study comparisons of deep learning with other existing popular techniques, in respect to differences in classification or regression performance. Our findings indicate that deep learning provides high accuracy, outperforming existing commonly used image processing techniques.},
	urldate = {2025-03-26},
	journal = {Computers and Electronics in Agriculture},
	author = {Kamilaris, Andreas and Prenafeta-Boldu, Francesc X.},
	month = apr,
	year = {2018},
	note = {arXiv:1807.11809 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {70--90},
}

@article{luInstanceFusionRealtimeInstancelevel2020,
	title = {{InstanceFusion}: {Real}-time {Instance}-level {3D} {Reconstruction} {Using} a {Single} {RGBD} {Camera}},
	volume = {39},
	copyright = {© 2020 The Author(s) Computer Graphics Forum © 2020 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	shorttitle = {{InstanceFusion}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14157},
	doi = {10.1111/cgf.14157},
	abstract = {We present InstanceFusion, a robust real-time system to detect, segment, and reconstruct instance-level 3D objects of indoor scenes with a hand-held RGBD camera. It combines the strengths of deep learning and traditional SLAM techniques to produce visually compelling 3D semantic models. The key success comes from our novel segmentation scheme and the efficient instance-level data fusion, which are both implemented on GPU. Specifically, for each incoming RGBD frame, we take the advantages of the RGBD features, the 3D point cloud, and the reconstructed model to perform instance-level segmentation. The corresponding RGBD data along with the instance ID are then fused to the surfel-based models. In order to sufficiently store and update these data, we design and implement a new data structure using the OpenGL Shading Language. Experimental results show that our method advances the state-of-the-art (SOTA) methods in instance segmentation and data fusion by a big margin. In addition, our instance segmentation improves the precision of 3D reconstruction, especially in the loop closure. InstanceFusion system runs 20.5Hz on a consumer-level GPU, which supports a number of augmented reality (AR) applications (e.g., 3D model registration, virtual interaction, AR map) and robot applications (e.g., navigation, manipulation, grasping). To facilitate future research and reproduce our system more easily, the source code, data, and the trained model are released on Github: https://github.com/Fancomi2017/InstanceFusion.},
	language = {en},
	number = {7},
	urldate = {2025-03-26},
	journal = {Computer Graphics Forum},
	author = {Lu, Feixiang and Peng, Haotian and Wu, Hongyu and Yang, Jun and Yang, Xinhang and Cao, Ruizhi and Zhang, Liangjun and Yang, Ruigang and Zhou, Bin},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14157},
	keywords = {• Computing methodologies → Scene understanding, CCS Concepts, Perception, Vision for robotics},
	pages = {433--445},
}

@misc{GeneralizedLinearModels,
	title = {Generalized linear models by {P}. {McCullagh} {\textbar} {Open} {Library}},
	url = {https://openlibrary.org/books/OL1911874M/Generalized_linear_models},
	urldate = {2025-03-26},
}

@article{pandianjImprovedDeepResidual2022,
	title = {An {Improved} {Deep} {Residual} {Convolutional} {Neural} {Network} for {Plant} {Leaf} {Disease} {Detection}},
	volume = {2022},
	issn = {1687-5273},
	doi = {10.1155/2022/5102290},
	abstract = {In this research, we proposed a novel deep residual convolutional neural network with 197 layers (ResNet197) for the detection of various plant leaf diseases. Six blocks of layers were used to develop ResNet197. ResNet197 was trained and tested using a combined plant leaf disease image dataset. Scaling, cropping, flipping, padding, rotation, affine transformation, saturation, and hue transformation techniques were used to create the augmentation data of the plant leaf disease image dataset. The dataset consisted of 103 diseased and healthy image classes of 22 plants and 154,500 images of healthy and diseased plant leaves. The evolutionary search technique was used to optimise the layers and hyperparameter values of ResNet197. ResNet197 was trained on the combined plant leaf disease image dataset using a graphics processing unit (GPU) environment for 1000 epochs. It produced a 99.58 percentage average classification accuracy on the test dataset. The experimental results were superior to existing ResNet architectures and recent transfer learning techniques.},
	language = {eng},
	journal = {Computational Intelligence and Neuroscience},
	author = {Pandian J, Arun and K, Kanchanadevi and Rajalakshmi, N. R. and G Arulkumaran, null},
	year = {2022},
	pmid = {36156945},
	pmcid = {PMC9492343},
	keywords = {Neural Networks, Computer, Plant Leaves, Plant Diseases, Image Processing, Computer-Assisted, Rotation},
	pages = {5102290},
}

@article{bockVisualEstimatesFully2020,
	title = {From visual estimates to fully automated sensor-based measurements of plant disease severity: status and challenges for improving accuracy},
	volume = {2},
	issn = {2524-4167},
	shorttitle = {From visual estimates to fully automated sensor-based measurements of plant disease severity},
	url = {https://doi.org/10.1186/s42483-020-00049-8},
	doi = {10.1186/s42483-020-00049-8},
	abstract = {The severity of plant diseases, traditionally the proportion of the plant tissue exhibiting symptoms, is a key quantitative variable to know for many diseases and is prone to error. Good quality disease severity data should be accurate (close to the true value). Earliest quantification of disease severity was by visual estimates. Sensor-based image analysis including visible spectrum and hyperspectral and multispectral sensors are established technologies that promise to substitute, or complement visual ratings. Indeed, these technologies have measured disease severity accurately under controlled conditions but are yet to demonstrate their full potential for accurate measurement under field conditions. Sensor technology is advancing rapidly, and artificial intelligence may help overcome issues for automating severity measurement under hyper-variable field conditions. The adoption of appropriate scales, training, instruction and aids (standard area diagrams) has contributed to improved accuracy of visual estimates. The apogee of accuracy for visual estimation is likely being approached, and any remaining increases in accuracy are likely to be small. Due to automation and rapidity, sensor-based measurement offers potential advantages compared with visual estimates, but the latter will remain important for years to come. Mobile, automated sensor-based systems will become increasingly common in controlled conditions and, eventually, in the field for measuring plant disease severity for the purpose of research and decision making.},
	language = {en},
	number = {1},
	urldate = {2025-03-26},
	journal = {Phytopathology Research},
	author = {Bock, Clive H. and Barbedo, Jayme G. A. and Del Ponte, Emerson M. and Bohnenkamp, David and Mahlein, Anne-Katrin},
	month = apr,
	year = {2020},
	keywords = {Machine learning, Deep learning, Accuracy, Phenotyping, Artificial intelligence, Assessment, Digital technologies, Disease severity, Mobile device, Precision, Precision agriculture, Sensor},
	pages = {9},
}

@article{bockPlantDiseaseSeverity2010,
	title = {Plant {Disease} {Severity} {Estimated} {Visually}, by {Digital} {Photography} and {Image} {Analysis}, and by {Hyperspectral} {Imaging}},
	volume = {29},
	issn = {0735-2689},
	url = {https://doi.org/10.1080/07352681003617285},
	doi = {10.1080/07352681003617285},
	abstract = {Reliable, precise and accurate estimates of disease severity are important for predicting yield loss, monitoring and forecasting epidemics, for assessing crop germplasm for disease resistance, and for understanding fundamental biological processes including co-evolution. Disease assessments that are inaccurate and/or imprecise might lead to faulty conclusions being drawn from the data, which in turn can lead to incorrect actions being taken in disease management decisions. Plant disease can be quantified in several different ways. This review considers plant disease severity assessment at the scale of individual plant parts or plants, and describes our current understanding of the sources and causes of assessment error, a better understanding of which is required before improvements can be targeted. The review also considers how these can be identified using various statistical tools. Indeed, great strides have been made in the last thirty years in identifying the sources of assessment error inherent to visual rating, and this review highlights ways that assessment errors can be reduced—particularly by training raters or using assessment aids. Lesion number in relation to area infected is known to influence accuracy and precision of visual estimates—the greater the number of lesions for a given area infected results in more overestimation. Furthermore, there is a widespread tendency to overestimate disease severity at low severities ({\textless}10\%). Both interrater and intrarater reliability can be variable, particularly if training or rating aids are not used. During the last eighty years acceptable accuracy and precision of visual disease assessments have often been achieved using disease scales, particularly because of the time they allegedly save, and the ease with which they can be learned, but recent work suggests there can be some disadvantages to their use. This review considers new technologies that offer opportunity to assess disease with greater objectivity (reliability, precision, and accuracy). One of these, visible light photography and digital image analysis has been increasingly used over the last thirty years, as software has become more sophisticated and user-friendly. Indeed, some studies have produced very accurate estimates of disease using image analysis. In contrast, hyperspectral imagery is relatively recent and has not been widely applied in plant pathology. Nonetheless, it offers interesting and potentially discerning opportunities to assess disease. As plant disease assessment becomes better understood, it is against the backdrop of concepts of reliability, precision and accuracy (and agreement) in plant pathology and measurement science. This review briefly describes these concepts in relation to plant disease assessment. Various advantages and disadvantages of the different approaches to disease assessment are described. For each assessment method some future research priorities are identified that would be of value in better understanding the theory of disease assessment, as it applies to improving and fully realizing the potential of image analysis and hyperspectral imagery.},
	number = {2},
	urldate = {2025-03-26},
	journal = {Critical Reviews in Plant Sciences},
	author = {Bock, C. H. and , G. H., Poole and , P. E., Parker and and Gottwald, T. R.},
	month = mar,
	year = {2010},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/07352681003617285},
	keywords = {remote sensing, image analysis, error, hyperspectral imagery, plant disease assessment, variance},
	pages = {59--107},
}

@article{arnalbarbedoDigitalImageProcessing2013,
	title = {Digital image processing techniques for detecting, quantifying and classifying plant diseases},
	volume = {2},
	issn = {2193-1801},
	url = {https://doi.org/10.1186/2193-1801-2-660},
	doi = {10.1186/2193-1801-2-660},
	abstract = {This paper presents a survey on methods that use digital image processing techniques to detect, quantify and classify plant diseases from digital images in the visible spectrum. Although disease symptoms can manifest in any part of the plant, only methods that explore visible symptoms in leaves and stems were considered. This was done for two main reasons: to limit the length of the paper and because methods dealing with roots, seeds and fruits have some peculiarities that would warrant a specific survey. The selected proposals are divided into three classes according to their objective: detection, severity quantification, and classification. Each of those classes, in turn, are subdivided according to the main technical solution used in the algorithm. This paper is expected to be useful to researchers working both on vegetable pathology and pattern recognition, providing a comprehensive and accessible overview of this important field of research.},
	language = {en},
	number = {1},
	urldate = {2025-03-26},
	journal = {SpringerPlus},
	author = {Arnal Barbedo, Jayme Garcia},
	month = dec,
	year = {2013},
	keywords = {Diseased Region, Powdery Mildew, Radial Basis Function, Texture Feature},
	pages = {660},
}

@article{barbedoAutomaticMethodDetect2014,
	title = {An {Automatic} {Method} to {Detect} and {Measure} {Leaf} {Disease} {Symptoms} {Using} {Digital} {Image} {Processing}},
	volume = {98},
	issn = {0191-2917},
	url = {https://apsjournals.apsnet.org/doi/abs/10.1094/PDIS-03-14-0290-RE},
	doi = {10.1094/PDIS-03-14-0290-RE},
	abstract = {A method is presented to detect and quantify leaf symptoms using conventional color digital images. The method was designed to be completely automatic, eliminating the possibility of human error and reducing time taken to measure disease severity. The program is capable of dealing with images containing multiple leaves, further reducing the time taken. Accurate results are possible when the symptoms and leaf veins have similar color and shade characteristics. The algorithm is subject to one constraint: the background must be as close to white or black as possible. Tests showed that the method provided accurate estimates over a wide variety of conditions, being robust to variation in size, shape, and color of leaves; symptoms; and leaf veins. Low rates of false positives and false negatives occurred due to extrinsic factors such as issues with image capture and the use of extreme file compression ratios.},
	number = {12},
	urldate = {2025-03-26},
	journal = {Plant Disease},
	author = {Barbedo, Jayme Garcia Arnal},
	month = dec,
	year = {2014},
	note = {Publisher: Scientific Societies},
	pages = {1709--1716},
}

@article{bockPhytopathometryGlossaryTwentyfirst2022,
	title = {A phytopathometry glossary for the twenty-first century: towards consistency and precision in intra- and inter-disciplinary dialogues},
	volume = {47},
	issn = {1983-2052},
	shorttitle = {A phytopathometry glossary for the twenty-first century},
	url = {https://doi.org/10.1007/s40858-021-00454-0},
	doi = {10.1007/s40858-021-00454-0},
	abstract = {Phytopathometry can be defined as the branch of plant pathology (phytopathology) that is concerned with estimation or measurement of the amount of plant disease expressed by symptoms of disease or signs of a pathogen on a single or group of specimens. Phytopathometry is critical for many reasons, including analyzing yield loss due to disease, breeding for disease resistance, evaluating and comparing disease control methods, understanding coevolution, and studying disease epidemiology and pathogen ecology. Phytopathometry underpins all activities in plant pathology and extends into related disciplines, such as agronomy, horticulture, and plant breeding. Considering this central role, phytopathometry warrants status as a formally recognized branch of plant pathology. The glossary defines terms and concepts used in phytopathometry based on disease symptoms or visible pathogen structures and includes those terms commonly used in the visual estimation of disease severity and sensor-based methods of disease measurement. Relevant terms from the intersecting disciplines of measurement science, statistics, psychophysics, robotics, and artificial intelligence are also included. In particular, a new, broader definition is proposed for “disease severity,” and the terms “disease measurement” and “disease estimate” are specifically defined. It is hoped that the glossary contributes to a more unified cross-discipline approach to research in, and application of the tools available to phytopathometry.},
	language = {en},
	number = {1},
	urldate = {2025-03-26},
	journal = {Tropical Plant Pathology},
	author = {Bock, Clive H. and Pethybridge, Sarah J. and Barbedo, Jayme G. A. and Esker, Paul D. and Mahlein, Anne-Katrin and Del Ponte, Emerson M.},
	month = feb,
	year = {2022},
	keywords = {Accuracy, Reliability, Disease severity, Plant pathology, Sensors, Estimate, Imaging, Measurement, Visual},
	pages = {14--24},
}

@article{bockVisualEstimatesFully2020a,
	title = {From visual estimates to fully automated sensor-based measurements of plant disease severity: status and challenges for improving accuracy},
	volume = {2},
	issn = {2524-4167},
	shorttitle = {From visual estimates to fully automated sensor-based measurements of plant disease severity},
	url = {https://doi.org/10.1186/s42483-020-00049-8},
	doi = {10.1186/s42483-020-00049-8},
	abstract = {The severity of plant diseases, traditionally the proportion of the plant tissue exhibiting symptoms, is a key quantitative variable to know for many diseases and is prone to error. Good quality disease severity data should be accurate (close to the true value). Earliest quantification of disease severity was by visual estimates. Sensor-based image analysis including visible spectrum and hyperspectral and multispectral sensors are established technologies that promise to substitute, or complement visual ratings. Indeed, these technologies have measured disease severity accurately under controlled conditions but are yet to demonstrate their full potential for accurate measurement under field conditions. Sensor technology is advancing rapidly, and artificial intelligence may help overcome issues for automating severity measurement under hyper-variable field conditions. The adoption of appropriate scales, training, instruction and aids (standard area diagrams) has contributed to improved accuracy of visual estimates. The apogee of accuracy for visual estimation is likely being approached, and any remaining increases in accuracy are likely to be small. Due to automation and rapidity, sensor-based measurement offers potential advantages compared with visual estimates, but the latter will remain important for years to come. Mobile, automated sensor-based systems will become increasingly common in controlled conditions and, eventually, in the field for measuring plant disease severity for the purpose of research and decision making.},
	language = {en},
	number = {1},
	urldate = {2025-03-26},
	journal = {Phytopathology Research},
	author = {Bock, Clive H. and Barbedo, Jayme G. A. and Del Ponte, Emerson M. and Bohnenkamp, David and Mahlein, Anne-Katrin},
	month = apr,
	year = {2020},
	keywords = {Machine learning, Deep learning, Accuracy, Phenotyping, Artificial intelligence, Assessment, Digital technologies, Disease severity, Mobile device, Precision, Precision agriculture, Sensor},
	pages = {9},
}

@misc{MachineLearningApplications,
	title = {Machine {Learning} {Applications} in {Agriculture}: {Current} {Trends}, {Challenges}, and {Future} {Perspectives}},
	url = {https://www.mdpi.com/2073-4395/13/12/2976},
	urldate = {2025-03-26},
}

@incollection{kozaAutomatedDesignBoth1996,
	address = {Dordrecht},
	title = {Automated {Design} of {Both} the {Topology} and {Sizing} of {Analog} {Electrical} {Circuits} {Using} {Genetic} {Programming}},
	isbn = {978-94-009-0279-4},
	url = {https://doi.org/10.1007/978-94-009-0279-4_9},
	abstract = {This paper describes an automated process for designing analog electrical circuits based on the principles of natural selection, sexual recombination, and developmental biology. The design process starts with the random creation of a large population of program trees composed of circuit-constructing functions. Each program tree specifies the steps by which a fully developed circuit is to be progressively developed from a common embryonic circuit appropriate for the type of circuit that the user wishes to design. The fitness measure is a user-written computer program that may incorporate any calculable characteristic or combination of characteristics of the circuit. The population of program trees is genetically bred over a series of many generations using genetic programming. Genetic programming is driven by a fitness measure and employs genetic operations such as Darwinian reproduction, sexual recombination (crossover), and occasional mutation to create offspring. This automated evolutionary process produces both the topology of the circuit and the numerical values for each component. This paper describes how genetic programming can evolve the circuit for a difficult-to-design low-pass filter.},
	language = {en},
	urldate = {2025-03-26},
	booktitle = {Artificial {Intelligence} in {Design} ’96},
	publisher = {Springer Netherlands},
	author = {Koza, John R. and Bennett, Forrest H. and Andre, David and Keane, Martin A.},
	editor = {Gero, John S. and Sudweeks, Fay},
	year = {1996},
	doi = {10.1007/978-94-009-0279-4_9},
	pages = {151--170},
}

@book{hastieElementsStatisticalLearning2009,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-0-387-84857-0 978-0-387-84858-7},
	url = {http://link.springer.com/10.1007/978-0-387-84858-7},
	urldate = {2025-03-26},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2009},
	doi = {10.1007/978-0-387-84858-7},
	keywords = {Averaging, Boosting, classification, clustering, data mining, machine learning, Projection pursuit, Random Forest, supervised learning, Support Vector Machine, unsupervised learning},
}

@article{trevisanSpatialVariabilityCrop2021,
	title = {Spatial variability of crop responses to agronomic inputs in on-farm precision experimentation},
	volume = {22},
	issn = {1573-1618},
	url = {https://doi.org/10.1007/s11119-020-09720-8},
	doi = {10.1007/s11119-020-09720-8},
	abstract = {Within-field variability of crop yield levels has been extensively investigated, but the spatial variability of crop yield responses to agronomic treatments is less understood. On-farm precision experimentation (OFPE) can be a valuable tool for the estimation of in-field variation of optimal input rates and thus improve agronomic decisions. Therefore, the objectives of this study were to investigate the spatial variability of optimal input rates in OFPE and the potential economic benefit of site-specific input management. Mixed geographically weighted regression (GWR) models were used to estimate local yield response functions. The methodology was applied to investigate the spatial variability in corn response to nitrogen and seed rates in four cornfields in Illinois, USA. The results showed that spatial heterogeneity of model parameters was significant in all four fields evaluated. On average, the RMSE of the fitted yield decreased from 1.2 Mg ha−1 in the non-spatial global model to 0.7 Mg ha−1 in the GWR model, and the r-squared increased from 10 to 68\%. The average potential gain of using optimized uniform rates of seed and nitrogen was US\$ 65.00 ha−1, while the added potential gain of the site-specific application was US\$ 58.00 ha−1. The combination of OFPE and GWR proved to be an effective tool for testing precision agriculture’s central hypothesis of whether optimal input application rates display adequate spatial variability to justify the costs of the variable rate technology itself. The reported results encourage more research on response-based input management recommendations instead of the still widespread focus on yield-based algorithms.},
	language = {en},
	number = {2},
	urldate = {2025-03-29},
	journal = {Precision Agriculture},
	author = {Trevisan, R. G. and Bullock, D. S. and Martin, N. F.},
	month = apr,
	year = {2021},
	keywords = {Nitrogen, Geographically weighted regression, Seed, Variable rate application, Yield response functions, Zea mays L.},
	pages = {342--363},
}

@article{jinEfficientGeostatisticalAnalysis2021,
	title = {An efficient geostatistical analysis tool for on-farm experiments targeted at localised treatment},
	volume = {205},
	issn = {1537-5110},
	url = {https://www.sciencedirect.com/science/article/pii/S1537511021000453},
	doi = {10.1016/j.biosystemseng.2021.02.009},
	abstract = {On farm experimentation (OFE) has been a long-standing method for farmers to assess alternative management at scales relevant to their farming practices. Through the use of spatially distributed designs, whether simple strips or other ‘whole-of-block’ trials, OFE can provide information such as which treatment should be recommended at specific locations, and make important contributions to precision agriculture. However, when treatment response data sets become large, such as with tens of thousands of field observations that are readily collected using on-the-go sensors, existing geostatistical systems for analysing such experiments become computationally intensive, if not impossible. To enable farmers, or their consultants, to generate high-resolution treatment response and recommendation maps on their own computers within a reasonable time, we present a fast and adaptive local cokriging tool for non-colocated and non-stationary OFE data. It uses a spatially-varying neighbourhood radius. It has a graphical user interface accessible via QGIS, a free and open source software. The adaptive local cokriging is demonstrated on three OFE examples. It performs indistinguishably from global cokriging on a small data set, but for large data sets, for which global cokriging is impractical, it predicts significantly more accurately than spatial splines or sampling-based cokriging. It outperforms cokriging base on a fixed number of nearest neighbours when this fixed number is not carefully chosen.},
	urldate = {2025-03-29},
	journal = {Biosystems Engineering},
	author = {Jin, Huidong and Shuvo Bakar, K. and Henderson, Brent L. and Bramley, Robert G. V. and Gobbett, David L.},
	month = may,
	year = {2021},
	keywords = {Precision agriculture, Cokriging, Multivariate analysis tool, On-farm experimentation},
	pages = {121--136},
}

@misc{PDFContributionGeostatistics,
	title = {({PDF}) {The} {Contribution} of {Geostatistics} to {Precision} {Agriculture}},
	url = {https://www.researchgate.net/publication/311307357_The_Contribution_of_Geostatistics_to_Precision_Agriculture},
	abstract = {PDF {\textbar} On Nov 22, 2016, Gabriele Buttafuoco and others published The Contribution of Geostatistics to Precision Agriculture {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2025-03-29},
	journal = {ResearchGate},
}

@article{castrignanoGeostatisticalApproachModelling2017,
	title = {A geostatistical approach for modelling and combining spatial data with different support},
	volume = {8},
	issn = {2040-4700},
	url = {https://www.sciencedirect.com/science/article/pii/S2040470017000048},
	doi = {10.1017/S2040470017000048},
	number = {2},
	urldate = {2025-03-29},
	journal = {Advances in Animal Biosciences},
	author = {Castrignanò, A. and Quarto, R. and Venezia, A. and Buttafuoco, G.},
	month = jan,
	year = {2017},
	pages = {594--599},
}

@article{puntelLeveragingDigitalAgriculture2024,
	title = {Leveraging digital agriculture for on-farm testing of technologies},
	volume = {6},
	issn = {2673-3218},
	url = {https://www.frontiersin.org/journals/agronomy/articles/10.3389/fagro.2024.1234232/full},
	doi = {10.3389/fagro.2024.1234232},
	abstract = {{\textless}p{\textgreater}The Precision Nitrogen Project (PNP) worked with more than 80 corn and winter wheat producers to inexpensively design and implement randomized, replicated field strip trials on whole commercial farm fields, and to provide site-specific testing of current nitrogen (N) technologies. This article proposes a conceptual framework and detailed procedure to select the N technology to be tested; design and implement field trials; generate, process, and manage field trial data; and automatically analyze, report, and share benefits from precision N technology. The selection of the N technology was farmer-driven to ensure a good fit and to increase the likelihood of future technology adoption. The technology selection method was called the “N tiered approach”, which consisted of selecting a technology that progressively increases the level of complexity without exceeding the farmer’s learning process or farm logistic constraints. The N tools were classified into (1) crop model-based, (2) remote sensing-based, (3) enhanced efficiency fertilizers, and (4) biologicals. Field strip trials comparing producers’ traditional management and the selected N technology were combined with site-specific N rate blocks placed in contrasting areas of the fields. Yield data from the N rate blocks was utilized to derive the site-specific optimal N rate. The benefits of current N technologies were quantified by comparing their yield, profit, and N use efficiency (NUE) to growers’ traditional management and to the estimated site-specific optimal N rate. Communication of the trial results back to the growers was crucial to ensure the promotion and adoption of these N technologies farm wide. The framework and overall benefits from N technologies was presented and discussed. The proposed framework allowed researchers, agronomists, and farmers to carry out on-farm precision N experimentation using novel technologies to quantify benefits of digital ag technology and promote adoption.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-03-29},
	journal = {Frontiers in Agronomy},
	author = {Puntel, Laila A. and Thompson, Laura J. and Mieno, Taro},
	month = mar,
	year = {2024},
	note = {Publisher: Frontiers},
	keywords = {Nitrogen, Data driven, Digital technology, On farm, Trial design},
}

@article{bullockDataIntensiveFarmManagement2019,
	title = {The {Data}-{Intensive} {Farm} {Management} {Project}: {Changing} {Agronomic} {Research} {Through} {On}-{Farm} {Precision} {Experimentation}},
	volume = {111},
	copyright = {© 2019 The author(s).},
	issn = {1435-0645},
	shorttitle = {The {Data}-{Intensive} {Farm} {Management} {Project}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.2134/agronj2019.03.0165},
	doi = {10.2134/agronj2019.03.0165},
	abstract = {The Data-Intensive Farm Management (DIFM) project works with participating farmers, using precision technology to inexpensively design and run randomized agronomic field trials on whole commercial farm fields, to provide data-based, site-specific farm input management guidance, thus providing economic and environmental benefits. This article lays out a conceptual framework used by the multidisciplinary DIFM research team to facilitate collaboration and then presents details of DIFM's procedures for what it calls on-farm precision experimentation (OFPE), which includes field trial design and implementation, data generation, processing, and management, and analysis. It is argued that DIFM's data and the agricultural “Big Data” currently being collected with remote and proximal sensors are complementary; that is, more of either increases the value of the other. In 2019, DIFM and affiliates conducted over 120 trials, ranging from 10 to 100 ha in size, on maize, wheat, soybeans, cotton, and barley in eight US states, Argentina, Brazil, and South Africa. The DIFM is developing cyberinfrastructure to “scale up” its activities, to permit researchers and crop consultants worldwide to work with farmers to conduct trials, then process and manage the data. In Addition, DIFM is in the early stages of developing a software system for semi-automatic data analytics, and a cloud-based farm management aid, the purpose of which is to facilitate conversations between agronomists and farmers about implementing data-driven input management decisions. The proposed framework allows researchers, agronomists, and farmers to carry out on-farm precision experimentation using novel digital tools. Core Ideas The Data-Intensive Farm Management project's on-farm trials can generate massive amounts varied managed input data. The Data-Intensive Farm Management project's data fill a gap in agricultural “Big Data,” to enable data-intensive crop management. The Data-Intensive Farm Management project's protocols support trial design, data processing and analysis. The Data-Intensive Farm Management project can be implemented by researchers, consultants, and farmers in diverse agronomic scenarios.},
	language = {en},
	number = {6},
	urldate = {2025-03-29},
	journal = {Agronomy Journal},
	author = {Bullock, David S. and Boerngen, Maria and Tao, Haiying and Maxwell, Bruce and Luck, Joe D. and Shiratsuchi, Luciano and Puntel, Laila and Martin, Nicolas F.},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.2134/agronj2019.03.0165},
	pages = {2736--2746},
}

@article{rodriguez-alvarezFastSmoothingParameter2015,
	title = {Fast smoothing parameter separation in multidimensional generalized {P}-splines: the {SAP} algorithm},
	volume = {25},
	issn = {1573-1375},
	shorttitle = {Fast smoothing parameter separation in multidimensional generalized {P}-splines},
	url = {https://doi.org/10.1007/s11222-014-9464-2},
	doi = {10.1007/s11222-014-9464-2},
	abstract = {A new computational algorithm for estimating the smoothing parameters of a multidimensional penalized spline generalized linear model with anisotropic penalty is presented. This new proposal is based on the mixed model representation of a multidimensional P-spline, in which the smoothing parameter for each covariate is expressed in terms of variance components. On the basis of penalized quasi-likelihood methods, closed-form expressions for the estimates of the variance components are obtained. This formulation leads to an efficient implementation that considerably reduces the computational burden. The proposed algorithm can be seen as a generalization of the algorithm by Schall (1991)—for variance components estimation—to deal with non-standard structures of the covariance matrix of the random effects. The practical performance of the proposed algorithm is evaluated by means of simulations, and comparisons with alternative methods are made on the basis of the mean square error criterion and the computing time. Finally, we illustrate our proposal with the analysis of two real datasets: a two dimensional example of historical records of monthly precipitation data in USA and a three dimensional one of mortality data from respiratory disease according to the age at death, the year of death and the month of death.},
	language = {en},
	number = {5},
	urldate = {2025-03-29},
	journal = {Statistics and Computing},
	author = {Rodríguez-Álvarez, María Xosé and Lee, Dae-Jin and Kneib, Thomas and Durbán, María and Eilers, Paul},
	month = sep,
	year = {2015},
	keywords = {Artificial Intelligence, Mixed models, Anisotropic penalty, P-splines, Smoothing, Tensor product},
	pages = {941--957},
}

@article{leeEfficientTwodimensionalSmoothing2013,
	title = {Efficient two-dimensional smoothing with {P}{\textless}math{\textgreater}{\textless}mi is="true"{\textgreater}{P}{\textless}/mi{\textgreater}{\textless}/math{\textgreater}-spline {ANOVA} mixed models and nested bases},
	volume = {61},
	issn = {0167-9473},
	url = {https://www.sciencedirect.com/science/article/pii/S016794731200415X},
	doi = {10.1016/j.csda.2012.11.013},
	abstract = {Low-rank smoothing techniques have gained much popularity in non-standard regression modeling. In particular, penalized splines and tensor product smooths are used as flexible tools to study non-parametric relationships among several covariates. The use of standard statistical software facilitates their use for several types of problems and applications. However, when interaction terms are considered in the modeling, and multiple smoothing parameters need to be estimated standard software does not work well when datasets are large or higher-order interactions are included or need to be tested. In this paper, a general approach for constructing and estimating bivariate smooth models for additive and interaction terms using penalized splines is proposed. The formulation is based on the mixed model representation of the smooth-ANOVA model by Lee and Durbán (in press), and several nested models in terms of random effects components are proposed. Each component has a clear interpretation in terms of function shape and model identifiability constraints. The term PS-ANOVA is coined for this type of models. The estimation method is relatively straightforward based on the algorithm by Schall (1991) for generalized linear mixed models. Further, a simplification of the smooth interaction term is used by constructing lower-rank basis (nested basis). Finally, some simulation studies and real data examples are presented to evaluate the new model and the estimation method.},
	urldate = {2025-03-29},
	journal = {Computational Statistics \& Data Analysis},
	author = {Lee, Dae-Jin and Durbán, María and Eilers, Paul},
	month = may,
	year = {2013},
	keywords = {Mixed models, Penalized splines, Schall’s algorithm, Smooth-ANOVA decomposition},
	pages = {22--37},
}

@article{gilmourAccountingNaturalExtraneous1997,
	title = {Accounting for {Natural} and {Extraneous} {Variation} in the {Analysis} of {Field} {Experiments}},
	volume = {2},
	issn = {1085-7117},
	url = {https://www.jstor.org/stable/1400446},
	doi = {10.2307/1400446},
	abstract = {We identify three major components of spatial variation in plot errors from field experiments and extend the two-dimensional spatial procedures of Cullis and Gleeson (1991) to account for them. The components are nonstationary, large-scale (global) variation across the field, stationary variation within the trial (natural variation or local trend), and extraneous variation that is often induced by experimental procedures and is predominantly aligned with rows and columns. We present a strategy for identifying a model for the plot errors that uses a trellis plot of residuals, a perspective plot of the sample variogram and, where possible, likelihood ratio tests to identify which components are present. We demonstrate the strategy using two illustrative examples. We conclude that although there is no one model that adequately fits all field experiments, the separable autoregressive model is dominant. However, there is often additional identifiable variation present.},
	number = {3},
	urldate = {2025-03-29},
	journal = {Journal of Agricultural, Biological, and Environmental Statistics},
	author = {Gilmour, Arthur R. and Cullis, Brian R. and Verbyla, Arūnas P.},
	year = {1997},
	note = {Publisher: [International Biometric Society, Springer]},
	pages = {269--293},
}

@article{rodriguez-alvarezCorrectingSpatialHeterogeneity2018,
	title = {Correcting for spatial heterogeneity in plant breeding experiments with {P}-splines},
	volume = {23},
	issn = {2211-6753},
	url = {https://www.sciencedirect.com/science/article/pii/S2211675317301070},
	doi = {10.1016/j.spasta.2017.10.003},
	abstract = {An important aim of the analysis of agricultural field experiments is to obtain good predictions for genotypic performance, by correcting for spatial effects. In practice these corrections turn out to be complicated, since there can be different types of spatial effects; those due to management interventions applied to the field plots and those due to various kinds of erratic spatial trends. This paper explores the use of two-dimensional smooth surfaces to model random spatial variation. We propose the use of anisotropic tensor product P-splines to explicitly model large-scale (global trend) and small-scale (local trend) spatial dependence. On top of this spatial field, effects of genotypes, blocks, replicates, and/or other sources of spatial variation are described by a mixed model in a standard way. Each component in the model is shown to have an effective dimension. They are closely related to variance estimation, and helpful for characterising the importance of model components. An important result of this paper is the formal proof of the relation between several definitions of heritability and the effective dimension associated with the genetic component. The practical value of our approach is illustrated by simulations and analyses of large-scale plant breeding experiments. An R-package, SpATS, is provided.},
	urldate = {2025-03-29},
	journal = {Spatial Statistics},
	author = {Rodríguez-Álvarez, María Xosé and Boer, Martin P. and van Eeuwijk, Fred A. and Eilers, Paul H. C.},
	month = mar,
	year = {2018},
	keywords = {Effective dimension, Field trials, Heritability, Linear mixed model, Spatial analysis, Tensor product P-splines},
	pages = {52--71},
}

@book{oliverGeostatisticalApplicationsPrecision2010,
	address = {Dordrecht},
	title = {Geostatistical {Applications} for {Precision} {Agriculture}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-90-481-9132-1 978-90-481-9133-8},
	url = {http://link.springer.com/10.1007/978-90-481-9133-8},
	language = {en},
	urldate = {2025-03-29},
	publisher = {Springer Netherlands},
	editor = {Oliver, M.A.},
	year = {2010},
	doi = {10.1007/978-90-481-9133-8},
	keywords = {geostatistics, kriging, paper, Precision Agriculture, simulation, Site-specific Management, Variogram},
}

@book{websterGeostatisticsEnvironmentalScientists2007,
	title = {Geostatistics for {Environmental} {Scientists}},
	isbn = {978-0-470-51726-0},
	abstract = {Geostatistics is essential for environmental scientists. Weather and climate vary from place to place, soil varies at every scale at which it is examined, and even man-made attributes – such as the distribution of pollution – vary. The techniques used in geostatistics are ideally suited to the needs of environmental scientists, who use them to make the best of sparse data for prediction, and top plan future surveys when resources are limited. Geostatistical technology has advanced much in the last few years and many of these developments are being incorporated into the practitioner’s repertoire. This second edition describes these techniques for environmental scientists. Topics such as stochastic simulation, sampling, data screening, spatial covariances, the variogram and its modeling, and spatial prediction by kriging are described in rich detail. At each stage the underlying theory is fully explained, and the rationale behind the choices given, allowing the reader to appreciate the assumptions and constraints involved.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Webster, Richard and Oliver, Margaret A.},
	month = oct,
	year = {2007},
	note = {Google-Books-ID: WBwSyvIvNY8C},
	keywords = {Mathematics / General, Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes},
}

@book{cressieStatisticsSpatialData2015,
	title = {Statistics for {Spatial} {Data}},
	isbn = {978-1-119-11518-2},
	abstract = {The Wiley Classics Library consists of selected books that have been made more accessible to consumers in an effort to increase global appeal and general circulation. With these new unabridged softcover volumes, Wiley hopes to extend the lives of these works by making them available to future generations of statisticians, mathematicians, and scientists. Spatial statistics — analyzing spatial data through statistical models — has proven exceptionally versatile, encompassing problems ranging from the microscopic to the astronomic. However, for the scientist and engineer faced only with scattered and uneven treatments of the subject in the scientific literature, learning how to make practical use of spatial statistics in day-to-day analytical work is very difficult. Designed exclusively for scientists eager to tap into the enormous potential of this analytical tool and upgrade their range of technical skills, Statistics for Spatial Data is a comprehensive, single-source guide to both the theory and applied aspects of spatial statistical methods. The hard-cover edition was hailed by Mathematical Reviews as an "excellent book which will become a basic reference." This paper-back edition of the 1993 edition, is designed to meet the many technological challenges facing the scientist and engineer. Concentrating on the three areas of geostatistical data, lattice data, and point patterns, the book sheds light on the link between data and model, revealing how design, inference, and diagnostics are an outgrowth of that link. It then explores new methods to reveal just how spatial statistical models can be used to solve important problems in a host of areas in science and engineering. Discussion includes:  Exploratory spatial data analysis Spectral theory for stationary processes Spatial scale Simulation methods for spatial processes Spatial bootstrapping Statistical image analysis and remote sensing Computational aspects of model fitting Application of models to disease mapping  Designed to accommodate the practical needs of the professional, it features a unified and common notation for its subject as well as many detailed examples woven into the text, numerous illustrations (including graphs that illuminate the theory discussed) and over 1,000 references. Fully balancing theory with applications, Statistics for Spatial Data, Revised Edition is an exceptionally clear guide on making optimal use of one of the ascendant analytical tools of the decade, one that has begun to capture the imagination of professionals in biology, earth science, civil, electrical, and agricultural engineering, geography, epidemiology, and ecology.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Cressie, Noel},
	month = mar,
	year = {2015},
	note = {Google-Books-ID: MzN\_BwAAQBAJ},
	keywords = {Mathematics / General, Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes},
}

@book{goovaertsGeostatisticsNaturalResources1997,
	title = {Geostatistics for {Natural} {Resources} {Evaluation}},
	isbn = {978-0-19-511538-3},
	abstract = {This text fulfills a need for an advanced-level work covering both the theory and application of geostatistics. It covers the most important areas of geostatistical methodology, introducing tools for description, quantitative modeling of spatial continuity, spatial prediction, and assessment of local uncertainty and stochastic simulation. It also details the theoretical background underlying most GSLIB programs. The tools are applied to an environmental data set, but the book includes a general presentation of algorithms intended for students and practitioners in such diverse fields as soil science, mining, petroleum, remote sensing, hydrogeology, and the environmental sciences.},
	language = {en},
	publisher = {Oxford University Press},
	author = {Goovaerts, Pierre},
	year = {1997},
	keywords = {Mathematics / Applied, Science / Earth Sciences / Geology, Science / Earth Sciences / Hydrology, Science / Life Sciences / Botany, Science / Life Sciences / Ecology, Social Science / Statistics, Technology \& Engineering / Remote Sensing \& Geographic Information Systems},
}

@article{matheronPrinciplesGeostatistics1963,
	title = {Principles of geostatistics},
	volume = {58},
	issn = {0013-0109},
	url = {https://scispace.com/papers/principles-of-geostatistics-2evlrmai2z},
	doi = {10.2113/GSECONGEO.58.8.1246},
	abstract = {Knowledge of ore grades and ore reserves as well as error estimation of these values, is fundamental for mining engineers and mining geologists. Until now no appropriate scientific approach to those estimation problems has existed: geostatistics, the principles of which are summarized in this paper, constitutes a new science leading to such an approach. The author criticizes classical statistical methods still in use, and shows some of the main results given by geostatistics. Any ore deposit evaluation as well as proper decision of starting mining operations should be preceded by a geostatistical investigation which may avoid economic failures.},
	language = {en},
	number = {8},
	urldate = {2025-03-29},
	journal = {Economic Geology},
	author = {{Matheron}},
	month = dec,
	year = {1963},
	note = {Publisher: GeoScienceWorld},
	pages = {1246--1266},
}

@book{journelMiningGeostatistics2003,
	title = {Mining {Geostatistics}},
	isbn = {978-1-930665-91-0},
	abstract = {First published in 1978, this book was the first complete reference work on the subject of mining geostatistics, an attempt to synthesize the practical experience gained by researchers from the Centre de Morphologie Mathematique in France and by mining engineers and geologists all over the world who contributed their ideas. It was designed for students and engineers who wished to apply geostatistics to practical problems occurring in the lifetime of a mine and for this reason was built around typical problems, progressing from the simplest to the most complicated: structural analysis, guiding exploration, estimation of in situ resources and recoverable reserves, numerical models of deposits, simulation of mining and homogenization processes, ore grade control in production. The techniques developed are illustrated by a large number of case studies and, as an aid to the reader, each chapter begins with a summary of the contents and there is a guide to the notation used. "The book is a practical treatise, written by practicing mining engineers and intended for other practicing engineers . the best summary of geostatistical theory as it stands at the present time and as one of the standard reference texts for the next few years." Mining Magazine "This is the book for which so many of us have been waiting: a practical, authoritative and scholarly work on geostatistics applied to mining. It is well written, well illustrated and usable both as a textbook for advanced students of mining geology and as a reference book for professionals.. Altogether a good book, the best available on the subject." C. J. Dixon in IMM Bulletin},
	language = {en},
	publisher = {Blackburn Press},
	author = {Journel, A. G. and Journel, Andre G. and Huijbregts, Ch J.},
	year = {2003},
	note = {Google-Books-ID: Id1GAAAAYAAJ},
	keywords = {Science / Earth Sciences / Geology, Technology \& Engineering / Mining},
}

@article{mullerGeostatisticalModellingPython2022,
	title = {Geostatistical modelling in {Python}},
	volume = {15},
	issn = {1991-959X},
	shorttitle = {{GSTools} v1.3},
	url = {https://gmd.copernicus.org/articles/15/3161/2022/},
	doi = {10.5194/gmd-15-3161-2022},
	abstract = {Geostatistics as a subfield of statistics accounts for the spatial correlations encountered in many applications of, for example, earth sciences. Valuable information can be extracted from these correlations, also helping to address the often encountered burden of data scarcity. Despite the value of additional data, the use of geostatistics still falls short of its potential. This problem is often connected to the lack of user-friendly software hampering the use and application of geostatistics. We therefore present GSTools, a Python-based software suite for solving a wide range of geostatistical problems. We chose Python due to its unique balance between usability, flexibility, and efficiency and due to its adoption in the scientific community. GSTools provides methods for generating random fields; it can perform kriging, variogram estimation and much more. We demonstrate its abilities by virtue of a series of example applications detailing their use.},
	language = {English},
	number = {7},
	urldate = {2025-03-29},
	journal = {Geoscientific Model Development},
	author = {Müller, Sebastian and Schüler, Lennart and Zech, Alraune and Heße, Falk},
	month = apr,
	year = {2022},
	note = {Publisher: Copernicus GmbH},
	pages = {3161--3182},
}

@article{larkOptimizedSpatialSampling2002,
	title = {Optimized spatial sampling of soil for estimation of the variogram by maximum likelihood},
	volume = {105},
	issn = {0016-7061},
	url = {https://www.sciencedirect.com/science/article/pii/S0016706101000921},
	doi = {10.1016/S0016-7061(01)00092-1},
	abstract = {Recent studies have attempted to optimize the configuration of sample sites for estimation of the variogram by the usual method-of-moments. This paper shows that objective functions can readily be defined for estimation by the method of maximum likelihood. In both cases an objective function can only be defined for a specified variogram so some prior knowledge about the spatial variation of the property of interest is necessary. This paper describes the principles of the method, using Spatial Simulated Annealing for optimization, and applies optimized sample designs to simulated data. For practical applications it seems that the most fruitful way of using the technique is for supplementing simple systematic designs that provide an initial estimate of the variogram.},
	number = {1},
	urldate = {2025-03-29},
	journal = {Geoderma},
	author = {Lark, R. M},
	month = jan,
	year = {2002},
	keywords = {Geostatistics, Optimization, Soil sampling, Spatial variation},
	pages = {49--80},
}

@article{minasnyEfficiencyVariousApproaches2002,
	title = {The efficiency of various approaches to obtaining estimates of soil hydraulic properties},
	volume = {107},
	issn = {0016-7061},
	url = {https://www.sciencedirect.com/science/article/pii/S0016706101001380},
	doi = {10.1016/S0016-7061(01)00138-0},
	abstract = {A formal analysis was carried out to evaluate the efficiency of the different methods in predicting water retention and hydraulic conductivity. Efficiency can be defined in terms of effort, cost or value of information. The analysis identified the contribution of individual sources of measurement errors to the overall uncertainty. The value of information summarises the quality of the prediction, the cost of information, the application of predicted hydraulic properties, and the effect of spatial variability. For single measurements, the inverse disc-permeameter analysis is economically more efficient than using pedotransfer functions or measuring hydraulic properties in the laboratory. However, given the large amount of spatial variation of soil hydraulic properties it is found that lots of cheap and imprecise measurements, e.g. by hand texturing, are more efficient than a few expensive precise ones.},
	number = {1},
	urldate = {2025-03-29},
	journal = {Geoderma},
	author = {Minasny, Budiman and McBratney, Alex B},
	month = may,
	year = {2002},
	keywords = {Hydraulic conductivity, Inverse problem, Soil–water balance, Uncertainty analysis},
	pages = {55--70},
}

@article{wadouxSamplingDesignOptimization2019,
	title = {Sampling design optimization for soil mapping with random forest},
	volume = {355},
	issn = {0016-7061},
	url = {https://www.sciencedirect.com/science/article/pii/S0016706119306792},
	doi = {10.1016/j.geoderma.2019.113913},
	abstract = {Machine learning techniques are widely employed to generate digital soil maps. The map accuracy is partly determined by the number and spatial locations of the measurements used to calibrate the machine learning model. However, determining the optimal sampling design for mapping with machine learning techniques has not yet been considered in detail in digital soil mapping studies. In this paper, we investigate sampling design optimization for soil mapping with random forest. A design is optimized using spatial simulated annealing by minimizing the mean squared prediction error (MSE). We applied this approach to mapping soil organic carbon for a part of Europe using subsamples of the LUCAS dataset. The optimized subsamples are used as input for the random forest machine learning model, using a large set of readily available environmental data as covariates. We also predicted the same soil property using subsamples selected by simple random sampling, conditioned Latin Hypercube sampling (cLHS), spatial coverage sampling and feature space coverage sampling. Distributions of the estimated population MSEs are obtained through repeated random splitting of the LUCAS dataset, serving as the population of interest, into subsets used for validation, testing and selection of calibration samples, and repeated selection of calibration samples with the various sampling designs. The differences between the medians of the MSE distributions were tested for significance using the non-parametric Mann-Whitney test. The process was repeated for different sample sizes. We also analyzed the spread of the optimized designs in both geographic and feature space to reveal their characteristics. Results show that optimization of the sampling design by minimizing the MSE is worthwhile for small sample sizes. However, an important disadvantage of sampling design optimization using MSE is that it requires known values of the soil property at all locations and as a consequence is only feasible for subsampling an existing dataset. For larger sample sizes, the effect of using an MSE optimized design diminishes. In this case, we recommend to use a sample spread uniformly in the feature (i.e. covariate) space of the most important random forest covariates. The results also show that for our case study, cLHS sampling performs worse than the other sampling designs for mapping with random forest. We stress that comparison of sampling designs for calibration by splitting the data just once is very sensitive to the data split that one happens to use if the validation set is small.},
	urldate = {2025-03-29},
	journal = {Geoderma},
	author = {Wadoux, Alexandre M. J-C. and Brus, Dick J. and Heuvelink, Gerard B. M.},
	month = dec,
	year = {2019},
	keywords = {-means, Conditioned Latin Hypercube, LUCAS, Optimal design, Pedometrics, Random forest, Spatial coverage, Spatial simulated annealing, Uncertainty assessment},
	pages = {113913},
}

@article{tomislavhenglPracticalGuideGeostatistical2007,
	title = {A {Practical} {Guide} to {Geostatistical}  {Mapping} of {Environmental} {Variables}},
	issn = {1018-5593},
	url = {https://esdac.jrc.ec.europa.eu/Esdb_Archive/eusoils_docs/other/EUR22904EN.pdf},
	urldate = {2025-03-29},
	author = {{Tomislav Hengl}},
	month = sep,
	year = {2007},
}

@book{henglPracticalGuideGeostatistical2007,
	title = {A {Practical} {Guide} to {Geostatistical} {Mapping} of {Environmental} {Variables}},
	isbn = {978-92-79-06904-8},
	abstract = {Geostatistical mapping can be defined as analytical production of maps by using field observations, auxiliary information and a computer program that calculates values at locations of interest. Today, increasingly the heart of a mapping project is, in fact, the computer program that implements some (geo)statistical algorithm to a given point data set. Purpose of this guide is to assist you in producing quality maps by using fully-operational tools, without a need for serious additional investments. It will first introduce you the to the basic principles of geostatistical mapping and regression-kriging, as the key prediction technique, then it will guide you through four software packages: ILWIS GIS, R+gstat, SAGA GIS and Google Earth, which will be used to prepare the data, run analysis and make final layouts. These materials have been used for the five-days advanced training course "Hands-on-geostatistics: merging GIS and spatial statistics", that is regularly organized by the author and collaborators. Visit the course website to obtain a copy of the datasets used in this exercise. [Résumé de l'auteur].},
	language = {en},
	publisher = {European commission. Joint research centre. Institute for environment and sustainability (Ispra, Italie)},
	author = {Hengl, Tomislav},
	year = {2007},
	note = {Google-Books-ID: HdNEzQEACAAJ},
}

@article{mahleinPlantDiseaseDetection2016,
	title = {Plant {Disease} {Detection} by {Imaging} {Sensors} – {Parallels} and {Specific} {Demands} for {Precision} {Agriculture} and {Plant} {Phenotyping}},
	volume = {100},
	issn = {0191-2917},
	url = {https://apsjournals.apsnet.org/doi/10.1094/PDIS-03-15-0340-FE},
	doi = {10.1094/PDIS-03-15-0340-FE},
	abstract = {Early and accurate detection and diagnosis of plant diseases are key factors in plant production and the reduction of both qualitative and quantitative losses in crop yield. Optical techniques, such as RGB imaging, multi- and hyperspectral sensors, thermography, or chlorophyll fluorescence, have proven their potential in automated, objective, and reproducible detection systems for the identification and quantification of plant diseases at early time points in epidemics. Recently, 3D scanning has also been added as an optical analysis that supplies additional information on crop plant vitality. Different platforms from proximal to remote sensing are available for multiscale monitoring of single crop organs or entire fields. Accurate and reliable detection of diseases is facilitated by highly sophisticated and innovative methods of data analysis that lead to new insights derived from sensor data for complex plant-pathogen systems. Nondestructive, sensor-based methods support and expand upon visual and/or molecular approaches to plant disease assessment. The most relevant areas of application of sensor-based analyses are precision agriculture and plant phenotyping.},
	number = {2},
	urldate = {2025-03-29},
	journal = {Plant Disease},
	author = {Mahlein, Anne-Katrin},
	month = feb,
	year = {2016},
	note = {Publisher: Scientific Societies},
	pages = {241--251},
}

@article{wanOptimizingUAVbasedUncooled2024,
	title = {Optimizing {UAV}-based uncooled thermal cameras in field conditions for precision agriculture},
	volume = {134},
	issn = {1569-8432},
	url = {https://research.wur.nl/en/publications/optimizing-uav-based-uncooled-thermal-cameras-in-field-conditions},
	doi = {10.1016/j.jag.2024.104184},
	language = {English},
	urldate = {2025-03-29},
	journal = {International Journal of applied Earth Observation and Geoinformation},
	author = {Wan, Quanxing and Smigaj, Magdalena and Brede, Benjamin and Kooistra, Lammert},
	month = nov,
	year = {2024},
	note = {Publisher: Elsevier},
	pages = {104184},
}
