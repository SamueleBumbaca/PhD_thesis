@misc{14090575ImageNetLarge,
  title = {[1409.0575] {{ImageNet Large Scale Visual Recognition Challenge}}},
  urldate = {2025-02-25}
}

@inproceedings{akyonSlicingAidedHyper2022,
  title = {Slicing {{Aided Hyper Inference}} and {{Fine-tuning}} for {{Small Object Detection}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Akyon, Fatih Cagatay and Altinuc, Sinan Onur and Temizel, Alptekin},
  year = {2022},
  month = oct,
  eprint = {2202.06934},
  primaryclass = {cs},
  pages = {966--970},
  doi = {10.1109/ICIP46576.2022.9897990},
  urldate = {2025-03-02},
  abstract = {Detection of small objects and objects far away in the scene is a major challenge in surveillance applications. Such objects are represented by small number of pixels in the image and lack sufficient details, making them difficult to detect using conventional detectors. In this work, an open-source framework called Slicing Aided Hyper Inference (SAHI) is proposed that provides a generic slicing aided inference and fine-tuning pipeline for small object detection. The proposed technique is generic in the sense that it can be applied on top of any available object detector without any fine-tuning. Experimental evaluations, using object detection baselines on the Visdrone and xView aerial object detection datasets show that the proposed inference method can increase object detection AP by 6.8\%, 5.1\% and 5.3\% for FCOS, VFNet and TOOD detectors, respectively. Moreover, the detection accuracy can be further increased with a slicing aided fine-tuning, resulting in a cumulative increase of 12.7\%, 13.4\% and 14.5\% AP in the same order. Proposed technique has been integrated with Detectron2, MMDetection and YOLOv5 models and it is publicly available at https://github.com/obss/sahi.git .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@inproceedings{alhazmiEffectsAnnotationQuality2021,
  title = {Effects of Annotation Quality on Model Performance},
  booktitle = {2021 {{International Conference}} on {{Artificial Intelligence}} in {{Information}} and {{Communication}} ({{ICAIIC}})},
  author = {Alhazmi, Khaled and Alsumari, Walaa and Seppo, Indrek and Podkuiko, Lara and Simon, Martin},
  year = {2021},
  month = apr,
  pages = {063--067},
  doi = {10.1109/ICAIIC51459.2021.9415271},
  urldate = {2025-02-21},
  abstract = {Supervised machine learning generally requires pre-labelled data. Although there are several open access and pre-annotated datasets available for training machine learning algorithms, most contain a limited number of object classes, which may not be suitable for specific tasks. As previously available pre-annotated data is not usually sufficient for custom models, most of the real world applications require collecting and preparing training data. There is an obvious trade-off between annotation quality and quantity. Time and resources can be allocated for ensuring superior data quality or for increasing the quantity of the annotated data. We test the degree of the detrimental effect caused by the annotation errors. We conclude that while the results deteriorate if annotations are erroneous; the effect - at least while using relatively homogeneous sequential video data - is limited. The benefits from the increased annotated data set size (created by using imperfect auto-annotation methods) outweighs the deterioration caused by annotated data.},
  keywords = {annotation quality,Annotations,computer vision,custom dataset,Learning (artificial intelligence),Machine learning,Machine learning algorithms,object detection,Object detection,Open Access,supervised learning data,Training,training data,Training data}
}

@article{alsahiliPowerTransferLearning2022,
  title = {The Power of Transfer Learning in Agricultural Applications: {{AgriNet}}},
  shorttitle = {The Power of Transfer Learning in Agricultural Applications},
  author = {Al Sahili, Zahraa and Awad, Mariette},
  year = {2022},
  month = dec,
  journal = {Frontiers in Plant Science},
  volume = {13},
  publisher = {Frontiers},
  issn = {1664-462X},
  doi = {10.3389/fpls.2022.992700},
  urldate = {2025-02-24},
  abstract = {{$<$}p{$>$}Advances in deep learning and transfer learning have paved the way for various automation classification tasks in agriculture, including plant diseases, pests, weeds, and plant species detection. However, agriculture automation still faces various challenges, such as the limited size of datasets and the absence of plant-domain-specific pretrained models. Domain specific pretrained models have shown state of art performance in various computer vision tasks including face recognition and medical imaging diagnosis. In this paper, we propose AgriNet dataset, a collection of 160k agricultural images from more than 19 geographical locations, several images captioning devices, and more than 423 classes of plant species and diseases. We also introduce AgriNet models, a set of pretrained models on five ImageNet architectures: VGG16, VGG19, Inception-v3, InceptionResNet-v2, and Xception. AgriNet-VGG19 achieved the highest classification accuracy of 94\% and the highest F1-score of 92\%. Additionally, all proposed models were found to accurately classify the 423 classes of plant species, diseases, pests, and weeds with a minimum accuracy of 87\% for the Inception-v3 model. Finally, experiments to evaluate of superiority of AgriNet models compared to ImageNet models were conducted on two external datasets: pest and plant diseases dataset from Bangladesh and a plant diseases dataset from Kashmir.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Agriculture,Convolutional Neural Network,Pest,plant disease,Plant species,pretrained models,Transfer Learning,weed}
}

@article{amirkolaeeAdaTreeFormerFewShot2024,
  title = {{{AdaTreeFormer}}: {{Few}} Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image},
  shorttitle = {{{AdaTreeFormer}}},
  author = {Amirkolaee, Hamed Amini and Shi, Miaojing and He, Lianghua and Mulligan, Mark},
  year = {2024},
  month = aug,
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {214},
  pages = {193--208},
  issn = {0924-2716},
  doi = {10.1016/j.isprsjprs.2024.06.015},
  urldate = {2025-03-01},
  abstract = {The process of estimating and counting tree density using only a single aerial or satellite image is a difficult task in the fields of photogrammetry and remote sensing. However, it plays a crucial role in the management of forests. The huge variety of trees in varied topography severely hinders tree counting models to perform well. The purpose of this paper is to propose a framework that is learnt from the source domain with sufficient labeled trees and is adapted to the target domain with only a limited number of labeled trees. Our method, termed as AdaTreeFormer, contains one shared encoder with a hierarchical feature extraction scheme to extract robust features from the source and target domains. It also consists of three subnets: two for extracting self-domain attention maps from source and target domains respectively and one for extracting cross-domain attention maps. For the latter, an attention-to-adapt mechanism is introduced to distill relevant information from different domains while generating tree density maps; a hierarchical cross-domain feature alignment scheme is proposed that progressively aligns the features from the source and target domains. We also adopt adversarial learning into the framework to further reduce the gap between source and target domains. Our AdaTreeFormer is evaluated on six designed domain adaptation tasks using three tree counting datasets, i.e. Jiangsu, Yosemite, and London. Experimental results show that AdaTreeFormer significantly surpasses the state of the art, e.g. in the cross domain from the Yosemite to Jiangsu dataset, it achieves a reduction of 15.9 points in terms of the absolute counting errors and an increase of 10.8\% in the accuracy of the detected trees' locations. The codes and datasets are available at https://github.com/HAAClassic/AdaTreeFormer.},
  keywords = {Attention-to-adapt,Few-shot domain adaptation,Remote sensing,Transformer,Tree counting}
}

@article{amjoudObjectDetectionUsing2023,
  title = {Object {{Detection Using Deep Learning}}, {{CNNs}} and {{Vision Transformers}}: {{A Review}}},
  shorttitle = {Object {{Detection Using Deep Learning}}, {{CNNs}} and {{Vision Transformers}}},
  author = {Amjoud, Ayoub Benali and Amrouch, Mustapha},
  year = {2023},
  journal = {IEEE access : practical innovations, open solutions},
  volume = {11},
  pages = {35479--35516},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3266093},
  urldate = {2025-03-03},
  abstract = {Detecting objects remains one of computer vision and image understanding applications' most fundamental and challenging aspects. Significant advances in object detection have been achieved through improved object representation and the use of deep neural network models. This paper examines more closely how object detection has evolved in the era of deep learning over the past years. We present a literature review on various state-of-the-art object detection algorithms and the underlying concepts behind these methods. We classify these methods into three main groups: anchor-based, anchor-free, and transformer-based detectors. Those approaches are distinct in the way they identify objects in the image. We discuss the insights behind these algorithms and experimental analyses to compare quality metrics, speed/accuracy tradeoffs, and training methodologies. The survey compares the major convolutional neural networks for object detection. It also covers the strengths and limitations of each object detector model and draws significant conclusions. We provide simple graphical illustrations summarising the development of object detection methods under deep learning. Finally, we identify where future research will be conducted.},
  keywords = {convolutional neural networks,Convolutional neural networks,deep learning,Deep learning,Detectors,Feature extraction,neural networks,Neural networks,Object detection,review,survey,transformers,Transformers,Visualization}
}

@article{andvaagCountingCanolaGeneralizable2024,
  title = {Counting {{Canola}}: {{Toward Generalizable Aerial Plant Detection Models}}},
  shorttitle = {Counting {{Canola}}},
  author = {Andvaag, Erik and Krys, Kaylie and Shirtliffe, Steven J. and Stavness, Ian},
  year = {2024},
  month = nov,
  journal = {Plant phenomics (Washington, D.C.)},
  volume = {6},
  pages = {0268},
  publisher = {American Association for the Advancement of Science},
  doi = {10.34133/plantphenomics.0268},
  urldate = {2025-02-20},
  abstract = {Plant population counts are highly valued by crop producers as important early-season indicators of field health. Traditionally, emergence rate estimates have been acquired through manual counting, an approach that is labor-intensive and relies heavily on sampling techniques. By applying deep learning-based object detection models to aerial field imagery, accurate plant population counts can be obtained for much larger areas of a field. Unfortunately, current detection models often perform poorly when they are faced with image conditions that do not closely resemble the data found in their training sets. In this paper, we explore how specific facets of a plant detector's training set can affect its ability to generalize to unseen image sets. In particular, we examine how a plant detection model's generalizability is influenced by the size, diversity, and quality of its training data. Our experiments show that the gap between in-distribution and out-of-distribution performance cannot be closed by merely increasing the size of a model's training set. We also demonstrate the importance of training set diversity in producing generalizable models, and show how different types of annotation noise can elicit different model behaviors in out-of-distribution test sets. We conduct our investigations with a large and diverse dataset of canola field imagery that we assembled over several years. We also present a new web tool, Canola Counter, which is specifically designed for remote-sensed aerial plant detection tasks. We use the Canola Counter tool to prepare our annotated canola seedling dataset and conduct our experiments. Both our dataset and web tool are publicly available.}
}

@misc{AnonymizedRepositoryAnonymous,
  title = {Anonymized {{Repository}} - {{Anonymous GitHub}}},
  urldate = {2025-02-24}
}

@misc{antoniouDataAugmentationGenerative2018,
  title = {Data {{Augmentation Generative Adversarial Networks}}},
  author = {Antoniou, Antreas and Storkey, Amos and Edwards, Harrison},
  year = {2018},
  month = mar,
  number = {arXiv:1711.04340},
  eprint = {1711.04340},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1711.04340},
  urldate = {2025-03-01},
  abstract = {Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, takes data from a source domain and learns to take any data item and generalise it to generate other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes of data. We show that a Data Augmentation Generative Adversarial Network (DAGAN) augments standard vanilla classifiers well. We also show a DAGAN can enhance few-shot learning systems such as Matching Networks. We demonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In our experiments we can see over 13\% increase in accuracy in the low-data regime experiments in Omniglot (from 69\% to 82\%), EMNIST (73.9\% to 76\%) and VGG-Face (4.5\% to 12\%); in Matching Networks for Omniglot we observe an increase of 0.5\% (from 96.9\% to 97.4\%) and an increase of 1.8\% in EMNIST (from 59.5\% to 61.3\%).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@misc{arshadAgEvalBenchmarkZeroShot2024,
  title = {{{AgEval}}: {{A Benchmark}} for {{Zero-Shot}} and {{Few-Shot Plant Stress Phenotyping}} with {{Multimodal LLMs}}},
  shorttitle = {{{AgEval}}},
  author = {Arshad, Muhammad Arbab and Jubery, Talukder Zaki and Roy, Tirtho and Nassiri, Rim and Singh, Asheesh K. and Singh, Arti and Hegde, Chinmay and Ganapathysubramanian, Baskar and Balu, Aditya and Krishnamurthy, Adarsh and Sarkar, Soumik},
  year = {2024},
  month = jul,
  number = {arXiv:2407.19617},
  eprint = {2407.19617},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.19617},
  urldate = {2025-02-24},
  abstract = {Plant stress phenotyping traditionally relies on expert assessments and specialized models, limiting scalability in agriculture. Recent advances in multimodal large language models (LLMs) offer potential solutions to this challenge. We present AgEval, a benchmark comprising 12 diverse plant stress phenotyping tasks, to evaluate these models' capabilities. Our study assesses zero-shot and few-shot in-context learning performance of state-of-the-art models, including Claude, GPT, Gemini, and LLaVA. Results show significant performance improvements with few-shot learning, with F1 scores increasing from 46.24\% to 73.37\% in 8-shot identification for the best-performing model. Few-shot examples from other classes in the dataset have negligible or negative impacts, although having the exact category example helps to increase performance by 15.38\%. We also quantify the consistency of model performance across different classes within each task, finding that the coefficient of variance (CV) ranges from 26.02\% to 58.03\% across models, implying that subject matter expertise is needed - of 'difficult' classes - to achieve reliability in performance. AgEval establishes baseline metrics for multimodal LLMs in agricultural applications, offering insights into their promise for enhancing plant stress phenotyping at scale. Benchmark and code can be accessed at: https://anonymous.4open.science/r/AgEval/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{badgujarAgriculturalObjectDetection2024,
  title = {Agricultural Object Detection with {{You Only Look Once}} ({{YOLO}}) {{Algorithm}}: {{A}} Bibliometric and Systematic Literature Review},
  shorttitle = {Agricultural Object Detection with {{You Only Look Once}} ({{YOLO}}) {{Algorithm}}},
  author = {Badgujar, Chetan M and Poulose, Alwin and Gan, Hao},
  year = {2024},
  month = aug,
  journal = {Computers and Electronics in Agriculture},
  volume = {223},
  pages = {109090},
  issn = {0168-1699},
  doi = {10.1016/j.compag.2024.109090},
  urldate = {2025-03-01},
  abstract = {Vision is a major component in several digital technologies and tools used in agriculture. Object detection plays a pivotal role in digital farming by automating the task of detecting, identifying, and localization of various objects in large-scale agrarian landscapes. The single-stage detection algorithm, You Only Look Once (YOLO), has gained popularity in agriculture in a relatively short span due to its state-of-the-art performance in terms of accuracy, speed, and network size. YOLO offers real-time detection performance with good accuracy and is implemented in various agricultural tasks, including monitoring, surveillance, sensing, automation, and robotics operations. The research and application of YOLO in agriculture are accelerating at a tremendous speed but are fragmented and multidisciplinary in nature. Moreover, the performance characteristics (i.e., accuracy, speed, computation) of the object detector influence the rate of technology implementation and adoption in agriculture. Therefore, this study aimed to collect extensive literature to document and critically evaluate the advances and application of YOLO for agricultural object recognition tasks. First, we conducted a bibliometric review of 257 selected articles to understand the scholarly landscape (i.e., research trends, evolution, global hotspots, and gaps) of YOLO in the broad agricultural domain. Secondly, we conducted a systematic literature review on 30 selected articles to identify current knowledge, critical gaps, and modifications in YOLO for specific agricultural tasks. The study critically assessed and summarized the information on YOLO's end-to-end learning approach, including data acquisition, processing, network modification, integration, and deployment. We also discussed task-specific YOLO algorithm modification and integration to meet the agricultural object or environment-specific challenges. In general, YOLO-integrated digital tools and technologies showed the potential for real-time, automated monitoring, surveillance, and object handling to reduce labor, production cost, and environmental impact while maximizing resource efficiency. The study provides detailed documentation and significantly advances the existing knowledge on applying YOLO in agriculture, which can greatly benefit the scientific community. The results of this study open the door for implementing YOLO-based solutions in practical agricultural scenarios and add to the expanding corpus of information on computer vision applications in agriculture.},
  keywords = {Automation,Computer vision,Deep learning,Digital tools,Fruit detection,Images,Transfer learning}
}

@inproceedings{bansalZeroShotObjectDetection2018,
  title = {Zero-{{Shot Object Detection}}},
  booktitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Bansal, Ankan and Sikka, Karan and Sharma, Gaurav and Chellappa, Rama and Divakaran, Ajay},
  year = {2018},
  pages = {384--400},
  urldate = {2025-02-26}
}

@article{barretoAutomaticUAVbasedCounting2021,
  title = {Automatic {{UAV-based}} Counting of Seedlings in Sugar-Beet Field and Extension to Maize and Strawberry},
  author = {Barreto, Abel and Lottes, Philipp and Ispizua Yamati, Facundo Ram{\'o}n and Baumgarten, Stephen and Wolf, Nina Anastasia and Stachniss, Cyrill and Mahlein, Anne-Katrin and Paulus, Stefan},
  year = {2021},
  month = dec,
  journal = {Computers and Electronics in Agriculture},
  volume = {191},
  pages = {106493},
  issn = {0168-1699},
  doi = {10.1016/j.compag.2021.106493},
  urldate = {2025-02-20},
  abstract = {Counting crop seedlings is a time-demanding activity involved in diverse agricultural practices like plant cultivating, experimental trials, plant breeding procedures, and weed control. Unmanned Aerial Vehicles (UAVs) carrying RGB cameras are novel tools for automatic field mapping, and the analysis of UAV images by deep learning methods can provide relevant agronomic information. UAV-based camera systems and a deep learning image analysis pipeline are implemented for a fully automated plant counting in sugar beet, maize, and strawberry fields in the present study. Five locations were monitored at different growth stages, and the crop number per plot was automatically predicted by using a fully convolutional network (FCN) pipeline. Our FCN-based approach is a single model for jointly determining both the exact stem location of crop and weed plants and a pixel-wise plant classification considering crop, weed, and soil. To determinate the approach performance, predicted crop counting was compared to visually assessed ground truth data. Results show that UAV-based counting of sugar-beet plants delivers forecast errors lower than 4.6\%, and the main factors for performance are related to the intra-row distance and the growth stage. The pipeline's extension to other crops is possible; the errors of the predictions are lower than 4\% under practical field conditions for maize and strawberry fields. This work highlight the feasibility of automatic crop counting, which can reduce manual effort to the farmers.},
  keywords = {Deep learning,FCN,Growth stage,Intra-row distance,Plant segmentation,Sugar beet,Time-series,UAV}
}

@misc{bealTransformerBasedObjectDetection2020,
  title = {Toward {{Transformer-Based Object Detection}}},
  author = {Beal, Josh and Kim, Eric and Tzeng, Eric and Park, Dong Huk and Zhai, Andrew and Kislyuk, Dmitry},
  year = {2020},
  month = dec,
  number = {arXiv:2012.09958},
  eprint = {2012.09958},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.09958},
  urldate = {2025-03-03},
  abstract = {Transformers have become the dominant model in natural language processing, owing to their ability to pretrain on massive amounts of data, then transfer to smaller, more specific tasks via fine-tuning. The Vision Transformer was the first major attempt to apply a pure transformer model directly to images as input, demonstrating that as compared to convolutional networks, transformer-based architectures can achieve competitive results on benchmark classification tasks. However, the computational complexity of the attention operator means that we are limited to low-resolution inputs. For more complex tasks such as detection or segmentation, maintaining a high input resolution is crucial to ensure that models can properly identify and reflect fine details in their output. This naturally raises the question of whether or not transformer-based architectures such as the Vision Transformer are capable of performing tasks other than classification. In this paper, we determine that Vision Transformers can be used as a backbone by a common detection task head to produce competitive COCO results. The model that we propose, ViT-FRCNN, demonstrates several known properties associated with transformers, including large pretraining capacity and fast fine-tuning performance. We also investigate improvements over a standard detection backbone, including superior performance on out-of-domain images, better performance on large objects, and a lessened reliance on non-maximum suppression. We view ViT-FRCNN as an important stepping stone toward a pure-transformer solution of complex vision tasks such as object detection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{bochkovskiyYOLOv4OptimalSpeed2020,
  title = {{{YOLOv4}}: {{Optimal Speed}} and {{Accuracy}} of {{Object Detection}}},
  shorttitle = {{{YOLOv4}}},
  author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
  year = {2020},
  month = apr,
  number = {arXiv:2004.10934},
  eprint = {2004.10934},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.10934},
  urldate = {2025-03-04},
  abstract = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5\% AP (65.7\% AP50) for the MS COCO dataset at a realtime speed of {\textasciitilde}65 FPS on Tesla V100. Source code is at https://github.com/AlexeyAB/darknet},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing}
}

@misc{brigatoCloseLookDeep2020,
  title = {A {{Close Look}} at {{Deep Learning}} with {{Small Data}}},
  author = {Brigato, L. and Iocchi, L.},
  year = {2020},
  month = oct,
  number = {arXiv:2003.12843},
  eprint = {2003.12843},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2003.12843},
  urldate = {2025-02-21},
  abstract = {In this work, we perform a wide variety of experiments with different deep learning architectures on datasets of limited size. According to our study, we show that model complexity is a critical factor when only a few samples per class are available. Differently from the literature, we show that in some configurations, the state of the art can be improved using low complexity models. For instance, in problems with scarce training samples and without data augmentation, low-complexity convolutional neural networks perform comparably well or better than state-of-the-art architectures. Moreover, we show that even standard data augmentation can boost recognition performance by large margins. This result suggests the development of more complex data generation/augmentation pipelines for cases when data is limited. Finally, we show that dropout, a widely used regularization technique, maintains its role as a good regularizer even when data is scarce. Our findings are empirically validated on the sub-sampled versions of popular CIFAR-10, Fashion-MNIST and, SVHN benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{carionEndtoEndObjectDetection2020,
  title = {End-to-{{End Object Detection}} with {{Transformers}}},
  author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  year = {2020},
  month = may,
  number = {arXiv:2005.12872},
  eprint = {2005.12872},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.12872},
  urldate = {2025-02-21},
  abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{chadebecDataAugmentationVariational2021,
  title = {Data {{Augmentation}} with {{Variational Autoencoders}} and {{Manifold Sampling}}},
  booktitle = {Deep {{Generative Models}}, and {{Data Augmentation}}, {{Labelling}}, and {{Imperfections}}},
  author = {Chadebec, Cl{\'e}ment and Allassonni{\`e}re, St{\'e}phanie},
  editor = {Engelhardt, Sandy and Oksuz, Ilkay and Zhu, Dajiang and Yuan, Yixuan and Mukhopadhyay, Anirban and Heller, Nicholas and Huang, Sharon Xiaolei and Nguyen, Hien and Sznitman, Raphael and Xue, Yuan},
  year = {2021},
  pages = {184--192},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-88210-5_17},
  abstract = {We propose a new efficient way to sample from a Variational Autoencoder in the challenging low sample size setting (A code is available at https://github.com/clementchadebec/Data\_Augmentation\_with\_VAE-DALI). This method reveals particularly well suited to perform data augmentation in such a low data regime and is validated across various standard and real-life data sets. In particular, this scheme allows to greatly improve classification results on the OASIS database where balanced accuracy jumps from 80.7\% for a classifier trained with the raw data to 88.6\% when trained only with the synthetic data generated by our method. Such results were also observed on 3 standard data sets and with other classifiers.},
  isbn = {978-3-030-88210-5},
  langid = {english},
  keywords = {Data augmentation,Latent space modelling,VAE}
}

@misc{chadebecDataAugmentationVariational2021a,
  title = {Data {{Augmentation}} with {{Variational Autoencoders}} and {{Manifold Sampling}}},
  author = {Chadebec, Cl{\'e}ment and Allassonni{\`e}re, St{\'e}phanie},
  year = {2021},
  month = sep,
  number = {arXiv:2103.13751},
  eprint = {2103.13751},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.13751},
  urldate = {2025-01-02},
  abstract = {We propose a new efficient way to sample from a Variational Autoencoder in the challenging low sample size setting. This method reveals particularly well suited to perform data augmentation in such a low data regime and is validated across various standard and real-life data sets. In particular, this scheme allows to greatly improve classification results on the OASIS database where balanced accuracy jumps from 80.7\% for a classifier trained with the raw data to 88.6\% when trained only with the synthetic data generated by our method. Such results were also observed on 3 standard data sets and with other classifiers. A code is available at https://github.com/clementchadebec/Data\_Augmentation\_with\_VAE-DALI.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{charisisDeepLearningbasedInstance2024,
  title = {Deep Learning-Based Instance Segmentation Architectures in Agriculture: {{A}} Review of the Scopes and Challenges},
  shorttitle = {Deep Learning-Based Instance Segmentation Architectures in Agriculture},
  author = {Charisis, Christos and Argyropoulos, Dimitrios},
  year = {2024},
  month = aug,
  journal = {Smart Agricultural Technology},
  volume = {8},
  pages = {100448},
  issn = {2772-3755},
  doi = {10.1016/j.atech.2024.100448},
  urldate = {2025-01-30},
  abstract = {Deep learning (DL) based instance segmentation has attracted a growing research interest in the scientific community to tackle precision agriculture problems over the past few years. However, accurate crop detection and localization in complex environments pose a significant challenge. Instance segmentation is considered as a promising DL technique that expands on object detection to perform pixel-wise image instance segmentation and address pattern recognition problems efficiently. In this review, we identify 77 relevant studies on DL-based instance segmentation implementations in agriculture and thoroughly investigate them from the following perspectives: i) the specific architecture employed; ii) the data type and availability, the data annotation process and the data pre-processing techniques; iii) the performance metrics used; and iv) hardware, inference time and GPU requirements. Our findings indicate that crop detection (48 papers) constitutes a fundamental task in a DL-based instance segmentation pipeline to enable crop growth monitoring (19 papers) and plant health analysis (10 papers). Among them, 6 papers reported robotic manipulation and other related automation tasks. Based on our findings we can conclude that there is a significant trend towards two-stage DL-based instance segmentation models i.e., Mask R-CNN baseline and customized architectures (69 papers). Limitations and challenges, such as availability of benchmark crop datasets, open-source codes for semi-automatic annotation tools, technical requirements and opportunities for future research are discussed.},
  keywords = {Automation machine vision,Image datasets,Mask R-CNN,Robotics,Specialty crops}
}

@inproceedings{choiPerformanceEvaluationRANSAC2009,
  title = {Performance {{Evaluation}} of {{RANSAC Family}}},
  booktitle = {Procedings of the {{British Machine Vision Conference}} 2009},
  author = {Choi, Sunglok and Kim, Taemin and Yu, Wonpil},
  year = {2009},
  pages = {81.1-81.12},
  publisher = {British Machine Vision Association},
  address = {London},
  doi = {10.5244/C.23.81},
  urldate = {2025-03-10},
  abstract = {RANSAC (Random Sample Consensus) has been popular in regression problem with samples contaminated with outliers. It has been a milestone of many researches on robust estimators, but there are a few survey and performance analysis on them. This paper categorizes them on their objectives: being accurate, being fast, and being robust. Performance evaluation performed on line fitting with various data distribution. Planar homography estimation was utilized to present performance in real data.},
  isbn = {978-1-901725-39-1},
  langid = {english}
}

@misc{cubukAutoAugmentLearningAugmentation2019,
  title = {{{AutoAugment}}: {{Learning Augmentation Policies}} from {{Data}}},
  shorttitle = {{{AutoAugment}}},
  author = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
  year = {2019},
  month = apr,
  number = {arXiv:1805.09501},
  eprint = {1805.09501},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1805.09501},
  urldate = {2025-03-01},
  abstract = {Data augmentation is an effective technique for improving the accuracy of modern image classifiers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called AutoAugment to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many sub-policies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to find the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain a Top-1 accuracy of 83.5\% which is 0.4\% better than the previous record of 83.1\%. On CIFAR-10, we achieve an error rate of 1.5\%, which is 0.6\% better than the previous state-of-the-art. Augmentation policies we find are transferable between datasets. The policy learned on ImageNet transfers well to achieve significant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{davidPlantDetectionCounting2021,
  title = {Plant Detection and Counting from High-Resolution {{RGB}} Images Acquired from {{UAVs}}: {{Comparison}} between Deep-Learning and Handcrafted Methods with Application to Maize, Sugar Beet, and Sunflowe},
  shorttitle = {Plant Detection and Counting from High-Resolution {{RGB}} Images Acquired from {{UAVs}}},
  author = {David, Etienne and Daubige, Ga{\"e}tan and Joudelat, Fran{\c c}ois and Burger, Philippe and Comar, Alexis and {de Solan}, Benoit and Baret, Fr{\'e}d{\'e}ric},
  year = {2021},
  doi = {10.1101/2021.04.27.441631},
  urldate = {2024-07-24},
  abstract = {Progresses in agronomy rely on accurate measurement of the experimentations conducted to improve the yield component. Measurement of the plant density is required for a number of applications since it drives part of the crop fate. The standard manual measurements in the field could be efficiently replaced by high-throughput techniques based on high-spatial resolution images taken from UAVs. This study compares several automated detection of individual plants in the images from which the plant density can be estimated. It is based on a large dataset of high resolution Red/Green/Blue (RGB) images acquired from Unmanned Aerial Vehicules (UAVs) during several years and experiments over maize, sugar beet and sunflower crops at early stages. A total of 16247 plants have been labelled interactively on the images. Performances of handcrafted method (HC) were compared to those of deep learning (DL). The HC method consists in segmenting the image into green and background pixels, identifying rows, then objects corresponding to plants thanks to knowledge of the sowing pattern as prior information. The DL method is based on the Faster Region with Convolutional Neural Network (Faster RCNN) model trained over 2/3 of the images selected to represent a good balance between plant development stage and sessions. One model is trained for each crop. Results: show that simple DL methods generally outperforms simple HC, particularly for maize and sunflower crops. A significant level of variability of plant detection performances is observed between the several experiments. This was explained by the variability of image acquisition conditions including illumination, plant development stage, background complexity and weed infestation. The image quality determines part of the performances for HC methods which makes the segmentation step more difficult. Performances of DL methods are limited mainly by the presence of weeds. A hybrid method (HY) was proposed to eliminate weeds between the rows using the rules developed for the HC method. HY improves slightly DL performances in the case of high weed infestation. When few images corresponding to the conditions of the testing dataset were complementing the training dataset for DL, a drastic increase of performances for all the crops is observed, with relative RMSE below 5\% for the estimation of the plant density.}
}

@misc{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2025-03-02},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{directive_91_414_EEC,
  title = {Council {{Directive}} 91/414/{{EEC}} of 15 {{July}} 1991 Concerning the Placing of Plant Protection Products on the Market},
  author = {{Council of the European Communities}},
  year = {1991},
  volume = {L 230},
  pages = {1--32},
  urldate = {2025-03-12}
}

@inproceedings{dosovitskiyImageWorth16x162020,
  title = {An {{Image Is Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image Is Worth}} 16x16 {{Words}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2020},
  month = oct,
  urldate = {2025-03-04},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  langid = {english}
}

@misc{dosovitskiyImageWorth16x162021,
  title = {An {{Image Is Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image Is Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  month = jun,
  number = {arXiv:2010.11929},
  eprint = {2010.11929},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.11929},
  urldate = {2025-03-02},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@inproceedings{duSpineNetLearningScalePermuted2020,
  title = {{{SpineNet}}: {{Learning Scale-Permuted Backbone}} for {{Recognition}} and {{Localization}}},
  shorttitle = {{{SpineNet}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Du, Xianzhi and Lin, Tsung-Yi and Jin, Pengchong and Ghiasi, Golnaz and Tan, Mingxing and Cui, Yin and Le, Quoc V. and Song, Xiaodan},
  year = {2020},
  pages = {11592--11601},
  urldate = {2025-02-26}
}

@misc{EC_Regulation_1107_2009,
  title = {Regulation ({{EC}}) {{No}} 1107/2009 of the {{European Parliament}} and of the {{Council}} of 21 {{October}} 2009 Concerning the Placing of Plant Protection Products on the Market},
  author = {{European Parliament and Council}},
  year = {2009},
  volume = {L 309},
  pages = {1--50},
  urldate = {2025-03-12}
}

@techreport{EPPO_PP1_135,
  title = {{{PP}} 1/135(4) Phytotoxicity Assessment},
  author = {{EPPO}},
  year = {2014},
  institution = {{European and Mediterranean Plant Protection Organization}},
  urldate = {2025-03-12}
}

@techreport{EPPO_PP1_152,
  title = {{{PP}} 1/152 {{Design}} and Analysis of Efficacy Evaluation Trials},
  author = {{EPPO}},
  year = {2012},
  institution = {{European and Mediterranean Plant Protection Organization}},
  urldate = {2025-03-12}
}

@techreport{EPPO_PP1_181,
  title = {{{PP}} 1/181(5) {{Conduct}} and Reporting of Efficacy Evaluation Trials, Including Good Experimental Practice},
  author = {{EPPO}},
  year = {2021},
  institution = {{European and Mediterranean Plant Protection Organization}},
  urldate = {2025-03-12}
}

@techreport{EPPO_PP1_93,
  title = {{{PP}} 1/93(3) Weeds in Cereals},
  author = {{EPPO}},
  year = {2015},
  institution = {{European and Mediterranean Plant Protection Organization}},
  urldate = {2025-03-12}
}

@misc{EstimatesMaizePlant,
  title = {Estimates of {{Maize Plant Density}} from {{UAV RGB Images Using Faster-RCNN Detection Model}}: {{Impact}} of the {{Spatial Resolution}}},
  shorttitle = {Estimates of {{Maize Plant Density}} from {{UAV RGB Images Using Faster-RCNN Detection Model}}},
  doi = {10.34133/2021/9824843},
  urldate = {2025-02-25},
  langid = {english}
}

@misc{EstimatesMaizePlanta,
  title = {Estimates of {{Maize Plant Density}} from {{UAV RGB Images Using Faster-RCNN Detection Model}}: {{Impact}} of the {{Spatial Resolution}} {\textbar} {{Plant Phenomics}}},
  urldate = {2025-03-01}
}

@misc{EURLex1997265,
  title = {Uniform {{Principles}} for Evaluation and Authorisation of Plant Protection Products},
  author = {{European Commission}},
  year = {1997},
  volume = {L 265},
  pages = {87--109},
  urldate = {2025-03-12}
}

@misc{EURLex1997265,
  title = {Council Directive 97/57/{{EC}} of 22 September 1997},
  year = {1997},
  howpublished = {{$<$}a href="\{"{$>$}\{{$<$}/a{$>$}https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX\vphantom{\}\}}}
}

@misc{EURLex1997265,
  title = {Council Directive 97/57/{{EC}}},
  author = {{European Commission}},
  year = {1997}
}

@misc{EURLex1997265a,
  title = {{{EUR-Lex}} - {{L}}:1997:265:{{TOC}} - {{EN}} - {{EUR-Lex}}},
  shorttitle = {{{EUR-Lex}} - {{L}}},
  urldate = {2025-03-12},
  howpublished = {https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=oj:JOL\_1997\_265\_R\_TOC},
  langid = {english},
  annotation = {Doc ID: L:1997:265:TOC\\
Doc Sector: other\\
Doc Title: Official Journal of the European Communities, L 265, 27 September 1997\\
Doc Type: other\\
Usr\_lan: en}
}

@article{EWRS_score,
  title = {Einheitliche Codierung Der Ph{\"a}nologischen Entwicklungsstadien Mono- Und Dikotyler Pflanzen - Erweiterte {{BBCH-skala}}, Allgemein},
  author = {Bleiholder, H. and {van den Boom}, T. and Langel{\"u}ddeke, P. and Stauss, R.},
  year = {1991},
  journal = {Nachrichtenblatt des Deutschen Pflanzenschutzdienstes},
  volume = {43},
  pages = {265--270}
}

@incollection{fao2024,
  booktitle = {Agricultural Production Statistics 2010--2023},
  author = {{FAO}},
  year = {2024},
  volume = {Analytical Briefs},
  publisher = {FAOSTAT},
  address = {Rome},
  urldate = {2025-03-01},
  langid = {english}
}

@misc{FasterRCNNRealTime,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} with {{Region Proposal Networks}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2025-02-21}
}

@incollection{fischlerRandomSampleConsensus1987,
  title = {Random {{Sample Consensus}}: {{A Paradigm}} for {{Model Fitting}} with {{Applications}} to {{Image Analysis}} and {{Automated Cartography}}},
  shorttitle = {Random {{Sample Consensus}}},
  booktitle = {Readings in {{Computer Vision}}},
  author = {Fischler, Martin A. and Bolles, Robert C.},
  editor = {Fischler, Martin A. and Firschein, Oscar},
  year = {1987},
  month = jan,
  pages = {726--740},
  publisher = {Morgan Kaufmann},
  address = {San Francisco (CA)},
  doi = {10.1016/B978-0-08-051581-6.50070-2},
  urldate = {2025-03-10},
  abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced, RANSAC is capable of interpreting/ smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing and analysis conditions. Implementation details and computational examples are also presented.},
  isbn = {978-0-08-051581-6}
}

@incollection{fisherStatisticalMethodsResearch1992,
  title = {Statistical {{Methods}} for {{Research Workers}}},
  booktitle = {Breakthroughs in {{Statistics}}: {{Methodology}} and {{Distribution}}},
  author = {Fisher, R. A.},
  editor = {Kotz, Samuel and Johnson, Norman L.},
  year = {1992},
  pages = {66--70},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-1-4612-4380-9_6},
  urldate = {2025-03-14},
  abstract = {The prime object of this book is to put into the hands of research workers, and especially of biologists, the means of applying statistical tests accurately to numerical data accumulated in their own laboratories or available in the literature.},
  isbn = {978-1-4612-4380-9},
  langid = {english}
}

@misc{fuCrossDomainFewShotObject2024,
  title = {Cross-{{Domain Few-Shot Object Detection}} via {{Enhanced Open-Set Object Detector}}},
  author = {Fu, Yuqian and Wang, Yu and Pan, Yixuan and Huai, Lian and Qiu, Xingyu and Shangguan, Zeyu and Liu, Tong and Fu, Yanwei and Gool, Luc Van and Jiang, Xingqun},
  year = {2024},
  month = sep,
  number = {arXiv:2402.03094},
  eprint = {2402.03094},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.03094},
  urldate = {2025-02-26},
  abstract = {This paper studies the challenging cross-domain few-shot object detection (CD-FSOD), aiming to develop an accurate object detector for novel domains with minimal labeled examples. While transformer-based open-set detectors, such as DE-ViT, show promise in traditional few-shot object detection, their generalization to CD-FSOD remains unclear: 1) can such open-set detection methods easily generalize to CD-FSOD? 2) If not, how can models be enhanced when facing huge domain gaps? To answer the first question, we employ measures including style, inter-class variance (ICV), and indefinable boundaries (IB) to understand the domain gap. Based on these measures, we establish a new benchmark named CD-FSOD to evaluate object detection methods, revealing that most of the current approaches fail to generalize across domains. Technically, we observe that the performance decline is associated with our proposed measures: style, ICV, and IB. Consequently, we propose several novel modules to address these issues. First, the learnable instance features align initial fixed instances with target categories, enhancing feature distinctiveness. Second, the instance reweighting module assigns higher importance to high-quality instances with slight IB. Third, the domain prompter encourages features resilient to different styles by synthesizing imaginary domains without altering semantic contents. These techniques collectively contribute to the development of the Cross-Domain Vision Transformer for CD-FSOD (CD-ViTO), significantly improving upon the base DE-ViT. Experimental results validate the efficacy of our model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{fuMetaSSDFastAdaptation2019,
  title = {Meta-{{SSD}}: {{Towards Fast Adaptation}} for {{Few-Shot Object Detection With Meta-Learning}}},
  shorttitle = {Meta-{{SSD}}},
  author = {Fu, Kun and Zhang, Tengfei and Zhang, Yue and Yan, Menglong and Chang, Zhonghan and Zhang, Zhengyuan and Sun, Xian},
  year = {2019},
  journal = {IEEE access : practical innovations, open solutions},
  volume = {7},
  pages = {77597--77606},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2922438},
  urldate = {2025-02-26},
  abstract = {The state-of-the-art object detection frameworks require the training on large-scale datasets, which is the crux of the present dilemma: overfitting or degrading performance with insufficient samples and time-consuming training process. On the basis of meta-learning, this paper proposes a generalized Few-Shot Detection (FSD) framework to overcome the above drawbacks of the current advances in object detection. The proposed framework consists of a meta-learner and an object detector. It can learn the general knowledge and proper fast adaptation strategies across many tasks. The meta-learner can teach the detector how to learn from few examples in just one updating step. Here, the object detector can be any supervised learning detection models in theory. Specifically, the proposed FSD framework employs Single-Shot MultiBox Detector (SSD) as the object detector in this paper, thus called Meta-SSD. Besides, a novel benchmark is constructed from Pascal VOC dataset for training and evaluation of meta-learning FSD. Experiments show that the Meta-SSD yields a promising result for FSD. Furthermore, the properties of Meta-SSD is analyzed. This paper can serve as a strong baseline and provide some inspiration for meta-learning FSD.},
  keywords = {Adaptation models,Detectors,fast adaptation,Feature extraction,few-shot,Meta-learning,object detection,Object detection,Proposals,Task analysis,Training}
}

@article{garcia-martinezDigitalCountCorn2020,
  title = {Digital {{Count}} of {{Corn Plants Using Images Taken}} by {{Unmanned Aerial Vehicles}} and {{Cross Correlation}} of {{Templates}}},
  author = {{Garc{\'i}a-Mart{\'i}nez}, H{\'e}ctor and {Flores-Magdaleno}, H{\'e}ctor and {Khalil-Gardezi}, Abdul and {Ascencio-Hern{\'a}ndez}, Roberto and {Tijerina-Ch{\'a}vez}, Leonardo and {V{\'a}zquez-Pe{\~n}a}, Mario A. and {Mancilla-Villa}, Oscar R.},
  year = {2020},
  month = apr,
  journal = {Agronomy},
  volume = {10},
  number = {4},
  pages = {469},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2073-4395},
  doi = {10.3390/agronomy10040469},
  urldate = {2025-03-01},
  abstract = {The number of plants, or planting density, is a key factor in corn crop yield. The objective of the present research work was to count corn plants using images obtained by sensors mounted on an unmanned aerial vehicle (UAV). An experiment was set up with five levels of nitrogen fertilization (140, 200, 260, 320 and 380 kg/ha) and four replicates, resulting in 20 experimental plots. The images were taken at 23, 44 and 65 days after sowing (DAS) at a flight altitude of 30 m, using two drones equipped with RGB sensors of 12, 16 and 20 megapixels (Canon PowerShot S100\_5.2, Sequoia\_4.9, DJI FC6310\_8.8). Counting was done through normalized cross-correlation (NCC) for four, eight and twelve plant samples or templates in the a* channel of the CIELAB color space because it represented the green color that allowed plant segmentation. A mean precision of 99\% was obtained for a pixel size of 0.49 cm, with a mean error of 2.2\% and a determination coefficient of 0.90 at 44 DAS. Precision values above 91\% were obtained at 23 and 44 DAS, with a mean error between plants counted digitally and visually of {\textpm}5.4\%. Increasing the number of samples or templates in the correlation estimation improved the counting precision. Good precision was achieved in the first growth stages of the crop when the plants do not overlap and there are no weeds. Using sensors and unmanned aerial vehicles, it is possible to determine the emergence of seedlings in the field and more precisely evaluate planting density, having more accurate information for better management of corn fields.},
  langid = {english},
  keywords = {corn,cross-correlation,image,plant counting,sensors,UAV}
}

@book{gburAnalysisGeneralizedLinear2020,
  title = {Analysis of {{Generalized Linear Mixed Models}} in the {{Agricultural}} and {{Natural Resources Sciences}}},
  author = {Gbur, Edward E. and Stroup, Walter W. and McCarter, Kevin S. and Durham, Susan and Young, Linda J. and Christman, Mary and West, Mark and Kramer, Matthew},
  year = {2020},
  month = jan,
  publisher = {John Wiley \& Sons},
  abstract = {Generalized Linear Mixed Models in the Agricultural and Natural Resources Sciences provides readers with an understanding and appreciation for the design and analysis of mixed models for non-normally distributed data. It is the only publication of its kind directed specifically toward the agricultural and natural resources sciences audience. Readers will especially benefit from the numerous worked examples based on actual experimental data and the discussion of pitfalls associated with incorrect analyses.},
  googlebooks = {BgnMDwAAQBAJ},
  isbn = {978-0-89118-182-8},
  langid = {english},
  keywords = {Science / Life Sciences / Horticulture,Technology & Engineering / Agriculture / Agronomy / Crop Science,Technology & Engineering / Agriculture / General}
}

@article{gengResearchSegmentationMethod2024,
  title = {Research on {{Segmentation Method}} of {{Maize Seedling Plant Instances Based}} on {{UAV Multispectral Remote Sensing Images}}},
  author = {Geng, Tingting and Yu, Haiyang and Yuan, Xinru and Ma, Ruopu and Li, Pengao},
  year = {2024},
  month = jan,
  journal = {Plants},
  volume = {13},
  number = {13},
  pages = {1842},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2223-7747},
  doi = {10.3390/plants13131842},
  urldate = {2025-01-30},
  abstract = {The accurate instance segmentation of individual crop plants is crucial for achieving a high-throughput phenotypic analysis of seedlings and smart field management in agriculture. Current crop monitoring techniques employing remote sensing predominantly focus on population analysis, thereby lacking precise estimations for individual plants. This study concentrates on maize, a critical staple crop, and leverages multispectral remote sensing data sourced from unmanned aerial vehicles (UAVs). A large-scale SAM image segmentation model is employed to efficiently annotate maize plant instances, thereby constructing a dataset for maize seedling instance segmentation. The study evaluates the experimental accuracy of six instance segmentation algorithms: Mask R-CNN, Cascade Mask R-CNN, PointRend, YOLOv5, Mask Scoring R-CNN, and YOLOv8, employing various combinations of multispectral bands for a comparative analysis. The experimental findings indicate that the YOLOv8 model exhibits exceptional segmentation accuracy, notably in the NRG band, with bbox\_mAP50 and segm\_mAP50 accuracies reaching 95.2\% and 94\%, respectively, surpassing other models. Furthermore, YOLOv8 demonstrates robust performance in generalization experiments, indicating its adaptability across diverse environments and conditions. Additionally, this study simulates and analyzes the impact of different resolutions on the model's segmentation accuracy. The findings reveal that the YOLOv8 model sustains high segmentation accuracy even at reduced resolutions (1.333 cm/px), meeting the phenotypic analysis and field management criteria.},
  langid = {english},
  keywords = {instance segmentation,maize seedlings,multispectral data,YOLOv8 model}
}

@misc{goldblumBattleBackbonesLargeScale2023,
  title = {Battle of the {{Backbones}}: {{A Large-Scale Comparison}} of {{Pretrained Models}} across {{Computer Vision Tasks}}},
  shorttitle = {Battle of the {{Backbones}}},
  author = {Goldblum, Micah and Souri, Hossein and Ni, Renkun and Shu, Manli and Prabhu, Viraj and Somepalli, Gowthami and Chattopadhyay, Prithvijit and Ibrahim, Mark and Bardes, Adrien and Hoffman, Judy and Chellappa, Rama and Wilson, Andrew Gordon and Goldstein, Tom},
  year = {2023},
  month = nov,
  number = {arXiv:2310.19909},
  eprint = {2310.19909},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.19909},
  urldate = {2025-03-01},
  abstract = {Neural network based computer vision systems are typically built on a backbone, a pretrained or randomly initialized feature extractor. Several years ago, the default option was an ImageNet-trained convolutional neural network. However, the recent past has seen the emergence of countless backbones pretrained using various algorithms and datasets. While this abundance of choice has led to performance increases for a range of systems, it is difficult for practitioners to make informed decisions about which backbone to choose. Battle of the Backbones (BoB) makes this choice easier by benchmarking a diverse suite of pretrained models, including vision-language models, those trained via self-supervised learning, and the Stable Diffusion backbone, across a diverse set of computer vision tasks ranging from classification to object detection to OOD generalization and more. Furthermore, BoB sheds light on promising directions for the research community to advance computer vision by illuminating strengths and weakness of existing approaches through a comprehensive analysis conducted on more than 1500 training runs. While vision transformers (ViTs) and self-supervised learning (SSL) are increasingly popular, we find that convolutional neural networks pretrained in a supervised fashion on large training sets still perform best on most tasks among the models we consider. Moreover, in apples-to-apples comparisons on the same architectures and similarly sized pretraining datasets, we find that SSL backbones are highly competitive, indicating that future works should perform SSL pretraining with advanced architectures and larger pretraining datasets. We release the raw results of our experiments along with code that allows researchers to put their own backbones through the gauntlet here: https://github.com/hsouri/Battle-of-the-Backbones},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{heiderSurveyDatasetsComputer2025,
  title = {A {{Survey}} of {{Datasets}} for {{Computer Vision}} in {{Agriculture}}: {{A}} Catalogue of High-Quality {{RGB}} Image Datasets of Natural Field Scenes},
  author = {Heider, Nico and Gunreben, Lorenz and Z{\"u}rner, Sebastian and Schieck, Martin},
  year = {2025},
  journal = {45. GIL-Jahrestagung, Digitale Infrastrukturen f{\"u}r eine nachhaltige Land-, Forst und Ern{\"a}hrungswirtschaft},
  pages = {35--47},
  publisher = {Gesellschaft f{\"u}r Informatik eV}
}

@misc{hendrycksManyFacesRobustness2020,
  title = {The {{Many Faces}} of {{Robustness}}: {{A Critical Analysis}} of {{Out-of-Distribution Generalization}}},
  shorttitle = {The {{Many Faces}} of {{Robustness}}},
  author = {Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and Song, Dawn and Steinhardt, Jacob and Gilmer, Justin},
  year = {2020},
  month = jun,
  number = {arXiv:2006.16241},
  eprint = {2006.16241},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.16241},
  urldate = {2025-03-01},
  abstract = {We introduce three new robustness benchmarks consisting of naturally occurring distribution changes in image style, geographic location, camera operation, and more. Using our benchmarks, we take stock of previously proposed hypotheses for out-of-distribution robustness and put them to the test. We find that using larger models and synthetic data augmentation can improve robustness on real-world distribution shifts, contrary to claims in prior work. Motivated by this, we introduce a new data augmentation method which advances the state-of-the-art and outperforms models pretrained with 1000x more labeled data. We find that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. We conclude that future research must study multiple distribution shifts simultaneously.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{hestnessDeepLearningScaling2017,
  title = {Deep {{Learning Scaling}} Is {{Predictable}}, {{Empirically}}},
  author = {Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  year = {2017},
  month = dec,
  number = {arXiv:1712.00409},
  eprint = {1712.00409},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1712.00409},
  urldate = {2025-02-21},
  abstract = {Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the "steepness" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{hiphenHiphenPlantDetection2021,
  title = {Hiphen - {{Plant}} Detection and Counting from High-Resolution {{RGB}} Images Acquired from {{UAVs}}: {{Comparison}} between Deep-Learning and Handcrafted Methods},
  shorttitle = {Hiphen - {{Plant}} Detection and Counting from High-Resolution {{RGB}} Images Acquired from {{UAVs}}},
  author = {{Hiphen}},
  year = {2021},
  month = jun,
  urldate = {2024-07-24},
  abstract = {Plants density is a key information on crop growth. Usually done manually, this task can beneficiate from advances in image analysis technics. Automated detection of individual plants in images is a key step to estimate this density. To develop and evaluate dedicated processing technics, high resolution RGB images were acquired from UAVs during several years...},
  langid = {american}
}

@article{hosseinyAutomatedFrameworkPlant2020,
  title = {An {{Automated Framework}} for {{Plant Detection Based}} on {{Deep Simulated Learning}} from {{Drone Imagery}}},
  author = {Hosseiny, Benyamin and Rastiveis, Heidar and Homayouni, Saeid},
  year = {2020},
  month = jan,
  journal = {Remote Sensing},
  volume = {12},
  number = {21},
  pages = {3521},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2072-4292},
  doi = {10.3390/rs12213521},
  urldate = {2024-07-24},
  abstract = {Traditional mapping and monitoring of agricultural fields are expensive, laborious, and may contain human errors. Technological advances in platforms and sensors, followed by artificial intelligence (AI) and deep learning (DL) breakthroughs in intelligent data processing, led to improving the remote sensing applications for precision agriculture (PA). Therefore, technological advances in platforms and sensors and intelligent data processing methods, such as machine learning and DL, and geospatial and remote sensing technologies, have improved the quality of agricultural land monitoring for PA needs. However, providing ground truth data for model training is a time-consuming and tedious task and may contain multiple human errors. This paper proposes an automated and fully unsupervised framework based on image processing and DL methods for plant detection in agricultural lands from very high-resolution drone remote sensing imagery. The proposed framework's main idea is to automatically generate an unlimited amount of simulated training data from the input image. This capability is advantageous for DL methods and can solve their biggest drawback, i.e., requiring a considerable amount of training data. This framework's core is based on the faster regional convolutional neural network (R-CNN) with the backbone of ResNet-101 for object detection. The proposed framework's efficiency was evaluated by two different image sets from two cornfields, acquired by an RGB camera mounted on a drone. The results show that the proposed method leads to an average counting accuracy of 90.9\%. Furthermore, based on the average Hausdorff distance (AHD), an average object detection localization error of 11 pixels was obtained. Additionally, by evaluating the object detection metrics, the resulting mean precision, recall, and F1 for plant detection were 0.868, 0.849, and 0.855, respectively, which seem to be promising for an unsupervised plant detection method.},
  langid = {english},
  keywords = {deep learning,drone imagery,faster R-CNN,plant detection,precision agriculture,ResNet-101}
}

@misc{huangSurveySelfSupervisedFewShot2022,
  title = {A {{Survey}} of {{Self-Supervised}} and {{Few-Shot Object Detection}}},
  author = {Huang, Gabriel and Laradji, Issam and Vazquez, David and {Lacoste-Julien}, Simon and Rodriguez, Pau},
  year = {2022},
  month = aug,
  number = {arXiv:2110.14711},
  eprint = {2110.14711},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.14711},
  urldate = {2025-02-26},
  abstract = {Labeling data is often expensive and time-consuming, especially for tasks such as object detection and instance segmentation, which require dense labeling of the image. While few-shot object detection is about training a model on novel (unseen) object classes with little data, it still requires prior training on many labeled examples of base (seen) classes. On the other hand, self-supervised methods aim at learning representations from unlabeled data which transfer well to downstream tasks such as object detection. Combining few-shot and self-supervised object detection is a promising research direction. In this survey, we review and characterize the most recent approaches on few-shot and self-supervised object detection. Then, we give our main takeaways and discuss future research directions. Project page at https://gabrielhuang.github.io/fsod-survey/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{HybridCNNTransformerNetwork,
  title = {A {{Hybrid CNN-Transformer Network}} for {{Object Detection}} in {{Optical Remote Sensing Images}}: {{Integrating Local}} and {{Global Feature Fusion}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2025-03-03}
}

@misc{IPPC,
  title = {International Standards for Phytosanitary Measures (Ispms)},
  author = {{International Plant Protection Convention}},
  year = {2022},
  urldate = {2025-03-12}
}

@misc{jeevanWhichBackboneUse2024,
  title = {Which {{Backbone}} to {{Use}}: {{A Resource-efficient Domain Specific Comparison}} for {{Computer Vision}}},
  shorttitle = {Which {{Backbone}} to {{Use}}},
  author = {Jeevan, Pranav and Sethi, Amit},
  year = {2024},
  month = jun,
  number = {arXiv:2406.05612},
  eprint = {2406.05612},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.05612},
  urldate = {2025-03-01},
  abstract = {In contemporary computer vision applications, particularly image classification, architectural backbones pre-trained on large datasets like ImageNet are commonly employed as feature extractors. Despite the widespread use of these pre-trained convolutional neural networks (CNNs), there remains a gap in understanding the performance of various resource-efficient backbones across diverse domains and dataset sizes. Our study systematically evaluates multiple lightweight, pre-trained CNN backbones under consistent training settings across a variety of datasets, including natural images, medical images, galaxy images, and remote sensing images. This comprehensive analysis aims to aid machine learning practitioners in selecting the most suitable backbone for their specific problem, especially in scenarios involving small datasets where fine-tuning a pre-trained network is crucial. Even though attention-based architectures are gaining popularity, we observed that they tend to perform poorly under low data finetuning tasks compared to CNNs. We also observed that some CNN architectures such as ConvNeXt, RegNet and EfficientNet performs well compared to others on a diverse set of domains consistently. Our findings provide actionable insights into the performance trade-offs and effectiveness of different backbones, facilitating informed decision-making in model selection for a broad spectrum of computer vision domains. Our code is available here: https://github.com/pranavphoenix/Backbones},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{jeghamEvaluatingEvolutionYOLO2024,
  title = {Evaluating the {{Evolution}} of {{YOLO}} ({{You Only Look Once}}) {{Models}}: {{A Comprehensive Benchmark Study}} of {{YOLO11}} and {{Its Predecessors}}},
  shorttitle = {Evaluating the {{Evolution}} of {{YOLO}} ({{You Only Look Once}}) {{Models}}},
  author = {Jegham, Nidhal and Koh, Chan Young and Abdelatti, Marwan and Hendawi, Abdeltawab},
  year = {2024},
  month = oct,
  number = {arXiv:2411.00201},
  eprint = {2411.00201},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.00201},
  urldate = {2025-02-24},
  abstract = {This study presents a comprehensive benchmark analysis of various YOLO (You Only Look Once) algorithms, from YOLOv3 to the newest addition. It represents the first research to comprehensively evaluate the performance of YOLO11, the latest addition to the YOLO family. It evaluates their performance on three diverse datasets: Traffic Signs (with varying object sizes), African Wildlife (with diverse aspect ratios and at least one instance of the object per image), and Ships and Vessels (with small-sized objects of a single class), ensuring a comprehensive assessment across datasets with distinct challenges. To ensure a robust evaluation, we employ a comprehensive set of metrics, including Precision, Recall, Mean Average Precision (mAP), Processing Time, GFLOPs count, and Model Size. Our analysis highlights the distinctive strengths and limitations of each YOLO version. For example: YOLOv9 demonstrates substantial accuracy but struggles with detecting small objects and efficiency whereas YOLOv10 exhibits relatively lower accuracy due to architectural choices that affect its performance in overlapping object detection but excels in speed and efficiency. Additionally, the YOLO11 family consistently shows superior performance in terms of accuracy, speed, computational efficiency, and model size. YOLO11m achieved a remarkable balance of accuracy and efficiency, scoring mAP50-95 scores of 0.795, 0.81, and 0.325 on the Traffic Signs, African Wildlife, and Ships datasets, respectively, while maintaining an average inference time of 2.4ms, a model size of 38.8Mb, and around 67.6 GFLOPs on average. These results provide critical insights for both industry and academia, facilitating the selection of the most suitable YOLO algorithm for diverse applications and guiding future enhancements.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{jiangDeepSeedlingDeepConvolutional2019,
  title = {{{DeepSeedling}}: {{Deep}} Convolutional Network and {{Kalman}} Filter for Plant Seedling Detection and Counting in the Field},
  shorttitle = {{{DeepSeedling}}},
  author = {Jiang, Yu and Li, Changying and Paterson, Andrew H. and Robertson, Jon S.},
  year = {2019},
  month = nov,
  journal = {Plant Methods},
  volume = {15},
  number = {1},
  pages = {141},
  issn = {1746-4811},
  doi = {10.1186/s13007-019-0528-3},
  urldate = {2025-02-20},
  abstract = {Plant population density is an important factor for agricultural production systems due to its substantial influence on crop yield and quality. Traditionally, plant population density is estimated by using either field assessment or a germination-test-based approach. These approaches can be laborious and inaccurate. Recent advances in deep learning provide new tools to solve challenging computer vision tasks such as object detection, which can be used for detecting and counting plant seedlings in the field. The goal of this study was to develop a deep-learning-based approach to count plant seedlings in the field.},
  langid = {english},
  keywords = {Cotton,Faster RCNN,Object detection,Population density,Video tracking}
}

@misc{jingSelfsupervisedVisualFeature2019,
  title = {Self-Supervised {{Visual Feature Learning}} with {{Deep Neural Networks}}: {{A Survey}}},
  shorttitle = {Self-Supervised {{Visual Feature Learning}} with {{Deep Neural Networks}}},
  author = {Jing, Longlong and Tian, Yingli},
  year = {2019},
  month = feb,
  number = {arXiv:1902.06162},
  eprint = {1902.06162},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1902.06162},
  urldate = {2025-02-25},
  abstract = {Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the main components and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used image and video datasets and the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{jocherGitHubUltralyticsYOLO2023,
  title = {{{GitHub Ultralytics YOLO}}},
  author = {Jocher, Glenn and Qiu, Jing and Chaurasia, Ayush},
  year = {2023},
  month = jan,
  urldate = {2025-03-01},
  abstract = {Ultralytics YOLO11 🚀},
  copyright = {AGPL-3.0}
}

@misc{kangFewshotObjectDetection2019,
  title = {Few-{{Shot Object Detection}} via {{Feature Reweighting}}},
  author = {Kang, Bingyi and Liu, Zhuang and Wang, Xin and Yu, Fisher and Feng, Jiashi and Darrell, Trevor},
  year = {2019},
  month = oct,
  number = {arXiv:1812.01866},
  eprint = {1812.01866},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.01866},
  urldate = {2025-02-26},
  abstract = {Conventional training of a deep CNN based object detector demands a large number of bounding box annotations, which may be unavailable for rare categories. In this work we develop a few-shot object detector that can learn to detect novel objects from only a few annotated examples. Our proposed model leverages fully labeled base classes and quickly adapts to novel classes, using a meta feature learner and a reweighting module within a one-stage detection architecture. The feature learner extracts meta features that are generalizable to detect novel object classes, using training data from base classes with sufficient samples. The reweighting module transforms a few support examples from the novel classes to a global vector that indicates the importance or relevance of meta features for detecting the corresponding objects. These two modules, together with a detection prediction module, are trained end-to-end based on an episodic few-shot learning scheme and a carefully designed loss function. Through extensive experiments we demonstrate that our model outperforms well-established baselines by a large margin for few-shot object detection, on multiple datasets and settings. We also present analysis on various aspects of our proposed model, aiming to provide some inspiration for future few-shot detection works.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{karamiAutomaticPlantCounting2020,
  title = {Automatic {{Plant Counting}} and {{Location Based}} on a {{Few-Shot Learning Technique}}},
  author = {Karami, Azam and Crawford, Melba and Delp, Edward J.},
  year = {2020},
  journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  volume = {13},
  pages = {5872--5886},
  issn = {2151-1535},
  doi = {10.1109/JSTARS.2020.3025790},
  urldate = {2025-02-14},
  abstract = {Plant counting and location are essential for both plant breeding experiments and production agriculture. Stand count indicates the overall emergence of plants compared to the number of seeds that were planted, while location provides information on the associated variability within a plot or geographic area of a field. Deep learning has been successfully applied in various application domains, including plant phenotyping. This article proposes the use of deep learning techniques, more specifically, anchor-free detectors, to identify and count maize plants in RGB images acquired from unmanned aerial vehicles. The results were obtained using a modified CenterNet architecture, with validation performed against manual human annotation. Experimental results demonstrated an overall precision {$>$}95\% for examples where training and testing were performed on the same field. Few-shot learning was also explored, where the trained network was 1) directly applied to the fields in other geographic areas and 2) updated using small quantities of training data from the other locations.},
  keywords = {Agriculture,CenterNet,Detectors,Feature extraction,few-shot learning (FSL),image-based plant phenotyping,localization and counting,Object detection,Plants (biology),Remote sensing,Training,transfer learning (TL)}
}

@article{katariIntegratingAutomatedLabeling2024,
  title = {Integrating {{Automated Labeling Framework}} for {{Enhancing Deep Learning Models}} to {{Count Corn Plants Using UAS Imagery}}},
  author = {Katari, Sushma and Venkatesh, Sandeep and Stewart, Christopher and Khanal, Sami},
  year = {2024},
  month = jan,
  journal = {Sensors},
  volume = {24},
  number = {19},
  pages = {6467},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s24196467},
  urldate = {2025-01-30},
  abstract = {Plant counting is a critical aspect of crop management, providing farmers with valuable insights into seed germination success and within-field variation in crop population density, both of which are key indicators of crop yield and quality. Recent advancements in Unmanned Aerial System (UAS) technology, coupled with deep learning techniques, have facilitated the development of automated plant counting methods. Various computer vision models based on UAS images are available for detecting and classifying crop plants. However, their accuracy relies largely on the availability of substantial manually labeled training datasets. The objective of this study was to develop a robust corn counting model by developing and integrating an automatic image annotation framework. This study used high-spatial-resolution images collected with a DJI Mavic Pro 2 at the V2--V4 growth stage of corn plants from a field in Wooster, Ohio. The automated image annotation process involved extracting corn rows and applying image enhancement techniques to automatically annotate images as either corn or non-corn, resulting in 80\% accuracy in identifying corn plants. The accuracy of corn stand identification was further improved by training four deep learning (DL) models, including InceptionV3, VGG16, VGG19, and Vision Transformer (ViT), with annotated images across various datasets. Notably, VGG16 outperformed the other three models, achieving an F1 score of 0.955. When the corn counts were compared to ground truth data across five test regions, VGG achieved an R2 of 0.94 and an RMSE of 9.95. The integration of an automated image annotation process into the training of the DL models provided notable benefits in terms of model scaling and consistency. The developed framework can efficiently manage large-scale data generation, streamlining the process for the rapid development and deployment of corn counting DL models.},
  langid = {english},
  keywords = {automatic labeling,crop rows,plant stand count,UAS}
}

@misc{khanamYOLOv11OverviewKey2024,
  title = {{{YOLOv11}}: {{An Overview}} of the {{Key Architectural Enhancements}}},
  shorttitle = {{{YOLOv11}}},
  author = {Khanam, Rahima and Hussain, Muhammad},
  year = {2024},
  month = oct,
  number = {arXiv:2410.17725},
  eprint = {2410.17725},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.17725},
  urldate = {2025-03-01},
  abstract = {This study presents an architectural analysis of YOLOv11, the latest iteration in the YOLO (You Only Look Once) series of object detection models. We examine the models architectural innovations, including the introduction of the C3k2 (Cross Stage Partial with kernel size 2) block, SPPF (Spatial Pyramid Pooling - Fast), and C2PSA (Convolutional block with Parallel Spatial Attention) components, which contribute in improving the models performance in several ways such as enhanced feature extraction. The paper explores YOLOv11's expanded capabilities across various computer vision tasks, including object detection, instance segmentation, pose estimation, and oriented object detection (OBB). We review the model's performance improvements in terms of mean Average Precision (mAP) and computational efficiency compared to its predecessors, with a focus on the trade-off between parameter count and accuracy. Additionally, the study discusses YOLOv11's versatility across different model sizes, from nano to extra-large, catering to diverse application needs from edge devices to high-performance computing environments. Our research provides insights into YOLOv11's position within the broader landscape of object detection and its potential impact on real-time computer vision applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{khanSurveyVisionTransformers2023,
  title = {A Survey of the Vision Transformers and Their {{CNN-transformer}} Based Variants},
  author = {Khan, Asifullah and Rauf, Zunaira and Sohail, Anabia and Khan, Abdul Rehman and Asif, Hifsa and Asif, Aqsa and Farooq, Umair},
  year = {2023},
  month = dec,
  journal = {Artificial Intelligence Review},
  volume = {56},
  number = {3},
  pages = {2917--2970},
  issn = {1573-7462},
  doi = {10.1007/s10462-023-10595-0},
  urldate = {2025-02-26},
  abstract = {Vision transformers have become popular as a possible substitute to convolutional neural networks (CNNs) for a variety of computer vision applications. These transformers, with their ability to focus on global relationships in images, offer large learning capacity. However, they may suffer from limited generalization as they do not tend to model local correlation in images. Recently, in vision transformers hybridization of both the convolution operation and self-attention mechanism has emerged, to exploit both the local and global image representations. These hybrid vision transformers, also referred to as CNN-Transformer architectures, have demonstrated remarkable results in vision applications. Given the rapidly growing number of hybrid vision transformers, it has become necessary to provide a taxonomy and explanation of these hybrid architectures. This survey presents a taxonomy of the recent vision transformer architectures and more specifically that of the hybrid vision transformers. Additionally, the key features of these architectures such as the attention mechanisms, positional embeddings, multi-scale processing, and convolution are also discussed. In contrast to the previous survey papers that are primarily focused on individual vision transformer architectures or CNNs, this survey uniquely emphasizes the emerging trend of hybrid vision transformers. By showcasing the potential of hybrid vision transformers to deliver exceptional performance across a range of computer vision tasks, this survey sheds light on the future directions of this rapidly evolving architecture.},
  langid = {english},
  keywords = {Artificial Intelligence,Auto encoder,Channel boosting,Computer vision,Convolutional neural networks,Deep learning,Hybrid vision transformers,Image processing,Self-attention,Transformer}
}

@article{kitanoCornPlantCounting2019,
  title = {Corn {{Plant Counting Using Deep Learning}} and {{UAV Images}}},
  author = {Kitano, Bruno T. and Mendes, Caio C. T. and Geus, Andr{\'e} R. and Oliveira, Henrique C. and Souza, Jefferson R.},
  year = {2019},
  journal = {IEEE Geoscience and Remote Sensing Letters},
  pages = {1--5},
  issn = {1558-0571},
  doi = {10.1109/LGRS.2019.2930549},
  urldate = {2025-02-20},
  abstract = {The adoption of new technologies, such as unmanned aerial vehicles (UAVs), image processing, and machine learning, is disrupting traditional concepts in agriculture, with a new range of possibilities opening in its fields of research. Plant density is one of the most important corn (Zea mays L.) yield factors, yet its precise measurement after the emergence of plants is impractical in large-scale production fields due to the amount of labor required. This letter aims to develop techniques that enable corn plant counting and the automation of this process through deep learning and computational vision, using images of several corn crops obtained using a low-cost unmanned aerial vehicle (UAV) platform assembled with an RGB sensor.},
  keywords = {Agriculture,Cameras,Computer architecture,Deep learning,Deep learning (DL),Image segmentation,plant counting,precision agriculture.,Training,Unmanned aerial vehicles}
}

@book{krausPhotogrammetryGeometryImages2011,
  title = {Photogrammetry: {{Geometry}} from {{Images}} and {{Laser Scans}}},
  shorttitle = {Photogrammetry},
  author = {Kraus, Karl},
  year = {2011},
  month = oct,
  publisher = {De Gruyter},
  doi = {10.1515/9783110892871},
  urldate = {2023-01-25},
  abstract = {This textbook deals with the basics and methods of photogrammetry and laser scanning which are used to determine the form and location of objects, with measurements provided by sensors placed in air planes as well as on terrestrial platforms. Many examples and exercises with solutions are included. Photogrammetry, Laserscanning.},
  isbn = {978-3-11-089287-1},
  langid = {english},
  keywords = {Cartography,Geodesy,Geology,Geophysics}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-03-10},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.}
}

@article{kumleEstimatingPowerGeneralized2021,
  title = {Estimating Power in (Generalized) Linear Mixed Models: {{An}} Open Introduction and Tutorial in {{R}}},
  shorttitle = {Estimating Power in (Generalized) Linear Mixed Models},
  author = {Kumle, Levi and V{\~o}, Melissa L.-H. and Draschkow, Dejan},
  year = {2021},
  month = dec,
  journal = {Behavior Research Methods},
  volume = {53},
  number = {6},
  pages = {2528--2543},
  issn = {1554-3528},
  doi = {10.3758/s13428-021-01546-0},
  urldate = {2025-03-13},
  abstract = {Abstract                            Mixed-effects models are a powerful tool for modeling fixed and random effects simultaneously, but do not offer a feasible analytic solution for estimating the probability that a test correctly rejects the null hypothesis. Being able to estimate this probability, however, is critical for sample size planning, as power is closely linked to the reliability and replicability of empirical findings. A flexible and very intuitive alternative to               analytic               power solutions are               simulation-based               power analyses. Although various tools for conducting simulation-based power analyses for mixed-effects models are available, there is lack of guidance on how to appropriately use them. In this tutorial, we discuss how to estimate power for mixed-effects models in different use cases: first, how to use models that were fit on available (e.g. published) data to determine sample size; second, how to determine the number of stimuli required for sufficient power; and finally, how to conduct sample size planning without available data. Our examples cover both linear and generalized linear models and we provide code and resources for performing simulation-based power analyses on openly accessible data sets. The present work therefore helps researchers to navigate sound research design when using mixed-effects models, by summarizing resources, collating available knowledge, providing solutions and tools, and applying them to real-world problems in sample sizing planning when sophisticated analysis procedures like mixed-effects models are outlined as inferential procedures.},
  langid = {english}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  urldate = {2025-03-04},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  copyright = {2015 Springer Nature Limited},
  langid = {english},
  keywords = {Computer science,Mathematics and computing}
}

@article{liDCYOLOImprovedField2024,
  title = {{{DC-YOLO}}: {{An}} Improved Field Plant Detection Algorithm Based on {{YOLOv7-tiny}}},
  shorttitle = {{{DC-YOLO}}},
  author = {Li, Wenwen and Zhang, Yun},
  year = {2024},
  month = nov,
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  pages = {26430},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-77865-x},
  urldate = {2025-02-20},
  abstract = {Weeding is an important part of agricultural production. With the development of science and technology, automated weeding is regarded as the future development direction, and how to accurately and efficiently detect plants in the field is one of the key points. Corn seedlings and weeds are similar in color, shape and other characteristics, which brings serious challenges to plant detection. In this paper, we propose an improved model based on YOLOv7-tiny, called DC-YOLO. To improve the extraction of key features in the model, we propose Dual Coordinate Attention model (DCA). In addition, we introduce the Content-Aware ReAssembly of FEatures (CARAFE) operator to represent the up-sampling process as a learnable feature reorganization, which enriches the feature information of the sampled images. Finally, we decoupled the detection head to minimize conflicts between features from different tasks. The results show that applying the proposed method to corn and weed datasets, the detection accuracy of the model reaches 95.7\% mean Average Precision (mAP@0.5), the computational effort of the model is 13.083 Giga Floating-point Operations (GFLOPs), and the parameter size is 5.223 Millon (M), which is better than the rest of the mainstream light-weight target detection model.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computer science,Engineering}
}

@article{liImportanceBackboneAdversarial2025,
  title = {On the {{Importance}} of {{Backbone}} to the {{Adversarial Robustness}} of {{Object Detectors}}},
  author = {Li, Xiao and Chen, Hang and Hu, Xiaolin},
  year = {2025},
  journal = {IEEE Transactions on Information Forensics and Security},
  pages = {1--1},
  issn = {1556-6021},
  doi = {10.1109/TIFS.2025.3542964},
  urldate = {2025-02-26},
  abstract = {Object detection is a critical component of various security-sensitive applications, such as autonomous driving and video surveillance. However, existing object detectors are vulnerable to adversarial attacks, which poses a significant challenge to their reliability and security. Through experiments, first, we found that existing works on improving the adversarial robustness of object detectors give a false sense of security. Second, we found that adversarially pre-trained backbone networks were essential for enhancing the adversarial robustness of object detectors. We then proposed a simple yet effective recipe for fast adversarial fine-tuning on object detectors with adversarially pre-trained backbones. Without any modifications to the structure of object detectors, our recipe achieved significantly better adversarial robustness than previous works. Finally, we explored the potential of different modern object detector designs for improving adversarial robustness with our recipe and demonstrated interesting findings, which inspired us to design state-of-the-art (SOTA) robust detectors. Our empirical results set a new milestone for adversarially robust object detection. Code and trained checkpoints will be publicly available after the manuscript is revised.},
  keywords = {Adversarial robustness,Adversarial training,Detectors,Glass box,Head,Object detection,Perturbation methods,Robustness,Security,Standards,Training,Training data}
}

@misc{liMetaSGDLearningLearn2017,
  title = {Meta-{{SGD}}: {{Learning}} to {{Learn Quickly}} for {{Few-Shot Learning}}},
  shorttitle = {Meta-{{SGD}}},
  author = {Li, Zhenguo and Zhou, Fengwei and Chen, Fei and Li, Hang},
  year = {2017},
  month = sep,
  number = {arXiv:1707.09835},
  eprint = {1707.09835},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1707.09835},
  urldate = {2025-02-26},
  abstract = {Few-shot learning is challenging for learning algorithms that learn each task in isolation and from scratch. In contrast, meta-learning learns from many related tasks a meta-learner that can learn a new task more accurately and faster with fewer examples, where the choice of meta-learners is crucial. In this paper, we develop Meta-SGD, an SGD-like, easily trainable meta-learner that can initialize and adapt any differentiable learner in just one step, on both supervised learning and reinforcement learning. Compared to the popular meta-learner LSTM, Meta-SGD is conceptually simpler, easier to implement, and can be learned more efficiently. Compared to the latest meta-learner MAML, Meta-SGD has a much higher capacity by learning to learn not just the learner initialization, but also the learner update direction and learning rate, all in a single meta-learning process. Meta-SGD shows highly competitive performance for few-shot learning on regression, classification, and reinforcement learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@misc{linFocalLossDense2018,
  title = {Focal {{Loss}} for {{Dense Object Detection}}},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  year = {2018},
  month = feb,
  number = {arXiv:1708.02002},
  eprint = {1708.02002},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1708.02002},
  urldate = {2025-03-03},
  abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{linMicrosoftCOCOCommon2015,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Doll{\'a}r, Piotr},
  year = {2015},
  month = feb,
  number = {arXiv:1405.0312},
  eprint = {1405.0312},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1405.0312},
  urldate = {2025-02-25},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{liSeedlingMaizeCounting2022,
  title = {Seedling Maize Counting Method in Complex Backgrounds Based on {{YOLOV5}} and {{Kalman}} Filter Tracking Algorithm},
  author = {Li, Yang and Bao, Zhiyuan and Qi, Jiangtao},
  year = {2022},
  month = nov,
  journal = {Frontiers in Plant Science},
  volume = {13},
  publisher = {Frontiers},
  issn = {1664-462X},
  doi = {10.3389/fpls.2022.1030962},
  urldate = {2025-01-30},
  abstract = {{$<$}p{$>$}Maize population density is one of the most essential factors in agricultural production systems and has a significant impact on maize yield and quality. Therefore, it is essential to estimate maize population density timely and accurately. In order to address the problems of the low efficiency of the manual counting method and the stability problem of traditional image processing methods in the field complex background environment, a deep-learning-based method for counting maize plants was proposed. Image datasets of the maize field were collected by a low-altitude UAV with a camera onboard firstly. Then a real-time detection model of maize plants was trained based on the object detection model YOLOV5. Finally, the tracking and counting method of maize plants was realized through Hungarian matching and Kalman filtering algorithms. The detection model developed in this study had an average precision mAP@0.5 of 90.66\% on the test dataset, demonstrating the effectiveness of the SE-YOLOV5m model for maize plant detection. Application of the model to maize plant count trials showed that maize plant count results from test videos collected at multiple locations were highly correlated with manual count results ({$<$}italic{$>$}R{$^{2}<$}/italic{$>$} = 0.92), illustrating the accuracy and validity of the counting method. Therefore, the maize plant identification and counting method proposed in this study can better achieve the detection and counting of maize plants in complex backgrounds and provides a research basis and theoretical basis for the rapid acquisition of maize plant population density.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Counting prediction,Maize plants,object detection,Video tracing,YOLOv5}
}

@article{liTransformerObjectDetection2023,
  title = {Transformer for Object Detection: {{Review}} and Benchmark},
  shorttitle = {Transformer for Object Detection},
  author = {Li, Yong and Miao, Naipeng and Ma, Liangdi and Shuang, Feng and Huang, Xingwen},
  year = {2023},
  month = nov,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {126},
  pages = {107021},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2023.107021},
  urldate = {2025-03-03},
  abstract = {Object detection is a crucial task in computer vision (CV). With the rapid advancement of Transformer-based models in natural language processing (NLP) and various visual tasks, Transformer structures are becoming increasingly prevalent in CV tasks. In recent years, numerous Transformer-based object detectors have been proposed, achieving performance comparable to mainstream convolutional neural network-based (CNN-based) approaches. To provide researchers with a comprehensive understanding of the development, advantages, disadvantages, and future potential of Transformer-based object detectors in Artificial Intelligence (AI), this paper systematically reviews the mainstream methods and analyzes the limitations and challenges encountered in their current applications, while also offering insights into future research directions. We have reviewed a large number of papers, selected the most prominent Transformer detection methods, and divided them into Transformer Neck and Transformer Backbone categories for introduction and comparative analysis. Furthermore, we have constructed a benchmark using the COCO2017 dataset to evaluate different object detection algorithms. Finally, we summarize the challenges and prospects in this field.},
  keywords = {Benchmark,COCO2017 dataset,Object detection,Review,Transformer-based models}
}

@article{liuEstimatingMaizeSeedling2022,
  title = {Estimating Maize Seedling Number with {{UAV RGB}} Images and Advanced Image Processing Methods},
  author = {Liu, Shuaibing and Yin, Dameng and Feng, Haikuan and Li, Zhenhai and Xu, Xiaobin and Shi, Lei and Jin, Xiuliang},
  year = {2022},
  month = oct,
  journal = {Precision Agriculture},
  volume = {23},
  number = {5},
  pages = {1604--1632},
  issn = {1573-1618},
  doi = {10.1007/s11119-022-09899-y},
  urldate = {2025-01-30},
  abstract = {Accurately identifying the quantity of maize seedlings is useful in improving maize varieties with high seedling emergence rates in a breeding program. The traditional method is to calculate the number of crops manually, which is labor-intensive and time-consuming. Recently, observation methods utilizing a UAV have been widely employed to monitor crop growth due to their low cost, intuitive nature and ability to collect data without contacting the crop. However, most investigations have lacked a systematic strategy for seedling identification. Additionally, estimating the quantity of maize seedlings is challenging due to the complexity of field crop growth environments. The purpose of this research was to rapidly and automatically count maize seedlings. Three models for estimating the quantity of maize seedlings in the field were developed: corner detection model (C), linear regression model (L) and deep learning model (D). The robustness of these maize seedling counting models was validated using RGB images taken at various dates and locations. The maize seedling recognition rate of the three models were 99.78\% (C), 99.9\% (L) and 98.45\% (D) respectively. The L model can be well adapted to different data to identify the number of maize seedlings. The results indicated that the high-throughput and fast method of calculating the number of maize seedlings is a useful tool for maize phenotyping.},
  langid = {english},
  keywords = {Corner detection model,Faster R-CNN,Linear regression model,Maize seedlings,Unmanned aerial vehicle}
}

@inproceedings{liuGroundingDINOMarrying2025,
  title = {Grounding {{DINO}}: {{Marrying DINO}} with {{Grounded Pre-training}} for {{Open-Set Object Detection}}},
  shorttitle = {Grounding {{DINO}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2024},
  author = {Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and Zhang, Lei},
  editor = {Leonardis, Ale{\v s} and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, G{\"u}l},
  year = {2025},
  pages = {38--55},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-72970-6_3},
  abstract = {In this paper, we develop an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for modalities fusion. We first pre-train Grounding DINO on large-scale datasets, including object detection data, grounding data, and caption data, and evaluate the model on both open-set object detection and referring object detection benchmarks. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a 52.5 AP on the COCO zero-shot (In this paper, `zero-shot' refers to scenarios where the training split of the test dataset is not utilized in the training process) detection benchmark. It sets a new record on the ODinW zero-shot benchmark with a mean 26.1 AP. We release some checkpoints and inference codes at https://github.com/IDEA-Research/GroundingDINO.},
  isbn = {978-3-031-72970-6},
  langid = {english},
  keywords = {Image Grounding,Multi-modal learning,Object Detection}
}

@article{liuIntegrateNetDeepLearning2022,
  title = {{{IntegrateNet}}: {{A Deep Learning Network}} for {{Maize Stand Counting From UAV Imagery}} by {{Integrating Density}} and {{Local Count Maps}}},
  shorttitle = {{{IntegrateNet}}},
  author = {Liu, Wenxin and Zhou, Jing and Wang, Biwen and Costa, Martin and Kaeppler, Shawn M. and Zhang, Zhou},
  year = {2022},
  journal = {IEEE Geoscience and Remote Sensing Letters},
  volume = {19},
  pages = {1--5},
  issn = {1558-0571},
  doi = {10.1109/LGRS.2022.3186544},
  urldate = {2025-02-19},
  abstract = {Crop stand count plays an important role in modern agriculture as a reference for precision management and plant breeding. In this study, a new network---IntegrateNet---was proposed to supervise the learning of density map and local count simultaneously and thus boost the model performance by balancing the tradeoff between their errors. The IntegrateNet was trained and validated with an image set containing 124 maize images by an unmanned aerial vehicle. The model achieved an excellent result for 24 test images with the root-mean-square error of 2.28 and the coefficient of determination ( R{$^2$} ) of 0.9578 between the predicted and ground-truth maize stand counts. In conclusion, the proposed model provides an efficient solution for counting maize stands at early stages and could be used as a reference for similar studies.},
  keywords = {Computational modeling,Convolution,Crop counting,Crops,deep learning,density regression,Kernel,local count,maize,Object detection,Predictive models,Training,unmanned aerial vehicle (UAV)}
}

@article{luTasselNetCountingMaize2017,
  title = {{{TasselNet}}: {{Counting}} Maize Tassels in the Wild via Local Counts Regression Network},
  shorttitle = {{{TasselNet}}},
  author = {Lu, Hao and Cao, Zhiguo and Xiao, Yang and Zhuang, Bohan and Shen, Chunhua},
  year = {2017},
  month = nov,
  journal = {Plant Methods},
  volume = {13},
  number = {1},
  pages = {79},
  issn = {1746-4811},
  doi = {10.1186/s13007-017-0224-0},
  urldate = {2025-02-20},
  abstract = {Accurately counting maize tassels is important for monitoring the growth status of maize plants. This tedious task, however, is still mainly done by manual efforts. In the context of modern plant phenotyping, automating this task is required to meet the need of large-scale analysis of genotype and phenotype. In recent years, computer vision technologies have experienced a significant breakthrough due to the emergence of large-scale datasets and increased computational resources. Naturally image-based approaches have also received much attention in plant-related studies. Yet a fact is that most image-based systems for plant phenotyping are deployed under controlled laboratory environment. When transferring the application scenario to unconstrained in-field conditions, intrinsic and extrinsic variations in the wild pose great challenges for accurate counting of maize tassels, which goes beyond the ability of conventional image processing techniques. This calls for further robust computer vision approaches to address in-field variations.},
  langid = {english},
  keywords = {Computer vision,Convolutional neural networks,Deep learning,Maize tassels,Object counting}
}

@article{macheferMaskRCNNRefitting2020,
  title = {Mask {{R-CNN Refitting Strategy}} for {{Plant Counting}} and {{Sizing}} in {{UAV Imagery}}},
  author = {Machefer, M{\'e}lissande and Lemarchand, Fran{\c c}ois and Bonnefond, Virginie and Hitchins, Alasdair and Sidiropoulos, Panagiotis},
  year = {2020},
  month = jan,
  journal = {Remote Sensing},
  volume = {12},
  number = {18},
  pages = {3015},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2072-4292},
  doi = {10.3390/rs12183015},
  urldate = {2025-01-30},
  abstract = {This work introduces a method that combines remote sensing and deep learning into a framework that is tailored for accurate, reliable and efficient counting and sizing of plants in aerial images. The investigated task focuses on two low-density crops, potato and lettuce. This double objective of counting and sizing is achieved through the detection and segmentation of individual plants by fine-tuning an existing deep learning architecture called Mask R-CNN. This paper includes a thorough discussion on the optimal parametrisation to adapt the Mask R-CNN architecture to this novel task. As we examine the correlation of the Mask R-CNN performance to the annotation volume and granularity (coarse or refined) of remotely sensed images of plants, we conclude that transfer learning can be effectively used to reduce the required amount of labelled data. Indeed, a previously trained Mask R-CNN on a low-density crop can improve performances after training on new crops. Once trained for a given crop, the Mask R-CNN solution is shown to outperform a manually-tuned computer vision algorithm. Model performances are assessed using intuitive metrics such as Mean Average Precision (mAP) from Intersection over Union (IoU) of the masks for individual plant segmentation and Multiple Object Tracking Accuracy (MOTA) for detection. The presented model reaches an mAP of 0.418 for potato plants and 0.660 for lettuces for the individual plant segmentation task. In detection, we obtain a MOTA of 0.781 for potato plants and 0.918 for lettuces.},
  langid = {english},
  keywords = {crop mapping,deep learning,image analysis,individual plant segmentation,plant detection,precision agriculture,transfer learning,UAV}
}

@inproceedings{mahmoodHowMuchMore2022,
  title = {How {{Much More Data Do I Need}}? {{Estimating Requirements}} for {{Downstream Tasks}}},
  shorttitle = {How {{Much More Data Do I Need}}?},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Mahmood, Rafid and Lucas, James and Acuna, David and Li, Daiqing and Philion, Jonah and Alvarez, Jose M. and Yu, Zhiding and Fidler, Sania and Law, Marc T.},
  year = {2022},
  month = jun,
  pages = {275--284},
  publisher = {IEEE},
  address = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.00037},
  urldate = {2025-02-21},
  abstract = {Given a small training data set and a learning algorithm, how much more data is necessary to reach a target validation or test performance? This question is of critical importance in applications such as autonomous driving or medical imaging where collecting data is expensive and time-consuming. Overestimating or underestimating data requirements incurs substantial costs that could be avoided with an adequate budget. Prior work on neural scaling laws suggest that the power-law function can fit the validation performance curve and extrapolate it to larger data set sizes. We find that this does not immediately translate to the more difficult downstream task of estimating the required data set size to meet a target performance. In this work, we consider a broad class of computer vision tasks and systematically investigate a family of functions that generalize the power-law function to allow for better estimation of data requirements. Finally, we show that incorporating a tuned correction factor and collecting over multiple rounds significantly improves the performance of the data estimators. Using our guidelines, practitioners can accurately estimate data requirements of machine learning systems to gain savings in both development time and data acquisition costs.},
  isbn = {978-1-6654-6946-3},
  langid = {english}
}

@misc{Maize_seedingDatasetOverview,
  title = {Maize\_seeding {{Dataset}} {$>$} {{Overview}}},
  urldate = {2025-02-26},
  abstract = {174 open source maize\_seeding images. maize\_seeding dataset by objectDetection},
  langid = {english}
}

@misc{MaizeseedlingdetectionDatasetOverview,
  title = {Maize-Seedling-Detection {{Dataset}} {$>$} {{Overview}}},
  urldate = {2025-02-26},
  abstract = {90 open source maizeSeedling images. maize-seedling-detection dataset by fyxdds@icloud.com},
  langid = {english}
}

@article{meierBBCHSystemCoding2009,
  title = {The {{BBCH}} System to Coding the Phenological Growth Stages of Plants -- History and Publications --},
  author = {Meier, Uwe and Bleiholder, Hermann and Buhr, Liselotte and Feller, Carmen and Hack, Helmut and He{\ss}, Martin and Lancashire, Peter D. and Schnock, Uta and Stau{\ss}, Reinhold and {van den Boom}, Theo and Weber, Elfriede and Zwerger, Peter},
  year = {2009},
  month = feb,
  journal = {Journal f{\"u}r Kulturpflanzen},
  volume = {61},
  number = {2},
  pages = {41--52},
  issn = {1867-0938},
  doi = {10.5073/JfK.2009.02.01},
  urldate = {2025-03-01},
  abstract = {Die Entwicklungsstadien der wichtigsten Kulturpflanzen wurden in den vergangenen 19 Jahren von zahlreichen Wissenschaftlern nach den Prinzipien der erweiterten BBCH-Skala beschrieben. Die BBCH-Skalen sind inzwischen weltweit bekannt und werden von Wissenschaft, Administration und Praxis in Landwirtschaft und Gartenbau ebenso genutzt, wie in der Ph{\"a}nologie als integrative Wissenschaft im Bereich Umwelt, Meteorologie und Klimatologie. Diese Tatsache weist darauf hin, dass Zielsetzung und Hoffnung, die seit dem Beginn der Arbeit damit verbunden waren, sich erf{\"u}llt haben. Die BBCH-Skalen haben sich als hilfreich und praktisch erwiesen. Das Ziel, die Harmonisierung in der Anwendung von Dezimalcodes f{\"u}r die Beschreibung der ph{\"a}nologischen Entwicklungsstadien von Kulturpflanzen und Unkr{\"a}utern herbeizuf{\"u}hren, wurde erreicht. Sie erf{\"u}llten auch die Hoffnung der Initiatoren, damit zur Verbesserung der internationalen agrarwissenschaftlichen und interdisziplin{\"a}ren Kommunikation beizutragen. In der vorliegenden Arbeit soll die Geschichte der BBCH-Skalen aufgezeigt werden, weil diese Einblick in die Hintergr{\"u}nde ihrer Entstehung und Entwicklung erlaubt. Alle Original-Publikationen werden mit ihren Literaturquellen zusammenfassend dargelegt. Die Arbeit soll die unterschiedliche Nutzung der BBCH-Skalen in den verschiedenen wissenschaftlichen Disziplinen dokumentieren. Es soll deutlich werden, dass der weltweite Erfolg der BBCH-Skalen vielen Wissenschaftlern rund um den Globus zu verdanken ist.},
  copyright = {Copyright (c) 2009 Verlag Eugen Ulmer KG, Stuttgart},
  langid = {english},
  keywords = {BBCH history,BBCH publications,BBCH-Geschichte,BBCH-Publikationen,BBCH-Skala,BBCHscale,phanologischeEntwicklungsstadien,phenologicalgrowthstages}
}

@misc{MetaLearningBasedIncrementalFewShot,
  title = {Meta-{{Learning-Based Incremental Few-Shot Object Detection}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2025-02-26}
}

@misc{MetaSSDFastAdaptation,
  title = {Meta-{{SSD}}: {{Towards Fast Adaptation}} for {{Few-Shot Object Detection With Meta-Learning}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2025-02-26}
}

@article{mindererScalingOpenVocabularyObject2023,
  title = {Scaling {{Open-Vocabulary Object Detection}}},
  author = {Minderer, Matthias and Gritsenko, Alexey and Houlsby, Neil},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {72983--73007},
  urldate = {2025-02-26},
  langid = {english}
}

@article{mohamedOptimisationDeepLearning2022,
  title = {Optimisation of {{Deep Learning Small-Object Detectors}} with {{Novel Explainable Verification}}},
  author = {Mohamed, Elhassan and Sirlantzis, Konstantinos and Howells, Gareth and Hoque, Sanaul},
  year = {2022},
  month = jul,
  journal = {Sensors},
  volume = {22},
  pages = {1--23},
  publisher = {MDPI},
  issn = {1424-8220},
  urldate = {2025-02-21},
  abstract = {In this paper, we present a novel methodology based on machine learning for identifying the most appropriate from a set of available state-of-the-art object detectors for a given application. Our particular interest is to develop a road map for identifying verifiably optimal selections, especially for challenging applications such as detecting small objects in a mixed-size object dataset. State-of{\dbend}the-art object detection systems often find the localisation of small-size objects challenging since most are usually trained on large-size objects. These contain abundant information as they occupy a large number of pixels relative to the total image size. This fact is normally exploited by the model during training and inference processes. To dissect and understand this process, our approach systematically examines detectors' performances using two very distinct deep convolutional networks. The first is the single-stage YOLO V3 and the second is the double-stage Faster R-CNN. Specifically, our proposed method explores and visually illustrates the impact of feature extraction layers, number of anchor boxes, data augmentation, etc., utilising ideas from the field of explainable Artificial Intelligence (XAI). Our results, for example, show that multi-head YOLO V3 detectors trained using augmented data produce better performance even with a fewer number of anchor boxes. Moreover, robustness regarding the detector's ability to explain how a specific decision was reached is investigated using different explanation techniques. Finally, two new visualisation techniques are proposed, WS-Grad and Concat-Grad, for identifying explanation cues of different detectors. These are applied to specific object detection tasks to illustrate their reliability and transparency with respect to the decision process. It is shown that the proposed techniques can result in high resolution and comprehensive heatmaps of the image areas, significantly affecting detector decisions as compared to the state-of-the-art techniques tested.},
  copyright = {cc\_by},
  langid = {english}
}

@misc{MoXingShiYanShi,
  title = {模型实验室},
  urldate = {2025-03-01}
}

@misc{mullerTrivialAugmentTuningfreeStateoftheArt2021,
  title = {{{TrivialAugment}}: {{Tuning-free Yet State-of-the-Art Data Augmentation}}},
  shorttitle = {{{TrivialAugment}}},
  author = {M{\"u}ller, Samuel G. and Hutter, Frank},
  year = {2021},
  month = aug,
  number = {arXiv:2103.10158},
  eprint = {2103.10158},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.10158},
  urldate = {2025-03-01},
  abstract = {Automatic augmentation methods have recently become a crucial pillar for strong model performance in vision tasks. While existing automatic augmentation methods need to trade off simplicity, cost and performance, we present a most simple baseline, TrivialAugment, that outperforms previous methods for almost free. TrivialAugment is parameter-free and only applies a single augmentation to each image. Thus, TrivialAugment's effectiveness is very unexpected to us and we performed very thorough experiments to study its performance. First, we compare TrivialAugment to previous state-of-the-art methods in a variety of image classification scenarios. Then, we perform multiple ablation studies with different augmentation spaces, augmentation methods and setups to understand the crucial requirements for its performance. Additionally, we provide a simple interface to facilitate the widespread adoption of automatic augmentation methods, as well as our full code base for reproducibility. Since our work reveals a stagnation in many parts of automatic augmentation research, we end with a short proposal of best practices for sustained future progress in automatic augmentation methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{nguyenEvaluationDeepLearning2020,
  title = {An {{Evaluation}} of {{Deep Learning Methods}} for {{Small Object Detection}}},
  author = {Nguyen, Nhat-Duy and Do, Tien and Ngo, Thanh Duc and Le, Duy-Dinh and Valenti, Cesare F.},
  year = {2020},
  month = jan,
  journal = {JECE},
  volume = {2020},
  issn = {2090-0147},
  doi = {10.1155/2020/3189691},
  urldate = {2025-02-21},
  abstract = {Small object detection is an interesting topic in computer vision. With the rapid development in deep learning, it has drawn attention of several researchers with innovations in approaches to join a race. These innovations proposed comprise region proposals, divided grid cell, multiscale feature maps, and new loss function. As a result, performance of object detection has recently had significant improvements. However, most of the state-of-the-art detectors, both in one-stage and two-stage approaches, have struggled with detecting small objects. In this study, we evaluate current state-of-the-art models based on deep learning in both approaches such as Fast RCNN, Faster RCNN, RetinaNet, and YOLOv3. We provide a profound assessment of the advantages and limitations of models. Specifically, we run models with different backbones on different datasets with multiscale objects to find out what types of objects are suitable for each model along with backbones. Extensive empirical evaluation was conducted on 2 standard datasets, namely, a small object dataset and a filtered dataset from PASCAL VOC 2007. Finally, comparative results and analyses are then presented.}
}

@misc{objectdetectionMaize_seedingDataset2024,
  type = {Open {{Source Dataset}}},
  title = {Maize\_seeding {{Dataset}}},
  author = {{objectDetection}},
  year = {2024},
  month = jun,
  publisher = {Roboflow}
}

@article{ogidiBenchmarkingSelfSupervisedContrastive2023,
  title = {Benchmarking {{Self-Supervised Contrastive Learning Methods}} for {{Image-Based Plant Phenotyping}}},
  author = {Ogidi, Franklin C. and Eramian, Mark G. and Stavness, Ian},
  year = {2023},
  month = jan,
  journal = {Plant phenomics (Washington, D.C.)},
  volume = {5},
  pages = {0037},
  publisher = {American Association for the Advancement of Science},
  doi = {10.34133/plantphenomics.0037},
  urldate = {2025-01-30},
  abstract = {The rise of self-supervised learning (SSL) methods in recent years presents an opportunity to leverage unlabeled and domain-specific datasets generated by image-based plant phenotyping platforms to accelerate plant breeding programs. Despite the surge of research on SSL, there has been a scarcity of research exploring the applications of SSL to image-based plant phenotyping tasks, particularly detection and counting tasks. We address this gap by benchmarking the performance of 2 SSL methods---momentum contrast (MoCo) v2 and dense contrastive learning (DenseCL)---against the conventional supervised learning method when transferring learned representations to 4 downstream (target) image-based plant phenotyping tasks: wheat head detection, plant instance detection, wheat spikelet counting, and leaf counting. We studied the effects of the domain of the pretraining (source) dataset on the downstream performance and the influence of redundancy in the pretraining dataset on the quality of learned representations. We also analyzed the similarity of the internal representations learned via the different pretraining methods. We find that supervised pretraining generally outperforms self-supervised pretraining and show that MoCo v2 and DenseCL learn different high-level representations compared to the supervised method. We also find that using a diverse source dataset in the same domain as or a similar domain to the target dataset maximizes performance in the downstream task. Finally, our results show that SSL methods may be more sensitive to redundancy in the pretraining dataset than the supervised pretraining method. We hope that this benchmark/evaluation study will guide practitioners in developing better SSL methods for image-based plant phenotyping.}
}

@misc{oquabDINOv2LearningRobust2024,
  title = {{{DINOv2}}: {{Learning Robust Visual Features}} without {{Supervision}}},
  shorttitle = {{{DINOv2}}},
  author = {Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and {El-Nouby}, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Herv{\'e} and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
  year = {2024},
  month = feb,
  number = {arXiv:2304.07193},
  eprint = {2304.07193},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.07193},
  urldate = {2025-03-05},
  abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{other_scores,
  title = {Nonlinear Regression Analysis and Its Applications},
  author = {Bates, D. M. and Watts, D. G.},
  year = {1988},
  journal = {Journal of Agricultural, Biological, and Environmental Statistics},
  volume = {1},
  number = {1},
  pages = {120--135}
}

@article{PDFCitationPlotLevel2024,
  title = {({{PDF}}) {{Citation}}: {{Plot-Level Maize Early Stage Stand Counting}} and {{Spacing Detection Using Advanced Deep Learning Algorithms Based}} on {{UAV Imagery}}. {{Plot-Level Maize Early Stage Stand Counting}} and {{Spacing Detection Using Advanced Deep Learning Algorithms Based}} on {{UAV Imagery}}},
  shorttitle = {({{PDF}}) {{Citation}}},
  year = {2024},
  month = dec,
  journal = {ResearchGate},
  doi = {10.3390/agronomy13071728},
  urldate = {2025-02-20},
  abstract = {PDF {\textbar} Phenotyping is one of the most important processes in modern breeding, especially for maize, which is an important crop for food, feeds, and... {\textbar} Find, read and cite all the research you need on ResearchGate},
  langid = {english}
}

@article{PP13332024,
  title = {{{{\textsc{PP}}}} 1/333 (1) {{Adoption}} of Digital Technology for Data Generation for the Efficacy Evaluation of Plant Protection Products},
  year = {2024},
  month = nov,
  journal = {EPPO Bulletin},
  pages = {epp.13037},
  issn = {0250-8052, 1365-2338},
  doi = {10.1111/epp.13037},
  urldate = {2025-03-12},
  langid = {english}
}

@article{PP13332024,
  title = {{{{\textsc{PP}}}} 1/333 (1) {{Adoption}} of Digital Technology for Data Generation for the Efficacy Evaluation of Plant Protection Products},
  year = {2024},
  month = nov,
  journal = {EPPO Bulletin},
  pages = {epp.13037},
  issn = {0250-8052, 1365-2338},
  doi = {10.1111/epp.13037},
  urldate = {2025-02-15},
  langid = {english}
}

@misc{rekavandiTransformersSmallObject2023,
  title = {Transformers in {{Small Object Detection}}: {{A Benchmark}} and {{Survey}} of {{State-of-the-Art}}},
  shorttitle = {Transformers in {{Small Object Detection}}},
  author = {Rekavandi, Aref Miri and Rashidi, Shima and Boussaid, Farid and Hoefs, Stephen and Akbas, Emre and {bennamoun}, Mohammed},
  year = {2023},
  month = sep,
  number = {arXiv:2309.04902},
  eprint = {2309.04902},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.04902},
  urldate = {2025-03-03},
  abstract = {Transformers have rapidly gained popularity in computer vision, especially in the field of object recognition and detection. Upon examining the outcomes of state-of-the-art object detection methods, we noticed that transformers consistently outperformed well-established CNN-based detectors in almost every video or image dataset. While transformer-based approaches remain at the forefront of small object detection (SOD) techniques, this paper aims to explore the performance benefits offered by such extensive networks and identify potential reasons for their SOD superiority. Small objects have been identified as one of the most challenging object types in detection frameworks due to their low visibility. We aim to investigate potential strategies that could enhance transformers' performance in SOD. This survey presents a taxonomy of over 60 research studies on developed transformers for the task of SOD, spanning the years 2020 to 2023. These studies encompass a variety of detection applications, including small object detection in generic images, aerial images, medical images, active millimeter images, underwater images, and videos. We also compile and present a list of 12 large-scale datasets suitable for SOD that were overlooked in previous studies and compare the performance of the reviewed studies using popular metrics such as mean Average Precision (mAP), Frames Per Second (FPS), number of parameters, and more. Researchers can keep track of newer studies on our web page, which is available at {\textbackslash}url\{https://github.com/arekavandi/Transformer-SOD\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{RemoteSensingFree,
  title = {Remote {{Sensing}} {\textbar} {{Free Full-Text}} {\textbar} {{An Automated Framework}} for {{Plant Detection Based}} on {{Deep Simulated Learning}} from {{Drone Imagery}}},
  urldate = {2024-07-15}
}

@inproceedings{riberaCountingPlantsUsing2017,
  title = {Counting Plants Using Deep Learning},
  booktitle = {2017 {{IEEE Global Conference}} on {{Signal}} and {{Information Processing}} ({{GlobalSIP}})},
  author = {Ribera, Javier and Chen, Yuhao and Boomsma, Christopher and Delp, Edward J.},
  year = {2017},
  month = nov,
  pages = {1344--1348},
  doi = {10.1109/GlobalSIP.2017.8309180},
  urldate = {2025-02-20},
  abstract = {In this paper we address the task of counting crop plants in a field using Convolutional Neural Networks (CNN). The number of plants in an Unmanned Aerial Vehicle (UAV) image of the field is estimated using regression instead of classification. This avoids to need to know (or guess) the maximum expected number of plants. We also describe a method to extract images of sections or ``plots'' from an orthorectified image of the entire crop field. These images will be used for training and evaluation of the CNN. Our experiments show that we can obtain a Mean Absolute Percentage Error (MAPE) as low as 6.7\% with the Inception-v3 CNN architecture.},
  keywords = {Agriculture,counting,deep learning,Machine learning,neural network,Neural networks,Neurons,phenotyping,regression,Task analysis,Training,Unmanned aerial vehicles}
}

@inproceedings{roggiolaniDomainSpecificPreTraining2023,
  title = {On {{Domain-Specific Pre- Training}} for {{Effective Semantic Perception}} in {{Agricultural Robotics}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Roggiolani, Gianmarco and Magistri, Federico and Guadagnino, Tiziano and Weyler, Jan and Grisetti, Giorgio and Stachniss, Cyrill and Behley, Jens},
  year = {2023},
  month = may,
  pages = {11786--11793},
  doi = {10.1109/ICRA48891.2023.10160624},
  urldate = {2024-07-24},
  abstract = {Agricultural robots have the prospect to enable more efficient and sustainable agricultural production of food, feed, and fiber. Perception of crops and weeds is a central component of agricultural robots that aim to monitor fields and assess the plants as well as their growth stage in an automatic manner. Semantic perception mostly relies on deep learning using supervised approaches, which require time and qualified workers to label fairly large amounts of data. In this paper, we look into the problem of reducing the amount of labels without compromising the final segmentation performance. For robots operating in the field, pre-training networks in a supervised way is already a popular method to reduce the number of required labeled images. We investigate the possibility of pre-training in a self-supervised fashion using data from the target domain. To better exploit this data, we propose a set of domain-specific augmentation strategies. We evaluate our pre-training on semantic segmentation and leaf instance segmentation, two important tasks in our domain. The experimental results suggest that pre-training with domain-specific data paired with our data augmentation strategy leads to superior performance compared to commonly used pre-trainings. Furthermore, the pre-trained networks obtain similar performance to the fully supervised with less labeled data.},
  keywords = {Agricultural robots,Optical fiber networks,Plants (biology),Production,Semantic segmentation,Semantics,Training}
}

@misc{roggiolaniDomainSpecificPreTrainingEffective2023,
  title = {On {{Domain-Specific Pre-Training}} for {{Effective Semantic Perception}} in {{Agricultural Robotics}}},
  author = {Roggiolani, Gianmarco and Magistri, Federico and Guadagnino, Tiziano and Weyler, Jan and Grisetti, Giorgio and Stachniss, Cyrill and Behley, Jens},
  year = {2023},
  month = mar,
  number = {arXiv:2303.12499},
  eprint = {2303.12499},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-24},
  abstract = {Agricultural robots have the prospect to enable more efficient and sustainable agricultural production of food, feed, and fiber. Perception of crops and weeds is a central component of agricultural robots that aim to monitor fields and assess the plants as well as their growth stage in an automatic manner. Semantic perception mostly relies on deep learning using supervised approaches, which require time and qualified workers to label fairly large amounts of data. In this paper, we look into the problem of reducing the amount of labels without compromising the final segmentation performance. For robots operating in the field, pre-training networks in a supervised way is already a popular method to reduce the number of required labeled images. We investigate the possibility of pretraining in a self-supervised fashion using data from the target domain. To better exploit this data, we propose a set of domainspecific augmentation strategies. We evaluate our pre-training on semantic segmentation and leaf instance segmentation, two important tasks in our domain. The experimental results suggest that pre-training with domain-specific data paired with our data augmentation strategy leads to superior performance compared to commonly used pre-trainings. Furthermore, the pre-trained networks obtain similar performance to the fully supervised with less labeled data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics}
}

@article{saBroaderStudyCrossdomain2023,
  title = {A Broader Study of Cross-Domain Few-Shot Object Detection},
  author = {Sa, Liangbing and Yu, Chongchong and Hong, Zhaorui and Zheng, Tong and Liu, Sihan},
  year = {2023},
  month = dec,
  journal = {Applied Intelligence},
  volume = {53},
  number = {23},
  pages = {29465--29485},
  issn = {1573-7497},
  doi = {10.1007/s10489-023-05082-6},
  urldate = {2025-02-24},
  abstract = {Latest studies on few-shot object detection (FSOD) mainly focuses on achieving better performance in novel class through few-shot fine-tuning. This approach is based on the large amount of annotated data in base class to pre-train models. However, in many scenarios it is difficult or impossible to gather substantial data for the base class of the same domain as the novel class. This leads to the cross-domain few-shot object detection problem. Existing research on scenarios of cross-domain few-shot object detection is only limited to generalization where the base and novel class are from highly similar domains of natural images (i.e., from COCO to VOC). In this respect, we propose the Broader Study of Cross-Domain Few-Shot Object Detection (BSCD-FSOD) benchmark. The generalization is thus possible both from a natural image domain to another, for instance from COCO to ExDark (low-light images) and from a natural image domain to a dissimilar one, for instance from COCO to Clipar1k (artistic media images), to NEU-DET (industrial defect images) or to DOTA (satellite images). We have performed extensive experiments on the proposed benchmark to evaluate state-of-art transfer learning approaches. Research results show that, previous state-of-art methods are no longer state-of-art. Pretraining models on COCO instead of VOC can help them learn a wider feature distribution. The less the domains are similar to those of natural images, the more trainable network components fine-tuning thus requires. The performance of all methods is correlated to both the target domain and the degree of similarity between source and target domain. The results we get show the diversity of data distribution and validate the value of our benchmark, which will offer guidance for future researches.},
  langid = {english},
  keywords = {Artificial Intelligence,Benchmark,Cross-domain,Few-shot object detection,Transfer learning}
}

@book{salinasruizGeneralizedLinearMixed2023,
  title = {Generalized {{Linear Mixed Models}} with {{Applications}} in {{Agriculture}} and {{Biology}}},
  author = {Salinas Ru{\'i}z, Josafhat and Montesinos L{\'o}pez, Osval Antonio and Hern{\'a}ndez Ram{\'i}rez, Gabriela and Crossa Hiriart, Jose},
  year = {2023},
  publisher = {Springer International Publishing AG},
  address = {Cham, SWITZERLAND},
  urldate = {2025-03-14},
  isbn = {978-3-031-32800-8}
}

@misc{SeedlingYOLOHighEfficiencyTarget,
  title = {Seedling-{{YOLO}}: {{High-Efficiency Target Detection Algorithm}} for {{Field Broccoli Seedling Transplanting Quality Based}} on {{YOLOv7-Tiny}}},
  urldate = {2025-03-01}
}

@misc{SelfSupervisedLearning,
  title = {Self {{Supervised Learning}} with {{Fastai}} {\textbar} Self\_supervised},
  urldate = {2025-02-06}
}

@article{shahObjectDetectionUsing2023,
  title = {Object Detection Using Convolutional Neural Networks and Transformer-Based Models: A Review},
  shorttitle = {Object Detection Using Convolutional Neural Networks and Transformer-Based Models},
  author = {Shah, Shrishti and Tembhurne, Jitendra},
  year = {2023},
  month = nov,
  journal = {Journal of Electrical Systems and Information Technology},
  volume = {10},
  number = {1},
  pages = {54},
  issn = {2314-7172},
  doi = {10.1186/s43067-023-00123-z},
  urldate = {2025-03-03},
  abstract = {Transformer models are evolving rapidly in standard natural language processing tasks; however, their application is drastically proliferating in computer vision (CV) as well. Transformers are either replacing convolution networks or being used in conjunction with them. This paper aims to differentiate the design of convolutional neural networks (CNNs) built models and models based on transformer, particularly in the domain of object detection. CNNs are designed to capture local spatial patterns through convolutional layers, which is well suited for tasks that involve understanding visual hierarchies and features. However, transformers bring a new paradigm to CV by leveraging self-attention mechanisms, which allows to capture both local and global context in images. Here, we target the various aspects such as basic level of understanding, comparative study, application of attention model, and highlighting tremendous growth along with delivering efficiency are presented effectively for object detection task. The main emphasis of this work is to offer basic understanding of architectures for object detection task and motivates to adopt the same in computer vision tasks. In addition, this paper highlights the evolution of transformer-based models in object detection and their growing importance in the field of computer vision, we also identified the open research direction in the same field.},
  langid = {english},
  keywords = {Artificial Intelligence,Convolutional neural network,Faster R-CNN,Object detection,Segmenter,Semantic segmentation,Transformer-based attention,YOLO's}
}

@article{shortenSurveyImageData2019,
  title = {A Survey on {{Image Data Augmentation}} for {{Deep Learning}}},
  author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
  year = {2019},
  month = jul,
  journal = {Journal of Big Data},
  volume = {6},
  number = {1},
  pages = {60},
  issn = {2196-1115},
  doi = {10.1186/s40537-019-0197-0},
  urldate = {2025-03-01},
  abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
  keywords = {Big data,Data Augmentation,Deep Learning,GANs,Image data}
}

@article{studentProbableErrorMean1908,
  title = {The {{Probable Error}} of a {{Mean}}},
  author = {{Student}},
  year = {1908},
  journal = {Biometrika},
  volume = {6},
  number = {1},
  eprint = {2331554},
  eprinttype = {jstor},
  pages = {1--25},
  publisher = {[Oxford University Press, Biometrika Trust]},
  issn = {0006-3444},
  doi = {10.2307/2331554},
  urldate = {2025-03-14}
}

@misc{sunRevisitingUnreasonableEffectiveness2017,
  title = {Revisiting {{Unreasonable Effectiveness}} of {{Data}} in {{Deep Learning Era}}},
  author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
  year = {2017},
  month = aug,
  number = {arXiv:1707.02968},
  eprint = {1707.02968},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1707.02968},
  urldate = {2025-02-21},
  abstract = {The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition}
}

@misc{tanEfficientDetScalableEfficient2020,
  title = {{{EfficientDet}}: {{Scalable}} and {{Efficient Object Detection}}},
  shorttitle = {{{EfficientDet}}},
  author = {Tan, Mingxing and Pang, Ruoming and Le, Quoc V.},
  year = {2020},
  month = jul,
  number = {arXiv:1911.09070},
  eprint = {1911.09070},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.09070},
  urldate = {2025-03-03},
  abstract = {Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multiscale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and better backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single model and single-scale, our EfficientDet-D7 achieves state-of-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs, being 4x - 9x smaller and using 13x - 42x fewer FLOPs than previous detectors. Code is available at https://github.com/google/automl/tree/master/efficientdet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is All You Need},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  series = {{{NIPS}}'17},
  pages = {6000--6010},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2025-03-04},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  isbn = {978-1-5108-6096-4}
}

@article{velumaniEstimatesMaizePlant2021,
  title = {Estimates of {{Maize Plant Density}} from {{UAV RGB Images Using Faster-RCNN Detection Model}}: {{Impact}} of the {{Spatial Resolution}}},
  shorttitle = {Estimates of {{Maize Plant Density}} from {{UAV RGB Images Using Faster-RCNN Detection Model}}},
  author = {Velumani, K. and {Lopez-Lozano}, R. and Madec, S. and Guo, W. and Gillet, J. and Comar, A. and Baret, F.},
  year = {2021},
  month = jan,
  journal = {Plant phenomics (Washington, D.C.)},
  volume = {2021},
  pages = {9824843},
  issn = {2643-6515},
  doi = {10.34133/2021/9824843},
  urldate = {2025-03-01},
  abstract = {Early-stage plant density is an essential trait that determines the fate of a genotype under given environmental conditions and management practices. The use of RGB images taken from UAVs may replace the traditional visual counting in fields with improved throughput, accuracy, and access to plant localization. However, high-resolution images are required to detect the small plants present at the early stages. This study explores the impact of image ground sampling distance (GSD) on the performances of maize plant detection at three-to-five leaves stage using Faster-RCNN object detection algorithm. Data collected at high resolution ( GSD{$\approx$}0.3cm) over six contrasted sites were used for model training. Two additional sites with images acquired both at high and low ( GSD{$\approx$}0.6cm) resolutions were used to evaluate the model performances. Results show that Faster-RCNN achieved very good plant detection and counting ( rRMSE=0.08) performances when native high-resolution images are used both for training and validation. Similarly, good performances were observed ( rRMSE=0.11) when the model is trained over synthetic low-resolution images obtained by downsampling the native training high-resolution images and applied to the synthetic low-resolution validation images. Conversely, poor performances are obtained when the model is trained on a given spatial resolution and applied to another spatial resolution. Training on a mix of high- and low-resolution images allows to get very good performances on the native high-resolution ( rRMSE=0.06) and synthetic low-resolution ( rRMSE=0.10) images. However, very low performances are still observed over the native low-resolution images ( rRMSE=0.48), mainly due to the poor quality of the native low-resolution images. Finally, an advanced super resolution method based on GAN (generative adversarial network) that introduces additional textural information derived from the native high-resolution images was applied to the native low-resolution validation images. Results show some significant improvement ( rRMSE=0.22) compared to bicubic upsampling approach, while still far below the performances achieved over the native high-resolution images.}
}

@article{vuongEmpiricalStudyAutomatic2025,
  title = {An Empirical Study of Automatic Wildlife Detection Using Drone-Derived Imagery and Object Detection},
  author = {Vuong, Tan and Chang, Miao and Palaparthi, Manas and Howell, Lachlan G. and Bonti, Alessio and Abdelrazek, Mohamed and Nguyen, Duc Thanh},
  year = {2025},
  month = feb,
  journal = {Multimedia Tools and Applications},
  issn = {1573-7721},
  doi = {10.1007/s11042-024-20522-2},
  urldate = {2025-02-26},
  abstract = {Artificial intelligence has the potential to make valuable contributions to wildlife management through cost-effective methods for the collection and interpretation of wildlife data. Recent advances in remotely piloted aircraft systems (RPAS or ``drones'') and thermal imaging technology have created new approaches to collect wildlife data. These emerging technologies could provide promising alternatives to standard labourious field techniques as well as cover much larger areas. In this study, we conduct a comprehensive review and empirical study of drone-based wildlife detection. Specifically, we collect a real-world dataset of drone-derived wildlife thermal detections. Wildlife detections, including arboreal (for instance, koalas, Phascolarctos cinereus) and ground dwelling species in our collected data are annotated via bounding boxes by experts. We then evaluate state-of-the-art object detection algorithms on our collected dataset and on a public dataset. We use these experimental results to identify issues and discuss future directions in automatic animal monitoring using drones.},
  langid = {english},
  keywords = {Artificial intelligence,Drone,Object detection,Wildlife detection}
}

@inproceedings{wangAdvancingImageRecognition2024,
  title = {Advancing {{Image Recognition}}: {{Towards Lightweight Few-shot Learning Model}} for {{Maize Seedling Detection}}},
  shorttitle = {Advancing {{Image Recognition}}},
  booktitle = {Proceedings of the 2024 {{International Conference}} on {{Smart City}} and {{Information System}}},
  author = {Wang, Dongxue and Parthasarathy, Rajamohan and Pan, Xian},
  year = {2024},
  month = aug,
  series = {{{ICSCIS}} '24},
  pages = {635--639},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3685088.3685198},
  urldate = {2025-03-01},
  abstract = {The rise of deep learning in agriculture prompts a reliance on neural networks to solve intricate image processing tasks. While current deep neural networks excel in computer vision, their success hinges on big data, big models, and big computing. However, this dependence poses challenges for widespread adoption in smart agriculture. Big models heavily lean on extensive, labelled datasets for training, posing hurdles in scenarios where agricultural samples are scarce and labelling is costly. Few-shot learning emerges as a solution to swiftly recognize new categories from limited samples in novel agricultural environments. Furthermore, the demand for substantial computing resources during both training and testing phases complicates deploying large models in resource-constrained agricultural settings. Model lightweighting thus becomes crucial to compress parameters for efficient deployment on mobile platforms. This paper addresses these challenges by proposing G-YOLOv5, a lightweight maize seedling detection model. It integrates Ghost convolutional layers to reduce model size and a pyramid network structure for enhanced multi-scale feature fusion. Additionally, an attention mechanism improves feature representation. G-YOLOv5 outperforms YOLOv5 in maize seedling and weed detection precision, recall, and mAP, while reducing model size. Experimental results validate its effectiveness and practicality, offering insights for real-world agricultural applications.},
  isbn = {979-8-4007-1015-5}
}

@misc{wangCSPNetNewBackbone2019,
  title = {{{CSPNet}}: {{A New Backbone}} That Can {{Enhance Learning Capability}} of {{CNN}}},
  shorttitle = {{{CSPNet}}},
  author = {Wang, Chien-Yao and Liao, Hong-Yuan Mark and Yeh, I.-Hau and Wu, Yueh-Hua and Chen, Ping-Yang and Hsieh, Jun-Wei},
  year = {2019},
  month = nov,
  number = {arXiv:1911.11929},
  eprint = {1911.11929},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.11929},
  urldate = {2025-03-02},
  abstract = {Neural networks have enabled state-of-the-art approaches to achieve incredible results on computer vision tasks such as object detection. However, such success greatly relies on costly computation resources, which hinders people with cheap devices from appreciating the advanced technology. In this paper, we propose Cross Stage Partial Network (CSPNet) to mitigate the problem that previous works require heavy inference computations from the network architecture perspective. We attribute the problem to the duplicate gradient information within network optimization. The proposed networks respect the variability of the gradients by integrating feature maps from the beginning and the end of a network stage, which, in our experiments, reduces computations by 20\% with equivalent or even superior accuracy on the ImageNet dataset, and significantly outperforms state-of-the-art approaches in terms of AP50 on the MS COCO object detection dataset. The CSPNet is easy to implement and general enough to cope with architectures based on ResNet, ResNeXt, and DenseNet. Source code is at https://github.com/WongKinYiu/CrossStagePartialNetworks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{wangOneAllUnified2024,
  title = {One to {{All}}: {{Toward}} a {{Unified Model}} for {{Counting Cereal Crop Heads Based}} on {{Few-Shot Learning}}},
  shorttitle = {One to {{All}}},
  author = {Wang, Qiang and Fan, Xijian and Zhuang, Ziqing and Tjahjadi, Tardi and Jin, Shichao and Huan, Honghua and Ye, Qiaolin},
  year = {2024},
  month = nov,
  journal = {Plant phenomics (Washington, D.C.)},
  volume = {6},
  pages = {0271},
  publisher = {American Association for the Advancement of Science},
  doi = {10.34133/plantphenomics.0271},
  urldate = {2025-03-01},
  abstract = {Accurate counting of cereals crops, e.g., maize, rice, sorghum, and wheat, is crucial for estimating grain production and ensuring food security. However, existing methods for counting cereal crops focus predominantly on building models for specific crop head; thus, they lack generalizability to different crop varieties. This paper presents Counting Heads of Cereal Crops Net (CHCNet), which is a unified model designed for counting multiple cereal crop heads by few-shot learning, which effectively reduces labeling costs. Specifically, a refined vision encoder is developed to enhance feature embedding, where a foundation model, namely, the segment anything model (SAM), is employed to emphasize the marked crop heads while mitigating complex background effects. Furthermore, a multiscale feature interaction module is proposed for integrating a similarity metric to facilitate automatic learning of crop-specific features across varying scales, which enhances the ability to describe crop heads of various sizes and shapes. The CHCNet model adopts a 2-stage training procedure. The initial stage focuses on latent feature mining to capture common feature representations of cereal crops. In the subsequent stage, inference is performed without additional training, by extracting domain-specific features of the target crop from selected exemplars to accomplish the counting task. In extensive experiments on 6 diverse crop datasets captured from ground cameras and drones, CHCNet substantially outperformed state-of-the-art counting methods in terms of cross-crop generalization ability, achieving mean absolute errors (MAEs) of 9.96 and 9.38 for maize, 13.94 for sorghum, 7.94 for rice, and 15.62 for mixed crops. A user-friendly interactive demo is available at http://cerealcropnet.com/, where researchers are invited to personally evaluate the proposed CHCNet. The source code for implementing CHCNet is available at https://github.com/Small-flyguy/CHCNet.}
}

@article{wangPlotLevelMaizeEarly2023,
  title = {Plot-{{Level Maize Early Stage Stand Counting}} and {{Spacing Detection Using Advanced Deep Learning Algorithms Based}} on {{UAV Imagery}}},
  author = {Wang, Biwen and Zhou, Jing and Costa, Martin and Kaeppler, Shawn M. and Zhang, Zhou},
  year = {2023},
  month = jul,
  journal = {Agronomy},
  volume = {13},
  number = {7},
  pages = {1728},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2073-4395},
  doi = {10.3390/agronomy13071728},
  urldate = {2025-02-20},
  abstract = {Phenotyping is one of the most important processes in modern breeding, especially for maize, which is an important crop for food, feeds, and industrial uses. Breeders invest considerable time in identifying genotypes with high productivity and stress tolerance. Plant spacing plays a critical role in determining the yield of crops in production settings to provide useful management information. In this study, we propose an automated solution using unmanned aerial vehicle (UAV) imagery and deep learning algorithms to provide accurate stand counting and plant-level spacing variabilities (PSV) in order to facilitate the breeders' decision making. A high-resolution UAV was used to train three deep learning models, namely, YOLOv5, YOLOX, and YOLOR, for both maize stand counting and PSV detection. The results indicate that after optimizing the non-maximum suppression (NMS) intersection of union (IoU) threshold, YOLOv5 obtained the best stand counting accuracy, with a coefficient of determination (R2) of 0.936 and mean absolute error (MAE) of 1.958. Furthermore, the YOLOX model subsequently achieved an F1-score value of 0.896 for PSV detection. This study shows the promising accuracy and reliability of processed UAV imagery for automating stand counting and spacing evaluation and its potential to be implemented further into real-time breeding decision making.},
  langid = {english},
  keywords = {deep learning,maize stand counting,plant spacing variability detection,unmanned aerial vehicle}
}

@inproceedings{wolfTransformersStateoftheArtNatural2020,
  title = {Transformers: {{State-of-the-Art Natural Language Processing}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and {von Platen}, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
  year = {2020},
  month = oct,
  pages = {38--45},
  publisher = {Association for Computational Linguistics},
  address = {Online}
}

@article{wosnerObjectDetectionAgricultural2021,
  title = {Object Detection in Agricultural Contexts: {{A}} Multiple Resolution Benchmark and Comparison to Human},
  shorttitle = {Object Detection in Agricultural Contexts},
  author = {Wosner, Omer and Farjon, Guy and {Bar-Hillel}, Aharon},
  year = {2021},
  month = oct,
  journal = {Computers and Electronics in Agriculture},
  volume = {189},
  pages = {106404},
  issn = {0168-1699},
  doi = {10.1016/j.compag.2021.106404},
  urldate = {2025-01-06},
  abstract = {Visual object detection is an important component in several applications of automated agriculture. In this paper we consider how to properly apply modern deep networks for detection tasks in agricultural contexts, benchmark their performance, and compare their accuracy to human performance. Seven diverse datasets were collected for the benchmark, with three recent networks tested. Experiments have revealed that handling small objects and large scale variance are important failure points, and hence a multiple-resolution approach for network usage was developed, which significantly increased detection accuracy on most datasets. Detection results were compared to human accuracy, judged based on the consistency of multiple annotators. Quantitative analysis shows that for large unoccluded objects accuracy of both algorithms and humans is near perfect, and quantifies the degradation of both due to occlusion and scale difficulties. Finally, application-specific accuracy metrics were suggested based on the needs of several agricultural tasks, and used for estimating the best performing detectors.},
  keywords = {Benchmark,Deep networks,Multiple resolution processing,Object detection}
}

@article{WTO_SPS_Agreement,
  title = {The {{WTO}} Agreement on the Application of Sanitary and Phytosanitary Measures ({{SPS}} Agreement)},
  author = {{World Trade Organization}},
  year = {1995},
  journal = {World Trade Organization},
  urldate = {2025-03-12}
}

@misc{wuDeeperLookSalient2020,
  title = {A {{Deeper Look}} at {{Salient Object Detection}}: {{Bi-stream Network}} with a {{Small Training Dataset}}},
  shorttitle = {A {{Deeper Look}} at {{Salient Object Detection}}},
  author = {Wu, Zhenyu and Li, Shuai and Chen, Chenglizhao and Hao, Aimin and Qin, Hong},
  year = {2020},
  month = aug,
  number = {arXiv:2008.02938},
  eprint = {2008.02938},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2008.02938},
  urldate = {2025-02-21},
  abstract = {Compared with the conventional hand-crafted approaches, the deep learning based methods have achieved tremendous performance improvements by training exquisitely crafted fancy networks over large-scale training sets. However, do we really need large-scale training set for salient object detection (SOD)? In this paper, we provide a deeper insight into the interrelationship between the SOD performances and the training sets. To alleviate the conventional demands for large-scale training data, we provide a feasible way to construct a novel small-scale training set, which only contains 4K images. Moreover, we propose a novel bi-stream network to take full advantage of our proposed small training set, which is consisted of two feature backbones with different structures, achieving complementary semantical saliency fusion via the proposed gate control unit. To our best knowledge, this is the first attempt to use a small-scale training set to outperform state-of-the-art models which are trained on large-scale training sets; nevertheless, our method can still achieve the leading state-of-the-art performance on five benchmark datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@inproceedings{wuMetaRCNNMetaLearning2020,
  title = {Meta-{{RCNN}}: {{Meta Learning}} for {{Few-Shot Object Detection}}},
  shorttitle = {Meta-{{RCNN}}},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Multimedia}}},
  author = {Wu, Xiongwei and Sahoo, Doyen and Hoi, Steven},
  year = {2020},
  month = oct,
  series = {{{MM}} '20},
  pages = {1679--1687},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3394171.3413832},
  urldate = {2025-02-26},
  abstract = {Despite significant advances in deep learning based object detection in recent years, training effective detectors in a small data regime remains an open challenge. This is very important since labelling training data for object detection is often very expensive and time-consuming. In this paper, we investigate the problem of few-shot object detection, where a detector has access to only limited amounts of annotated data. Based on the meta-learning principle, we propose a new meta-learning framework for object detection named "Meta-RCNN", which learns the ability to perform few-shot detection via meta-learning. Specifically, Meta-RCNN learns an object detector in an episodic learning paradigm on the (meta) training data. This learning scheme helps acquire a prior which enables Meta-RCNN to do few-shot detection on novel tasks. Built on top of the popular Faster RCNN detector, in Meta-RCNN, both the Region Proposal Network (RPN) and the object classification branch are meta-learned. The meta-trained RPN learns to provide class-specific proposals, while the object classifier learns to do few-shot classification. The novel loss objectives and learning strategy of Meta-RCNN can be trained in an end-to-end manner. We demonstrate the effectiveness of Meta-RCNN in few-shot detection on three datasets (Pascal-VOC, ImageNet-LOC and MSCOCO) with promising results.},
  isbn = {978-1-4503-7988-5}
}

@article{wuOptimizingConnectedComponent2005,
  title = {Optimizing Connected Component Labeling Algorithms},
  author = {Wu, Kesheng and Otoo, Ekow and Shoshani, Arie},
  year = {2005},
  month = jan,
  urldate = {2025-03-05},
  abstract = {This paper presents two new strategies that can be used to greatly improve the speed of connected component labeling algorithms. To assign a label to a new object, most connected component labeling algorithms use a scanning step that examines some of its neighbors. The first strategy exploits the dependencies among them to reduce the number of neighbors examined. When considering 8-connected components in a 2D image, this can reduce the number of neighbors examined from four to one in many cases. The second strategy uses an array to store the equivalence information among the labels. This replaces the pointer based rooted trees used to store the same equivalence information. It reduces the memory required and also produces consecutive final labels. Using an array instead of the pointer based rooted trees speeds up the connected component labeling algorithms by a factor of 5 {\textasciitilde}; 100 in our tests on random binary images.},
  langid = {english}
}

@misc{xuDeViTDecomposingVision2023,
  title = {{{DeViT}}: {{Decomposing Vision Transformers}} for {{Collaborative Inference}} in {{Edge Devices}}},
  shorttitle = {{{DeViT}}},
  author = {Xu, Guanyu and Hao, Zhiwei and Luo, Yong and Hu, Han and An, Jianping and Mao, Shiwen},
  year = {2023},
  month = sep,
  number = {arXiv:2309.05015},
  eprint = {2309.05015},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.05015},
  urldate = {2025-02-26},
  abstract = {Recent years have witnessed the great success of vision transformer (ViT), which has achieved state-of-the-art performance on multiple computer vision benchmarks. However, ViT models suffer from vast amounts of parameters and high computation cost, leading to difficult deployment on resource-constrained edge devices. Existing solutions mostly compress ViT models to a compact model but still cannot achieve real-time inference. To tackle this issue, we propose to explore the divisibility of transformer structure, and decompose the large ViT into multiple small models for collaborative inference at edge devices. Our objective is to achieve fast and energy-efficient collaborative inference while maintaining comparable accuracy compared with large ViTs. To this end, we first propose a collaborative inference framework termed DeViT to facilitate edge deployment by decomposing large ViTs. Subsequently, we design a decomposition-and-ensemble algorithm based on knowledge distillation, termed DEKD, to fuse multiple small decomposed models while dramatically reducing communication overheads, and handle heterogeneous models by developing a feature matching module to promote the imitations of decomposed models from the large ViT. Extensive experiments for three representative ViT backbones on four widely-used datasets demonstrate our method achieves efficient collaborative inference for ViTs and outperforms existing lightweight ViTs, striking a good trade-off between efficiency and accuracy. For example, our DeViTs improves end-to-end latency by 2.89\${\textbackslash}times\$ with only 1.65\% accuracy sacrifice using CIFAR-100 compared to the large ViT, ViT-L/16, on the GPU server. DeDeiTs surpasses the recent efficient ViT, MobileViT-S, by 3.54\% in accuracy on ImageNet-1K, while running 1.72\${\textbackslash}times\$ faster and requiring 55.28\% lower energy consumption on the edge device.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Performance}
}

@article{yangSurveyFewshotLearning2022,
  title = {A Survey of Few-Shot Learning in Smart Agriculture: {{Developments}}, Applications, and Challenges},
  shorttitle = {A Survey of Few-Shot Learning in Smart Agriculture},
  author = {Yang, Jiachen and Guo, Xiaolan and Li, Yang and Marinello, Francesco and Ercisli, Sezai and Zhang, Zhuo},
  year = {2022},
  month = mar,
  journal = {Plant Methods},
  volume = {18},
  number = {1},
  pages = {28},
  issn = {1746-4811},
  doi = {10.1186/s13007-022-00866-2},
  urldate = {2025-02-24},
  abstract = {With the rise of artificial intelligence, deep learning is gradually applied to the field of agriculture and plant science. However, the excellent performance of deep learning needs to be established on massive numbers of samples. In the field of plant science and biology, it is not easy to obtain a large amount of labeled data. The emergence of few-shot learning solves this problem. It imitates the ability of humans' rapid learning and can learn a new task with only a small number of labeled samples, which greatly reduces the time cost and financial resources. At present, the advanced few-shot learning methods are mainly divided into four categories based on: data augmentation, metric learning, external memory, and parameter optimization, solving the over-fitting problem from different viewpoints. This review comprehensively expounds on few-shot learning in smart agriculture, introduces the definition of few-shot learning, four kinds of learning methods, the publicly available datasets for few-shot learning, various applications in smart agriculture, and the challenges in smart agriculture in future development.},
  keywords = {Data augmentation,Deep learning,Few-shot learning,Metric learning}
}

@article{yangSurveyFewshotLearning2022a,
  title = {A Survey of Few-Shot Learning in Smart Agriculture: {{Developments}}, Applications, and Challenges},
  shorttitle = {A Survey of Few-Shot Learning in Smart Agriculture},
  author = {Yang, Jiachen and Guo, Xiaolan and Li, Yang and Marinello, Francesco and Ercisli, Sezai and Zhang, Zhuo},
  year = {2022},
  month = mar,
  journal = {Plant Methods},
  volume = {18},
  number = {1},
  pages = {28},
  issn = {1746-4811},
  doi = {10.1186/s13007-022-00866-2},
  urldate = {2025-03-01},
  abstract = {With the rise of artificial intelligence, deep learning is gradually applied to the field of agriculture and plant science. However, the excellent performance of deep learning needs to be established on massive numbers of samples. In the field of plant science and biology, it is not easy to obtain a large amount of labeled data. The emergence of few-shot learning solves this problem. It imitates the ability of humans' rapid learning and can learn a new task with only a small number of labeled samples, which greatly reduces the time cost and financial resources. At present, the advanced few-shot learning methods are mainly divided into four categories based on: data augmentation, metric learning, external memory, and parameter optimization, solving the over-fitting problem from different viewpoints. This review comprehensively expounds on few-shot learning in smart agriculture, introduces the definition of few-shot learning, four kinds of learning methods, the publicly available datasets for few-shot learning, various applications in smart agriculture, and the challenges in smart agriculture in future development.},
  langid = {english},
  keywords = {Data augmentation,Deep learning,Few-shot learning,Metric learning}
}

@misc{YOLOv5YOLOv8YOLOv10,
  title = {{{YOLOv5}}, {{YOLOv8}} and {{YOLOv10}}: {{The Go-To Detectors}} for {{Real-time Vision}}},
  urldate = {2025-03-01}
}

@misc{YouOnlyLook,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}} {\textbar} {{IEEE Conference Publication}} {\textbar} {{IEEE Xplore}}},
  urldate = {2025-02-21}
}

@misc{yuqianLovelyqianCDFSODbenchmark2025,
  title = {Lovelyqian/{{CDFSOD-benchmark}}},
  author = {Yuqian, Fu},
  year = {2025},
  month = feb,
  urldate = {2025-02-14},
  abstract = {A benchmark for cross-domain few-shot object detection (ECCV24 paper: Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector)},
  copyright = {Apache-2.0}
}

@article{zhangComparisonYOLObasedSorghum2025,
  title = {Comparison of {{YOLO-based}} Sorghum Spike Identification Detection Models and Monitoring at the Flowering Stage},
  author = {Zhang, Song and Yang, Yehua and Tu, Lei and Fu, Tianling and Chen, Shenxi and Cen, Fulang and Yang, Sanwei and Zhao, Quanzhi and Gao, Zhenran and He, Tengbing},
  year = {2025},
  month = feb,
  journal = {Plant Methods},
  volume = {21},
  number = {1},
  pages = {20},
  issn = {1746-4811},
  doi = {10.1186/s13007-025-01338-z},
  urldate = {2025-03-02},
  abstract = {Monitoring sorghum during the flowering stage is essential for effective fertilization management and improving yield quality, with spike identification serving as the core component of this process. Factors such as varying heights and weather conditions significantly influence the accuracy of sorghum spike detection models, and few comparative studies exist on model performance under different conditions. YOLO (You Only Look Once) is a deep learning object detection algorithm. In this research, images of sorghum during the flowering stage were captured at two heights (15 m and 30 m) in 2023 via a UAV and utilized to train and evaluate variants of YOLOv5, YOLOv8, YOLOv9, and YOLOv10. This investigation aimed to assess the impact of dataset size on model accuracy and predict sorghum flowering stages. The results indicated that YOLOv5, YOLOv8, YOLOv9, and YOLOv10 achieved mAP@50 values of 0.971, 0.968, 0.967, and 0.965, respectively, with dataset sizes ranging from 200 to 350. YOLOv8m performed best on 15sunny and 15cloudy clouds and, overall, exhibited superior adaptability and generalizability. The predictions of the flowering stage using YOLOv8m were more accurate at heights between 12 and 15 m, with R2 values ranging from 0.88 to 0.957 and rRMSE values between 0.111 and 0.396. This research addresses a significant gap in the comparative evaluation of models for sorghum spike detection, identifies YOLOv8m as the most effective model, and advances flowering stage monitoring. These findings provide theoretical and technical foundations for the application of YOLO models in sorghum spike detection and flowering stage monitoring. These findings provide a technical means for the timely and efficient management of sorghum flowering.},
  keywords = {Flowering stage monitoring,Sorghum,Spike identification,UAV,YOLO}
}

@book{zhangMetaDETRFewShotObject2021,
  title = {Meta-{{DETR}}: {{Few-Shot Object Detection}} via {{Unified Image-Level Meta-Learning}}},
  shorttitle = {Meta-{{DETR}}},
  author = {Zhang, Gongjie and Luo, Zhipeng and Cui, Kaiwen and Lu, Shijian},
  year = {2021},
  month = mar,
  doi = {10.48550/arXiv.2103.11731},
  abstract = {Few-shot object detection aims at detecting novel objects with only a few annotated examples. Prior works have proved meta-learning a promising solution, and most of them essentially address detection by meta-learning over regions for their classification and location fine-tuning. However, these methods substantially rely on initially well-located region proposals, which are usually hard to obtain under the few-shot settings. This paper presents a novel meta-detector framework, namely Meta-DETR, which eliminates region-wise prediction and instead meta-learns object localization and classification at image level in a unified and complementary manner. Specifically, it first encodes both support and query images into category-specific features and then feeds them into a category-agnostic decoder to directly generate predictions for specific categories. To facilitate meta-learning with deep networks, we design a simple but effective Semantic Alignment Mechanism (SAM), which aligns high-level and low-level feature semantics to improve the generalization of meta-learned representations. Experiments over multiple few-shot object detection benchmarks show that Meta-DETR outperforms state-of-the-art methods by large margins.}
}

@misc{zhaoDETRsBeatYOLOs2024,
  title = {{{DETRs Beat YOLOs}} on {{Real-time Object Detection}}},
  author = {Zhao, Yian and Lv, Wenyu and Xu, Shangliang and Wei, Jinman and Wang, Guanzhong and Dang, Qingqing and Liu, Yi and Chen, Jie},
  year = {2024},
  month = apr,
  number = {arXiv:2304.08069},
  eprint = {2304.08069},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.08069},
  urldate = {2025-03-01},
  abstract = {The YOLO series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy. However, we observe that the speed and accuracy of YOLOs are negatively affected by the NMS. Recently, end-to-end Transformer-based detectors (DETRs) have provided an alternative to eliminating NMS. Nevertheless, the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding NMS. In this paper, we propose the Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. We build RT-DETR in two steps, drawing on the advanced DETR: first we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy. Specifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. Then, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy. In addition, RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. Our RT-DETR-R50 / R101 achieves 53.1\% / 54.3\% AP on COCO and 108 / 74 FPS on T4 GPU, outperforming previously advanced YOLOs in both speed and accuracy. We also develop scaled RT-DETRs that outperform the lighter YOLO detectors (S and M models). Furthermore, RT-DETR-R50 outperforms DINO-R50 by 2.2\% AP in accuracy and about 21 times in FPS. After pre-training with Objects365, RT-DETR-R50 / R101 achieves 55.3\% / 56.2\% AP. The project page: https://zhao-yian.github.io/RTDETR.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{zhaoStudyLightweightModel2023,
  title = {Study on {{Lightweight Model}} of {{Maize Seedling Object Detection Based}} on {{YOLOv7}}},
  author = {Zhao, Kai and Zhao, Lulu and Zhao, Yanan and Deng, Hanbing},
  year = {2023},
  month = jan,
  journal = {Applied Sciences},
  volume = {13},
  number = {13},
  pages = {7731},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app13137731},
  urldate = {2025-01-30},
  abstract = {Traditional maize seedling detection mainly relies on manual observation and experience, which is time-consuming and prone to errors. With the rapid development of deep learning and object-detection technology, we propose a lightweight model LW-YOLOv7 to address the above issues. The new model can be deployed on mobile devices with limited memory and real-time detection of maize seedlings in the field. LW-YOLOv7 is based on YOLOv7 but incorporates GhostNet as the backbone network to reduce parameters. The Convolutional Block Attention Module (CBAM) enhances the network's attention to the target region. In the head of the model, the Path Aggregation Network (PANet) is replaced with a Bi-Directional Feature Pyramid Network (BiFPN) to improve semantic and location information. The SIoU loss function is used during training to enhance bounding box regression speed and detection accuracy. Experimental results reveal that LW-YOLOv7 outperforms YOLOv7 in terms of accuracy and parameter reduction. Compared to other object-detection models like Faster RCNN, YOLOv3, YOLOv4, and YOLOv5l, LW-YOLOv7 demonstrates increased accuracy, reduced parameters, and improved detection speed. The results indicate that LW-YOLOv7 is suitable for real-time object detection of maize seedlings in field environments and provides a practical solution for efficiently counting the number of seedling maize plants.},
  langid = {english},
  keywords = {attention models,detection model,lightweight,seedling maize,YOLOv7}
}

@inproceedings{zongDETRsCollaborativeHybrid2023,
  title = {{{DETRs}} with {{Collaborative Hybrid Assignments Training}}},
  booktitle = {2023 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Zong, Zhuofan and Song, Guanglu and Liu, Yu},
  year = {2023},
  month = oct,
  pages = {6725--6735},
  publisher = {IEEE},
  address = {Paris, France},
  doi = {10.1109/ICCV51070.2023.00621},
  urldate = {2025-02-25},
  abstract = {In this paper, we provide the observation that too few queries assigned as positive samples in DETR with oneto-one set matching leads to sparse supervision on the encoder's output which considerably hurt the discriminative feature learning of the encoder and vice visa for attention learning in the decoder. To alleviate this, we present a novel collaborative hybrid assignments training scheme, namely Co-DETR, to learn more efficient and effective DETR-based detectors from versatile label assignment manners. This new training scheme can easily enhance the encoder's learning ability in end-to-end detectors by training the multiple parallel auxiliary heads supervised by one-to-many label assignments such as ATSS and Faster RCNN. In addition, we conduct extra customized positive queries by extracting the positive coordinates from these auxiliary heads to improve the training efficiency of positive samples in the decoder. In inference, these auxiliary heads are discarded and thus our method introduces no additional parameters and computational cost to the original detector while requiring no hand-crafted non-maximum suppression (NMS). We conduct extensive experiments to evaluate the effectiveness of the proposed approach on DETR variants, including DAB-DETR, Deformable-DETR, and DINO-DeformableDETR. The state-of-the-art DINO-Deformable-DETR with Swin-L can be improved from 58.5\% to 59.5\% AP on COCO val. Surprisingly, incorporated with ViT-L backbone, we achieve 66.0\% AP on COCO test-dev and 67.9\% AP on LVIS val, outperforming previous methods by clear margins with much fewer model sizes. Codes are available at https://github.com/Sense-X/Co-DETR.},
  isbn = {979-8-3503-0718-4},
  langid = {english}
}

@misc{zotero-1243,
  urldate = {2025-03-12},
  howpublished = {https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=OJ:L:1997:265:FULL}
}

@misc{zotero-1245,
  urldate = {2025-03-12},
  howpublished = {https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=OJ:L:1997:265:FULL}
}

@article{zouMaizeTasselsDetection2020,
  title = {Maize Tassels Detection: A Benchmark of the State of the Art},
  shorttitle = {Maize Tassels Detection},
  author = {Zou, Hongwei and Lu, Hao and Li, Yanan and Liu, Liang and Cao, Zhiguo},
  year = {2020},
  month = aug,
  journal = {Plant Methods},
  volume = {16},
  number = {1},
  pages = {108},
  issn = {1746-4811},
  doi = {10.1186/s13007-020-00651-z},
  urldate = {2025-02-20},
  abstract = {The population of plants is a crucial indicator in plant phenotyping and agricultural production, such as growth status monitoring, yield estimation, and grain depot management. To enhance the production efficiency and liberate labor force, many automated counting methods have been proposed, in which computer vision-based approaches show great potentials due to the feasibility of high-throughput processing and low cost. In particular, with the success of deep learning, more and more deeper learning-based approaches are introduced to deal with agriculture automation. Since different detection- and regression-based counting models have distinct characteristics, how to choose an appropriate model given the target task at hand remains unexplored and is important for practitioners.},
  langid = {english},
  keywords = {Computer Vision,Convolutional neural networks,Deep learning,Maize tassels,Object counting,Object detection}
}

@book{zuurGAMZeroinflatedModels2019,
  title = {{{GAM}} and Zero-Inflated {{Models}}},
  author = {Zuur, Alain F. and Ieno, Elena N. and Savel'ev, Anatolij A. and Zuur, Alain F.},
  year = {2019},
  series = {Beginner's Guide to Spatial, Temporal and Spatial-Temporal Ecological Data Analysis with {{R-INLA}}},
  number = {volume 2},
  publisher = {Highland Statistics Ltd},
  address = {Newburgh, United Kingdom},
  isbn = {978-0-9571741-9-1},
  langid = {english}
}
