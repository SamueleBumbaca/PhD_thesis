\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\bibstyle{plain}
\HyPL@Entry{0<</S/D>>}
\citation{PP13332024}
\citation{zouMaizeTasselsDetection2020}
\citation{liuIntegrateNetDeepLearning2022,davidPlantDetectionCounting2021,barretoAutomaticUAVbasedCounting2021}
\citation{liuEstimatingMaizeSeedling2022,EstimatesMaizePlant}
\citation{zouMaizeTasselsDetection2020}
\citation{linMicrosoftCOCOCommon2015}
\citation{krausPhotogrammetryGeometryImages2011}
\HyPL@Entry{1<</S/D /St 28>>}
\@writefile{toc}{\contentsline {section}{\numberline {2.1.1}Introduction}{28}{section.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1.1}The Problem of Plant Counting}{28}{subsection.0.1.1}\protected@file@percent }
\citation{heiderSurveyDatasetsComputer2025}
\citation{meierBBCHSystemCoding2009}
\citation{davidPlantDetectionCounting2021,liuIntegrateNetDeepLearning2022}
\citation{Maize_seedingDatasetOverview,MaizeseedlingdetectionDatasetOverview}
\citation{fao2024}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1.2}Case study: Maize Seedling Counting}{29}{subsection.0.1.2}\protected@file@percent }
\citation{davidPlantDetectionCounting2021,garcia-martinezDigitalCountCorn2020}
\citation{lecunDeepLearning2015}
\citation{FasterRCNNRealTime}
\citation{YouOnlyLook}
\citation{vaswaniAttentionAllYou2017}
\citation{carionEndtoEndObjectDetection2020}
\citation{dosovitskiyImageWorth16x162021}
\citation{linMicrosoftCOCOCommon2015}
\citation{jingSelfsupervisedVisualFeature2019}
\citation{14090575ImageNetLarge}
\citation{zongDETRsCollaborativeHybrid2023}
\citation{khanSurveyVisionTransformers2023}
\citation{rekavandiTransformersSmallObject2023,liTransformerObjectDetection2023,amjoudObjectDetectionUsing2023}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1.3}Object Detection approaches}{30}{subsection.0.1.3}\protected@file@percent }
\newlabel{subsec:object_detection_approaches}{{2.1.1.3}{30}{Object Detection approaches}{subsection.0.1.3}{}}
\newlabel{subsec:object_detection_approaches@cref}{{[subsection][3][0,1]2.1.1.3}{[1][30][]30}{}{}{}}
\citation{badgujarAgriculturalObjectDetection2024}
\citation{tanEfficientDetScalableEfficient2020,linFocalLossDense2018,zhangComparisonYOLObasedSorghum2025}
\citation{zhaoDETRsBeatYOLOs2024}
\citation{khanamYOLOv11OverviewKey2024}
\citation{liMetaSGDLearningLearn2017}
\citation{bansalZeroShotObjectDetection2018}
\citation{huangSurveySelfSupervisedFewShot2022}
\citation{MetaLearningBasedIncrementalFewShot,wuMetaRCNNMetaLearning2020,fuMetaSSDFastAdaptation2019,zhangMetaDETRFewShotObject2021}
\citation{saBroaderStudyCrossdomain2023}
\citation{xuDeViTDecomposingVision2023}
\citation{fuCrossDomainFewShotObject2024}
\citation{kangFewshotObjectDetection2019}
\citation{mindererScalingOpenVocabularyObject2023}
\citation{liuGroundingDINOMarrying2025}
\citation{barretoAutomaticUAVbasedCounting2021,gengResearchSegmentationMethod2024,jiangDeepSeedlingDeepConvolutional2019,katariIntegratingAutomatedLabeling2024,kitanoCornPlantCounting2019,liDCYOLOImprovedField2024,liSeedlingMaizeCounting2022,liuIntegrateNetDeepLearning2022,liuEstimatingMaizeSeedling2022,luTasselNetCountingMaize2017,macheferMaskRCNNRefitting2020,velumaniEstimatesMaizePlant2021,wangPlotLevelMaizeEarly2023,zhaoStudyLightweightModel2023}
\citation{davidPlantDetectionCounting2021,andvaagCountingCanolaGeneralizable2024}
\citation{wangPlotLevelMaizeEarly2023,amirkolaeeAdaTreeFormerFewShot2024}
\citation{karamiAutomaticPlantCounting2020,wangAdvancingImageRecognition2024}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1.4}Dataset Size and Quality Requirements}{33}{subsection.0.1.4}\protected@file@percent }
\citation{sunRevisitingUnreasonableEffectiveness2017}
\citation{alhazmiEffectsAnnotationQuality2021}
\citation{hestnessDeepLearningScaling2017,mahmoodHowMuchMore2022}
\citation{nguyenEvaluationDeepLearning2020,brigatoCloseLookDeep2020}
\citation{duSpineNetLearningScalePermuted2020}
\citation{roggiolaniDomainSpecificPreTraining2023}
\citation{hendrycksManyFacesRobustness2020}
\citation{jeevanWhichBackboneUse2024}
\citation{goldblumBattleBackbonesLargeScale2023}
\citation{cubukAutoAugmentLearningAugmentation2019}
\citation{antoniouDataAugmentationGenerative2018}
\citation{shortenSurveyImageData2019,chadebecDataAugmentationVariational2021}
\citation{mullerTrivialAugmentTuningfreeStateoftheArt2021}
\citation{shortenSurveyImageData2019}
\citation{sunRevisitingUnreasonableEffectiveness2017}
\citation{davidPlantDetectionCounting2021,andvaagCountingCanolaGeneralizable2024}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1.5}Study Aim}{35}{subsection.0.1.5}\protected@file@percent }
\citation{meierBBCHSystemCoding2009}
\citation{liuEstimatingMaizeSeedling2022,davidPlantDetectionCounting2021}
\citation{Maize_seedingDatasetOverview,MaizeseedlingdetectionDatasetOverview}
\@writefile{toc}{\contentsline {section}{\numberline {2.1.2}Materials and Methods}{36}{section.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2.1}Datasets}{36}{subsection.0.2.1}\protected@file@percent }
\citation{krizhevskyImageNetClassificationDeep2012}
\citation{davidPlantDetectionCounting2021,garcia-martinezDigitalCountCorn2020,liuEstimatingMaizeSeedling2022}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2.2}Handcrafted object detector}{38}{subsection.0.2.2}\protected@file@percent }
\citation{davidPlantDetectionCounting2021}
\citation{liuEstimatingMaizeSeedling2022}
\citation{Maize_seedingDatasetOverview}
\citation{MaizeseedlingdetectionDatasetOverview}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Summary of Datasets Used in the Study}}{39}{table.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:datasets}{{1}{39}{Summary of Datasets Used in the Study}{table.caption.1}{}}
\newlabel{tab:datasets@cref}{{[table][1][0]1}{[1][39][]39}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Sample images from each dataset used in the study. The top row shows out-of-distribution datasets: \textbf  {(a)} DavidEtAl.2021, \textbf  {(b)} LiuEtAl.2022, \textbf  {(c)} Internet Maize stage V3, and \textbf  {(d)} Internet Maize stage V5. The bottom row shows in-domain datasets: \textbf  {(e)} ID\_1, \textbf  {(f)} ID\_2, and \textbf  {(g)} ID\_3.}}{40}{figure.caption.2}\protected@file@percent }
\newlabel{fig:datasets}{{1}{40}{Sample images from each dataset used in the study. The top row shows out-of-distribution datasets: \textbf {(a)} DavidEtAl.2021, \textbf {(b)} LiuEtAl.2022, \textbf {(c)} Internet Maize stage V3, and \textbf {(d)} Internet Maize stage V5. The bottom row shows in-domain datasets: \textbf {(e)} ID\_1, \textbf {(f)} ID\_2, and \textbf {(g)} ID\_3}{figure.caption.2}{}}
\newlabel{fig:datasets@cref}{{[figure][1][0]1}{[1][40][]40}{}{}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {\centering DavidEtAl.2021}}}{40}{subfigure.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {\centering LiuEtAl.2022}}}{40}{subfigure.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {\centering Internet Maize stage V3}}}{40}{subfigure.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {\centering Internet Maize stage V5}}}{40}{subfigure.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {\centering ID\_1}}}{40}{subfigure.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {\centering ID\_2}}}{40}{subfigure.1.6}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {\centering ID\_3}}}{40}{subfigure.1.7}\protected@file@percent }
\citation{wuOptimizingConnectedComponent2005}
\citation{fischlerRandomSampleConsensus1987}
\citation{jocherGitHubUltralyticsYOLO2023}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces H1}}{42}{algorithm.1}\protected@file@percent }
\newlabel{alg:H1}{{1}{42}{H1}{algorithm.1}{}}
\newlabel{alg:H1@cref}{{[algorithm][1][]1}{[1][41][]42}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2.3}Deep Learning object detectors}{42}{subsection.0.2.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces H2: Part I - Row Detection}}{43}{algorithm.2}\protected@file@percent }
\newlabel{alg:H2_part1}{{2}{43}{H2: Part I - Row Detection}{algorithm.2}{}}
\newlabel{alg:H2_part1@cref}{{[algorithm][2][]2}{[1][41][]43}{}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces H2: Part II - Row Verification}}{44}{algorithm.3}\protected@file@percent }
\newlabel{alg:H2_part2}{{3}{44}{H2: Part II - Row Verification}{algorithm.3}{}}
\newlabel{alg:H2_part2@cref}{{[algorithm][3][]3}{[1][41][]44}{}{}{}}
\citation{oquabDINOv2LearningRobust2024}
\citation{fuCrossDomainFewShotObject2024}
\citation{mindererScalingOpenVocabularyObject2023,liuGroundingDINOMarrying2025}
\citation{wolfTransformersStateoftheArtNatural2020}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Summary of Tested Architectures and Model Sizes\textsuperscript  {1}}}{48}{table.caption.3}\protected@file@percent }
\newlabel{tab:architectures}{{2}{48}{Summary of Tested Architectures and Model Sizes\textsuperscript {1}}{table.caption.3}{}}
\newlabel{tab:architectures@cref}{{[table][2][0]2}{[1][47][]48}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2.4}Minimum dataset size and quality modelling}{48}{subsection.0.2.4}\protected@file@percent }
\citation{mahmoodHowMuchMore2022}
\citation{akyonSlicingAidedHyper2022}
\@writefile{toc}{\contentsline {section}{\numberline {2.1.3}Results}{53}{section.0.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3.1}Handcrafted object detector}{53}{subsection.0.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces HC Object Detector Performance}}{53}{table.caption.4}\protected@file@percent }
\newlabel{tab:HC_results}{{3}{53}{HC Object Detector Performance}{table.caption.4}{}}
\newlabel{tab:HC_results@cref}{{[table][3][0]3}{[1][53][]53}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3.2}Many-shots object detectors}{54}{subsection.0.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{OOD training}{54}{subsubsection*.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Performance of the many-shots object detection models trained on the different out-of-distribution (OOD) datasets. Subplots represent: R², $RMSE$, MAPE, and $mAP$ respectively at the right top, left top, left bottom, and right bottom. Each subplot contains the boxplots positioned at the corresponding dataset size values and indicating the distribution of all the models prediction metric values for each dataset. Each data point is annotated with the , colored according to the model size. Benchmark thresholds are indicated with red dashed horizontal lines for R² (0.85) and $RMSE$ (0.39). Best fit lines for each metric are plotted using different fitting functions (logarithmic, arctan, and algebraic root), indicated with black dashed lines. $GoF$ values and best model are shown in the legend. A secondary x-axis at the top of each subplot shows the dataset names corresponding to the dataset sizes.}}{55}{figure.caption.7}\protected@file@percent }
\newlabel{fig:metrics_OOD_datasets}{{2}{55}{Performance of the many-shots object detection models trained on the different out-of-distribution (OOD) datasets. Subplots represent: R², $RMSE$, MAPE, and $mAP$ respectively at the right top, left top, left bottom, and right bottom. Each subplot contains the boxplots positioned at the corresponding dataset size values and indicating the distribution of all the models prediction metric values for each dataset. Each data point is annotated with the , colored according to the model size. Benchmark thresholds are indicated with red dashed horizontal lines for R² (0.85) and $RMSE$ (0.39). Best fit lines for each metric are plotted using different fitting functions (logarithmic, arctan, and algebraic root), indicated with black dashed lines. $GoF$ values and best model are shown in the legend. A secondary x-axis at the top of each subplot shows the dataset names corresponding to the dataset sizes}{figure.caption.7}{}}
\newlabel{fig:metrics_OOD_datasets@cref}{{[figure][2][0]2}{[1][55][]55}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{ID training}{56}{subsubsection*.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Dataset size vs. model performance (YOLOv5)}}{57}{figure.caption.8}\protected@file@percent }
\newlabel{fig:dataset_size_vs_performance_yolov5}{{3}{57}{Dataset size vs. model performance (YOLOv5)}{figure.caption.8}{}}
\newlabel{fig:dataset_size_vs_performance_yolov5@cref}{{[figure][3][0]3}{[1][57][]57}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Dataset size vs. model performance (YOLOv8)}}{58}{figure.caption.9}\protected@file@percent }
\newlabel{fig:dataset_size_vs_performance_yolov8}{{4}{58}{Dataset size vs. model performance (YOLOv8)}{figure.caption.9}{}}
\newlabel{fig:dataset_size_vs_performance_yolov8@cref}{{[figure][4][0]4}{[1][58][]58}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Dataset size vs. model performance (YOLO11)}}{59}{figure.caption.10}\protected@file@percent }
\newlabel{fig:dataset_size_vs_performance_yolo11}{{5}{59}{Dataset size vs. model performance (YOLO11)}{figure.caption.10}{}}
\newlabel{fig:dataset_size_vs_performance_yolo11@cref}{{[figure][5][0]5}{[1][59][]59}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Dataset size vs. model performance (RT-DETR)}}{60}{figure.caption.11}\protected@file@percent }
\newlabel{fig:dataset_size_vs_performance_rtdetr}{{6}{60}{Dataset size vs. model performance (RT-DETR)}{figure.caption.11}{}}
\newlabel{fig:dataset_size_vs_performance_rtdetr@cref}{{[figure][6][0]6}{[1][60][]60}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Relationship between dataset quality and model performance for all object detection models that achieved the benchmark. The x-axis represents the dataset quality, while the left y-axis represents the $R^2$ values. The red dashed horizontal line represents the benchmark $R^2$ value of 0.85. The legend in the lower right corner of the subplot shows the goodness of fit ($GoF$) for $R^2$.}}{61}{figure.caption.12}\protected@file@percent }
\newlabel{fig:dataset_quality}{{7}{61}{Relationship between dataset quality and model performance for all object detection models that achieved the benchmark. The x-axis represents the dataset quality, while the left y-axis represents the $R^2$ values. The red dashed horizontal line represents the benchmark $R^2$ value of 0.85. The legend in the lower right corner of the subplot shows the goodness of fit ($GoF$) for $R^2$}{figure.caption.12}{}}
\newlabel{fig:dataset_quality@cref}{{[figure][7][0]7}{[1][61][]61}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3.3}Few-shots object detectors}{62}{subsection.0.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The figure shows the relationship between shots and model performance for the CD-ViTO model trained and tested on ID datasets. The x-axis represents the number of shots. The solid lines represent the mean values, while the dashed lines indicate the shots amount/metric prediction model. The left and right y-axis represents the $R^2$ and $mAP$ values respectively. The red dashed horizontal line represents the benchmark $R^2$ value of 0.85. The combined legend in the lower right corner of each subplot shows the goodness of fit ($GoF$) for both $R^2$ and $mAP$.}}{63}{figure.caption.13}\protected@file@percent }
\newlabel{fig:shots_vs_performance1}{{8}{63}{The figure shows the relationship between shots and model performance for the CD-ViTO model trained and tested on ID datasets. The x-axis represents the number of shots. The solid lines represent the mean values, while the dashed lines indicate the shots amount/metric prediction model. The left and right y-axis represents the $R^2$ and $mAP$ values respectively. The red dashed horizontal line represents the benchmark $R^2$ value of 0.85. The combined legend in the lower right corner of each subplot shows the goodness of fit ($GoF$) for both $R^2$ and $mAP$}{figure.caption.13}{}}
\newlabel{fig:shots_vs_performance1@cref}{{[figure][8][0]8}{[1][63][]63}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The figure shows the relationship between shots and model performance for the CD-ViTO model trained and tested on ID datasets. The x-axis represents the number of shots. The solid lines represent the mean values, while the dashed lines indicate the shots amount/metric prediction model. The left and right y-axis represents the $RMSE$ and MAPE values respectively. The red dashed horizontal line represents the benchmark $RMSE$ value of 0.39. The combined legend in the upper right corner of each subplot shows the goodness of fit ($GoF$) for both $RMSE$ and MAPE.}}{64}{figure.caption.14}\protected@file@percent }
\newlabel{fig:shots_vs_performance2}{{9}{64}{The figure shows the relationship between shots and model performance for the CD-ViTO model trained and tested on ID datasets. The x-axis represents the number of shots. The solid lines represent the mean values, while the dashed lines indicate the shots amount/metric prediction model. The left and right y-axis represents the $RMSE$ and MAPE values respectively. The red dashed horizontal line represents the benchmark $RMSE$ value of 0.39. The combined legend in the upper right corner of each subplot shows the goodness of fit ($GoF$) for both $RMSE$ and MAPE}{figure.caption.14}{}}
\newlabel{fig:shots_vs_performance2@cref}{{[figure][9][0]9}{[1][64][]64}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3.4}Zero-shots object detectors}{65}{subsection.0.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces 50 shots CD-ViTO with ViT-B backbone predictions on the 1, 2 and 3 ID test datasets tile examples, respectively from the left hand side to the right. Black bounding boxes are the ground truth annotations, while the bounding boxes in viridis color scale are the model predictions.}}{66}{figure.caption.15}\protected@file@percent }
\newlabel{fig:annotations_few-shots}{{10}{66}{50 shots CD-ViTO with ViT-B backbone predictions on the 1, 2 and 3 ID test datasets tile examples, respectively from the left hand side to the right. Black bounding boxes are the ground truth annotations, while the bounding boxes in viridis color scale are the model predictions}{figure.caption.15}{}}
\newlabel{fig:annotations_few-shots@cref}{{[figure][10][0]10}{[1][66][]66}{}{}{}}
\citation{davidPlantDetectionCounting2021}
\citation{andvaagCountingCanolaGeneralizable2024}
\citation{badgujarAgriculturalObjectDetection2024}
\@writefile{toc}{\contentsline {section}{\numberline {2.1.4}Discussion}{67}{section.0.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4.1}Dataset Source Impact on Object Detection Performance}{67}{subsection.0.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The figure shows the relationship between the OWLv2 model size, used prompt and model performance. The x-axis represents the model settings and the model size. Colors represent the different prompts used. The four subplots show the $mAP$ (upper left corner), $R^2$ (upper right corner), $RMSE$ (lower left corner), and MAPE (lower right corner) values. The red dashed horizontal line in the $R^2$ and the $RMSE$ subplots represents respectively the benchmark of 0.85 and 0.39. }}{68}{figure.caption.16}\protected@file@percent }
\newlabel{fig:zeroshots_vs_performance}{{11}{68}{The figure shows the relationship between the OWLv2 model size, used prompt and model performance. The x-axis represents the model settings and the model size. Colors represent the different prompts used. The four subplots show the $mAP$ (upper left corner), $R^2$ (upper right corner), $RMSE$ (lower left corner), and MAPE (lower right corner) values. The red dashed horizontal line in the $R^2$ and the $RMSE$ subplots represents respectively the benchmark of 0.85 and 0.39}{figure.caption.16}{}}
\newlabel{fig:zeroshots_vs_performance@cref}{{[figure][11][0]11}{[1][68][]68}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The best predictions with the OWLv2 model. The ID\_1, ID\_2 and ID\_3 datasets respectively from the left hand side to the right. Prediction with owlv2-base-patch16-ensemble model of the ID\_1 dataset, and with owlv2-base-patch16 model on the other two datasets. All the predictions are made with the prompt "seedling". Black bounding boxes are the ground truth annotations, while the bounding boxes in viridis color scale are the model predictions.}}{69}{figure.caption.17}\protected@file@percent }
\newlabel{fig:annotations_zero-shots}{{12}{69}{The best predictions with the OWLv2 model. The ID\_1, ID\_2 and ID\_3 datasets respectively from the left hand side to the right. Prediction with owlv2-base-patch16-ensemble model of the ID\_1 dataset, and with owlv2-base-patch16 model on the other two datasets. All the predictions are made with the prompt "seedling". Black bounding boxes are the ground truth annotations, while the bounding boxes in viridis color scale are the model predictions}{figure.caption.17}{}}
\newlabel{fig:annotations_zero-shots@cref}{{[figure][12][0]12}{[1][69][]69}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4.2}Many-Shot Object Detection: Architecture and Dataset Requirements}{70}{subsection.0.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4.3}Dataset Quality Trade-offs}{72}{subsection.0.4.3}\protected@file@percent }
\citation{xuDeViTDecomposingVision2023,mindererScalingOpenVocabularyObject2023}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4.4}Few-Shot and Zero-Shot Approaches: Current Limitations}{73}{subsection.0.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4.5}Handcrafted Methods in the Deep Learning Era}{74}{subsection.0.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4.6}Implications for Practical Applications}{75}{subsection.0.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4.7}Future Work}{77}{subsection.0.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.1.5}Conclusions}{77}{section.0.5}\protected@file@percent }
\bibstyle{plainnat}
\bibdata{Phd_Thesis_SBumbaca}
\bibcite{14090575ImageNetLarge}{{1}{}{{}}{{}}}
\bibcite{EstimatesMaizePlant}{{2}{}{{}}{{}}}
\bibcite{FasterRCNNRealTime}{{3}{}{{}}{{}}}
\bibcite{MaizeseedlingdetectionDatasetOverview}{{4}{}{{}}{{}}}
\bibcite{Maize_seedingDatasetOverview}{{5}{}{{}}{{}}}
\bibcite{MetaLearningBasedIncrementalFewShot}{{6}{}{{}}{{}}}
\bibcite{YouOnlyLook}{{7}{}{{}}{{}}}
\bibcite{PP13332024}{{8}{}{{}}{{}}}
\bibcite{akyonSlicingAidedHyper2022}{{9}{}{{}}{{}}}
\bibcite{alhazmiEffectsAnnotationQuality2021}{{10}{}{{}}{{}}}
\bibcite{amirkolaeeAdaTreeFormerFewShot2024}{{11}{}{{}}{{}}}
\bibcite{amjoudObjectDetectionUsing2023}{{12}{}{{}}{{}}}
\bibcite{andvaagCountingCanolaGeneralizable2024}{{13}{}{{}}{{}}}
\bibcite{antoniouDataAugmentationGenerative2018}{{14}{}{{}}{{}}}
\bibcite{badgujarAgriculturalObjectDetection2024}{{15}{}{{}}{{}}}
\bibcite{bansalZeroShotObjectDetection2018}{{16}{}{{}}{{}}}
\bibcite{barretoAutomaticUAVbasedCounting2021}{{17}{}{{}}{{}}}
\bibcite{brigatoCloseLookDeep2020}{{18}{}{{}}{{}}}
\bibcite{carionEndtoEndObjectDetection2020}{{19}{}{{}}{{}}}
\bibcite{chadebecDataAugmentationVariational2021}{{20}{}{{}}{{}}}
\bibcite{cubukAutoAugmentLearningAugmentation2019}{{21}{}{{}}{{}}}
\bibcite{davidPlantDetectionCounting2021}{{22}{}{{}}{{}}}
\bibcite{dosovitskiyImageWorth16x162021}{{23}{}{{}}{{}}}
\bibcite{duSpineNetLearningScalePermuted2020}{{24}{}{{}}{{}}}
\bibcite{fao2024}{{25}{}{{}}{{}}}
\bibcite{fischlerRandomSampleConsensus1987}{{26}{}{{}}{{}}}
\bibcite{fuMetaSSDFastAdaptation2019}{{27}{}{{}}{{}}}
\bibcite{fuCrossDomainFewShotObject2024}{{28}{}{{}}{{}}}
\bibcite{garcia-martinezDigitalCountCorn2020}{{29}{}{{}}{{}}}
\bibcite{gengResearchSegmentationMethod2024}{{30}{}{{}}{{}}}
\bibcite{goldblumBattleBackbonesLargeScale2023}{{31}{}{{}}{{}}}
\bibcite{heiderSurveyDatasetsComputer2025}{{32}{}{{}}{{}}}
\bibcite{hendrycksManyFacesRobustness2020}{{33}{}{{}}{{}}}
\bibcite{hestnessDeepLearningScaling2017}{{34}{}{{}}{{}}}
\bibcite{huangSurveySelfSupervisedFewShot2022}{{35}{}{{}}{{}}}
\bibcite{jeevanWhichBackboneUse2024}{{36}{}{{}}{{}}}
\bibcite{jiangDeepSeedlingDeepConvolutional2019}{{37}{}{{}}{{}}}
\bibcite{jingSelfsupervisedVisualFeature2019}{{38}{}{{}}{{}}}
\bibcite{jocherGitHubUltralyticsYOLO2023}{{39}{}{{}}{{}}}
\bibcite{kangFewshotObjectDetection2019}{{40}{}{{}}{{}}}
\bibcite{karamiAutomaticPlantCounting2020}{{41}{}{{}}{{}}}
\bibcite{katariIntegratingAutomatedLabeling2024}{{42}{}{{}}{{}}}
\bibcite{khanSurveyVisionTransformers2023}{{43}{}{{}}{{}}}
\bibcite{khanamYOLOv11OverviewKey2024}{{44}{}{{}}{{}}}
\bibcite{kitanoCornPlantCounting2019}{{45}{}{{}}{{}}}
\bibcite{krausPhotogrammetryGeometryImages2011}{{46}{}{{}}{{}}}
\bibcite{krizhevskyImageNetClassificationDeep2012}{{47}{}{{}}{{}}}
\bibcite{lecunDeepLearning2015}{{48}{}{{}}{{}}}
\bibcite{liDCYOLOImprovedField2024}{{49}{}{{}}{{}}}
\bibcite{liSeedlingMaizeCounting2022}{{50}{}{{}}{{}}}
\bibcite{liTransformerObjectDetection2023}{{51}{}{{}}{{}}}
\bibcite{liMetaSGDLearningLearn2017}{{52}{}{{}}{{}}}
\bibcite{linFocalLossDense2018}{{53}{}{{}}{{}}}
\bibcite{linMicrosoftCOCOCommon2015}{{54}{}{{}}{{}}}
\bibcite{liuGroundingDINOMarrying2025}{{55}{}{{}}{{}}}
\bibcite{liuEstimatingMaizeSeedling2022}{{56}{}{{}}{{}}}
\bibcite{liuIntegrateNetDeepLearning2022}{{57}{}{{}}{{}}}
\bibcite{luTasselNetCountingMaize2017}{{58}{}{{}}{{}}}
\bibcite{macheferMaskRCNNRefitting2020}{{59}{}{{}}{{}}}
\bibcite{mahmoodHowMuchMore2022}{{60}{}{{}}{{}}}
\bibcite{meierBBCHSystemCoding2009}{{61}{}{{}}{{}}}
\bibcite{mindererScalingOpenVocabularyObject2023}{{62}{}{{}}{{}}}
\bibcite{mullerTrivialAugmentTuningfreeStateoftheArt2021}{{63}{}{{}}{{}}}
\bibcite{nguyenEvaluationDeepLearning2020}{{64}{}{{}}{{}}}
\bibcite{oquabDINOv2LearningRobust2024}{{65}{}{{}}{{}}}
\bibcite{rekavandiTransformersSmallObject2023}{{66}{}{{}}{{}}}
\bibcite{roggiolaniDomainSpecificPreTraining2023}{{67}{}{{}}{{}}}
\bibcite{saBroaderStudyCrossdomain2023}{{68}{}{{}}{{}}}
\bibcite{shortenSurveyImageData2019}{{69}{}{{}}{{}}}
\bibcite{sunRevisitingUnreasonableEffectiveness2017}{{70}{}{{}}{{}}}
\bibcite{tanEfficientDetScalableEfficient2020}{{71}{}{{}}{{}}}
\bibcite{vaswaniAttentionAllYou2017}{{72}{}{{}}{{}}}
\bibcite{velumaniEstimatesMaizePlant2021}{{73}{}{{}}{{}}}
\bibcite{wangPlotLevelMaizeEarly2023}{{74}{}{{}}{{}}}
\bibcite{wangAdvancingImageRecognition2024}{{75}{}{{}}{{}}}
\bibcite{wolfTransformersStateoftheArtNatural2020}{{76}{}{{}}{{}}}
\bibcite{wuOptimizingConnectedComponent2005}{{77}{}{{}}{{}}}
\bibcite{wuMetaRCNNMetaLearning2020}{{78}{}{{}}{{}}}
\bibcite{xuDeViTDecomposingVision2023}{{79}{}{{}}{{}}}
\bibcite{zhangMetaDETRFewShotObject2021}{{80}{}{{}}{{}}}
\bibcite{zhangComparisonYOLObasedSorghum2025}{{81}{}{{}}{{}}}
\bibcite{zhaoStudyLightweightModel2023}{{82}{}{{}}{{}}}
\bibcite{zhaoDETRsBeatYOLOs2024}{{83}{}{{}}{{}}}
\bibcite{zongDETRsCollaborativeHybrid2023}{{84}{}{{}}{{}}}
\bibcite{zouMaizeTasselsDetection2020}{{85}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\gdef \@abspage@last{67}
