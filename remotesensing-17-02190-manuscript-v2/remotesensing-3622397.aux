\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\bibstyle{plain}
\HyPL@Entry{0<</S/D>>}
\citation{blandinoeffetto2016}
\citation{10287390}
\citation{Saatkamp2019AResearchAF}
\citation{rs13051030}
\citation{PP13332024}
\citation{PP13332024}
\citation{zouMaizeTasselsDetection2020}
\HyPL@Entry{1<</S/D /St 53>>}
\@writefile{toc}{\contentsline {section}{\numberline {2.1.1}Introduction}{53}{section.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1.1}The Problem of Plant Counting}{53}{subsection.0.1.1}\protected@file@percent }
\citation{zouMaizeTasselsDetection2020}
\citation{linMicrosoftCOCOCommon2015}
\citation{krausPhotogrammetryGeometryImages2011}
\citation{pugh_comparison_2021,dhonju_web_2023}
\citation{habib_automated_2016,de_petris_rpas-based_2020}
\citation{zhang_georeferencing_2022}
\citation{farjon_deep-learning-based_2023}
\citation{meierBBCHSystemCoding2009}
\citation{davidPlantDetectionCounting2021,liuIntegrateNetDeepLearning2022}
\citation{Maize_seedingDatasetOverview,MaizeseedlingdetectionDatasetOverview}
\citation{fao2024}
\citation{torres-sanchez_early_2021}
\citation{zhang2020cut,davidPlantDetectionCounting2021}
\citation{davidPlantDetectionCounting2021,garcia-martinezDigitalCountCorn2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1.2}Evolution of Object Detection~Methods}{55}{subsection.0.1.2}\protected@file@percent }
\citation{lecunDeepLearning2015}
\citation{FasterRCNNRealTime}
\citation{YouOnlyLook}
\citation{vaswaniAttentionAllYou2017}
\citation{carionEndtoEndObjectDetection2020}
\citation{dosovitskiyImageWorth16x162021}
\citation{linMicrosoftCOCOCommon2015}
\citation{14090575ImageNetLarge}
\citation{zongDETRsCollaborativeHybrid2023}
\citation{khanSurveyVisionTransformers2023,badgujarAgriculturalObjectDetection2024}
\citation{rekavandiTransformersSmallObject2023,liTransformerObjectDetection2023}
\citation{badgujarAgriculturalObjectDetection2024}
\citation{zhaoDETRsBeatYOLOs2024}
\citation{khanamYOLOv11OverviewKey2024}
\citation{liMetaSGDLearningLearn2017}
\citation{bansalZeroShotObjectDetection2018}
\citation{kangFewshotObjectDetection2019}
\citation{mindererScalingOpenVocabularyObject2023}
\citation{liuGroundingDINOMarrying2025}
\citation{karamiAutomaticPlantCounting2020,wangAdvancingImageRecognition2024}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1.3}Data-Efficient Detection~Methods}{57}{subsection.0.1.3}\protected@file@percent }
\citation{barretoAutomaticUAVbasedCounting2021,liuIntegrateNetDeepLearning2022,kitanoCornPlantCounting2019}
\citation{davidPlantDetectionCounting2021,andvaagCountingCanolaGeneralizable2024}
\citation{sunRevisitingUnreasonableEffectiveness2017}
\citation{alhazmiEffectsAnnotationQuality2021}
\citation{hestnessDeepLearningScaling2017,mahmoodHowMuchMore2022}
\citation{nguyenEvaluationDeepLearning2020}
\citation{duSpineNetLearningScalePermuted2020}
\citation{shortenSurveyImageData2019}
\citation{davidPlantDetectionCounting2021,andvaagCountingCanolaGeneralizable2024}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1.4}Dataset Requirements for Agricultural Object~Detection}{58}{subsection.0.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1.5}Study~Aim}{59}{subsection.0.1.5}\protected@file@percent }
\citation{meierBBCHSystemCoding2009}
\citation{liuEstimatingMaizeSeedling2022,davidPlantDetectionCounting2021}
\citation{Maize_seedingDatasetOverview,MaizeseedlingdetectionDatasetOverview}
\citation{velumaniEstimatesMaizePlant2021}
\@writefile{toc}{\contentsline {section}{\numberline {2.1.2}Materials and~Methods}{61}{section.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2.1}Datasets}{61}{subsection.0.2.1}\protected@file@percent }
\citation{davidPlantDetectionCounting2021}
\citation{liuEstimatingMaizeSeedling2022}
\citation{Maize_seedingDatasetOverview}
\citation{MaizeseedlingdetectionDatasetOverview}
\citation{bumbaca202515235602}
\citation{davidPlantDetectionCounting2021,liuEstimatingMaizeSeedling2022}
\citation{krizhevskyImageNetClassificationDeep2012}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Summary of datasets used in the study.}}{63}{table.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:datasets}{{1}{63}{Summary of datasets used in the study}{table.caption.1}{}}
\newlabel{tab:datasets@cref}{{[table][1][0]1}{[1][62][]63}{}{}{}}
\citation{davidPlantDetectionCounting2021,garcia-martinezDigitalCountCorn2020,liuEstimatingMaizeSeedling2022}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Image examples taken from each dataset, ground truth bounding boxes are shown in red. (\textbf  {a})~DavidEtAl.2021, (\textbf  {b}) LiuEtAl.2022, (\textbf  {c}) Internet Maize stage V3, (\textbf  {d}) Internet Maize stage V5, (\textbf  {e})~ID\_1, (\textbf  {f})~ID\_2, (\textbf  {g})~ID\_3.}}{65}{figure.caption.2}\protected@file@percent }
\newlabel{fig:datasets}{{1}{65}{Image examples taken from each dataset, ground truth bounding boxes are shown in red. (\textbf {a})~DavidEtAl.2021, (\textbf {b}) LiuEtAl.2022, (\textbf {c}) Internet Maize stage V3, (\textbf {d}) Internet Maize stage V5, (\textbf {e})~ID\_1, (\textbf {f})~ID\_2, (\textbf {g})~ID\_3}{figure.caption.2}{}}
\newlabel{fig:datasets@cref}{{[figure][1][0]1}{[1][64][]65}{}{}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {\centering }}}{65}{subfigure.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {\centering }}}{65}{subfigure.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {\centering }}}{65}{subfigure.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {\centering }}}{65}{subfigure.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {\centering }}}{65}{subfigure.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {\centering }}}{65}{subfigure.1.6}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {\centering }}}{65}{subfigure.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2.2}Handcrafted Object~Detector}{65}{subsection.0.2.2}\protected@file@percent }
\citation{fischlerRandomSampleConsensus1987}
\citation{badgujarAgriculturalObjectDetection2024}
\citation{badgujarAgriculturalObjectDetection2024}
\citation{kitanoCornPlantCounting2019,barretoAutomaticUAVbasedCounting2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2.3}Deep Learning Object~Detectors}{67}{subsection.0.2.3}\protected@file@percent }
\citation{tervenComprehensiveReviewYOLO2023}
\citation{zhaoDETRsBeatYOLOs2024}
\citation{carionEndtoEndObjectDetection2020}
\citation{jocherGitHubUltralyticsYOLO2023}
\citation{oquabDINOv2LearningRobust2024}
\citation{fuCrossDomainFewShotObject2024}
\citation{mindererScalingOpenVocabularyObject2023,liuGroundingDINOMarrying2025}
\citation{wolfTransformersStateoftheArtNatural2020}
\citation{zhu_harnessing_2024}
\citation{s24186109}
\citation{chen_taskclip_2024}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Summary of tested architectures and model sizes.}}{73}{table.caption.3}\protected@file@percent }
\newlabel{tab:architectures}{{2}{73}{Summary of tested architectures and model sizes}{table.caption.3}{}}
\newlabel{tab:architectures@cref}{{[table][2][0]2}{[1][72][]73}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2.4}Minimum Dataset Size and Quality~Modelling}{73}{subsection.0.2.4}\protected@file@percent }
\citation{chicco_coefficient_2021}
\citation{draper1998applied}
\citation{chicco_coefficient_2021,draper1998applied}
\citation{draper1998applied}
\citation{ARMSTRONG199269}
\citation{linMicrosoftCOCOCommon2015,everingham_pascal_2010}
\citation{mahmoodHowMuchMore2022}
\citation{hestnessDeepLearningScaling2017}
\citation{vianna_analysis_2024}
\citation{hestnessDeepLearningScaling2017}
\citation{akyonSlicingAidedHyper2022}
\@writefile{toc}{\contentsline {section}{\numberline {2.1.3}Results}{79}{section.0.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3.1}Handcrafted Object~Detector}{79}{subsection.0.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces HC object detector~performance.}}{80}{table.caption.4}\protected@file@percent }
\newlabel{tab:HC_results}{{3}{80}{HC object detector~performance}{table.caption.4}{}}
\newlabel{tab:HC_results@cref}{{[table][3][0]3}{[1][79][]80}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3.2}Many-Shot Object~Detectors}{80}{subsection.0.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{OOD~Training}{80}{subsubsection*.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The figure shows the performance of the many-shot object detection models trained on the different Out-of-Distribution (OOD) datasets. The~subplots represent four different metrics: R², $RMSE$, MAPE, and~$mAP$, respectively, at the right top, left top, left bottom, and~right bottom. Each subplot contains the boxplots positioned at the corresponding dataset size values and indicating the distribution of all the model prediction metric values for each dataset. Benchmark thresholds are indicated with red dashed horizontal lines for R² (0.85) and $RMSE$ (0.39). Best fit lines for each metric are plotted using different fitting functions, indicated with black dashed lines. $GoF$ values and best model are shown in the legend. A secondary x-axis at the top of each subplot shows the dataset names corresponding to the dataset sizes.}}{82}{figure.caption.6}\protected@file@percent }
\newlabel{fig:metrics_OOD_datasets}{{2}{82}{The figure shows the performance of the many-shot object detection models trained on the different Out-of-Distribution (OOD) datasets. The~subplots represent four different metrics: R², $RMSE$, MAPE, and~$mAP$, respectively, at the right top, left top, left bottom, and~right bottom. Each subplot contains the boxplots positioned at the corresponding dataset size values and indicating the distribution of all the model prediction metric values for each dataset. Benchmark thresholds are indicated with red dashed horizontal lines for R² (0.85) and $RMSE$ (0.39). Best fit lines for each metric are plotted using different fitting functions, indicated with black dashed lines. $GoF$ values and best model are shown in the legend. A secondary x-axis at the top of each subplot shows the dataset names corresponding to the dataset sizes}{figure.caption.6}{}}
\newlabel{fig:metrics_OOD_datasets@cref}{{[figure][2][0]2}{[1][80][]82}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{ID~Training}{82}{subsubsection*.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Relationship between dataset size and model performance for different object detection models (YOLOv5 (\textbf  {a}), YOLOv8 (\textbf  {b}), YOLO11 (\textbf  {c}), and~RT-DETR (\textbf  {d})) trained and tested on ID datasets. On the same line, each subplot represents a different parameters size of the model, increasing from the left to the right. The~x-axis represents the dataset size, while the left and right y-axis represents the $R^2$ and $mAP$ values, respectively. The solid lines represent the mean values, while the dashed lines indicate the logarithmic fit. The shaded area around the solid lines represents the confidence interval (standard deviation) of $R^2$ or $mAP$. The red dashed horizontal line represents the benchmark $R^2$ value of 0.85. The~combined legend in the lower right corner of each subplot shows the Goodness of Fit ($GoF$) for both $R^2$ and $mAP$.}}{84}{figure.caption.8}\protected@file@percent }
\newlabel{fig:dataset_size_vs_performance}{{3}{84}{Relationship between dataset size and model performance for different object detection models (YOLOv5 (\textbf {a}), YOLOv8 (\textbf {b}), YOLO11 (\textbf {c}), and~RT-DETR (\textbf {d})) trained and tested on ID datasets. On the same line, each subplot represents a different parameters size of the model, increasing from the left to the right. The~x-axis represents the dataset size, while the left and right y-axis represents the $R^2$ and $mAP$ values, respectively. The solid lines represent the mean values, while the dashed lines indicate the logarithmic fit. The shaded area around the solid lines represents the confidence interval (standard deviation) of $R^2$ or $mAP$. The red dashed horizontal line represents the benchmark $R^2$ value of 0.85. The~combined legend in the lower right corner of each subplot shows the Goodness of Fit ($GoF$) for both $R^2$ and $mAP$}{figure.caption.8}{}}
\newlabel{fig:dataset_size_vs_performance@cref}{{[figure][3][0]3}{[1][83][]84}{}{}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {\centering }}}{84}{subfigure.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {\centering }}}{84}{subfigure.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {\centering }}}{84}{subfigure.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {\centering }}}{84}{subfigure.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Relationship between dataset quality and model performance for all object detection models that achieved the benchmark. The~x-axis represents the dataset quality, while the left y-axis represents the $R^2$ values. The red dashed horizontal line represents the benchmark $R^2$ value of 0.85. The~ legend in the lower right corner of the subplot shows the Goodness of Fit ($GoF$) for $R^2$.}}{86}{figure.caption.9}\protected@file@percent }
\newlabel{fig:dataset_quality}{{4}{86}{Relationship between dataset quality and model performance for all object detection models that achieved the benchmark. The~x-axis represents the dataset quality, while the left y-axis represents the $R^2$ values. The red dashed horizontal line represents the benchmark $R^2$ value of 0.85. The~ legend in the lower right corner of the subplot shows the Goodness of Fit ($GoF$) for $R^2$}{figure.caption.9}{}}
\newlabel{fig:dataset_quality@cref}{{[figure][4][0]4}{[1][86][]86}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Predictions from the RT-DETR L trained on 60 images. From the left hand side to the right hand side, the~images show the 1, 2, and 3 ID test dataset tile examples. The predicted bounding boxes in the images are the ones before non-maximum suppression and threshold. Black bounding boxes are the ground truth annotations, while the bounding boxes in the viridis color scale are the model~predictions.}}{86}{figure.caption.10}\protected@file@percent }
\newlabel{fig:annotations_many-shots_size}{{5}{86}{Predictions from the RT-DETR L trained on 60 images. From the left hand side to the right hand side, the~images show the 1, 2, and 3 ID test dataset tile examples. The predicted bounding boxes in the images are the ones before non-maximum suppression and threshold. Black bounding boxes are the ground truth annotations, while the bounding boxes in the viridis color scale are the model~predictions}{figure.caption.10}{}}
\newlabel{fig:annotations_many-shots_size@cref}{{[figure][5][0]5}{[1][86][]86}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Predictions from the RT-DETR X trained on 100 images with a reduction in quality of 35\%. From the left hand side to the right hand side, the~images show the 1, 2, and 3 ID test dataset tile examples. The predicted bounding boxes in the images are the ones before non-maximum suppression and threshold. Black bounding boxes are the ground truth annotations, while the bounding boxes in the viridis color scale are the model~predictions.}}{87}{figure.caption.11}\protected@file@percent }
\newlabel{fig:annotations_many-shots_quality}{{6}{87}{Predictions from the RT-DETR X trained on 100 images with a reduction in quality of 35\%. From the left hand side to the right hand side, the~images show the 1, 2, and 3 ID test dataset tile examples. The predicted bounding boxes in the images are the ones before non-maximum suppression and threshold. Black bounding boxes are the ground truth annotations, while the bounding boxes in the viridis color scale are the model~predictions}{figure.caption.11}{}}
\newlabel{fig:annotations_many-shots_quality@cref}{{[figure][6][0]6}{[1][86][]87}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3.3}Few-Shot Object~Detectors}{87}{subsection.0.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3.4}Zero-Shot Object~Detectors}{88}{subsection.0.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The figure shows the relationship between shots and model performance for the CD-ViTO model trained and tested on ID datasets. The x-axis represents the number of shots. The solid lines represent the mean values, while the dashed lines indicate the shot amount/metric prediction model. The shaded area around the solid lines represents the confidence interval (standard deviation) of the metric. (\textbf  {a}) The left and right y-axis represents the $R^2$ and $mAP$ values, respectively. The red dashed horizontal line represents the benchmark $R^2$ value of 0.85. The~combined legend in the lower right corner of each subplot shows the Goodness of Fit ($GoF$) for both $R^2$ and $mAP$. (\textbf  {b}) The left and right y-axis represents the $RMSE$ and $MAPE$ values respectively. The red dashed horizontal line represents the benchmark $RMSE$ value of 0.39. The~combined legend in the upper right corner of each subplot shows the Goodness of Fit ($GoF$) for both $RMSE$ and MAPE.}}{89}{figure.caption.12}\protected@file@percent }
\newlabel{fig:shots_vs_performance}{{7}{89}{The figure shows the relationship between shots and model performance for the CD-ViTO model trained and tested on ID datasets. The x-axis represents the number of shots. The solid lines represent the mean values, while the dashed lines indicate the shot amount/metric prediction model. The shaded area around the solid lines represents the confidence interval (standard deviation) of the metric. (\textbf {a}) The left and right y-axis represents the $R^2$ and $mAP$ values, respectively. The red dashed horizontal line represents the benchmark $R^2$ value of 0.85. The~combined legend in the lower right corner of each subplot shows the Goodness of Fit ($GoF$) for both $R^2$ and $mAP$. (\textbf {b}) The left and right y-axis represents the $RMSE$ and $MAPE$ values respectively. The red dashed horizontal line represents the benchmark $RMSE$ value of 0.39. The~combined legend in the upper right corner of each subplot shows the Goodness of Fit ($GoF$) for both $RMSE$ and MAPE}{figure.caption.12}{}}
\newlabel{fig:shots_vs_performance@cref}{{[figure][7][0]7}{[1][88][]89}{}{}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {\centering }}}{89}{subfigure.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {\centering }}}{89}{subfigure.7.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The 50 shot CD-ViTO with ViT-B backbone predictions on the 1, 2, and 3 ID test dataset tile examples, respectively, from the left hand side to the right. The predicted bounding boxes in the images are the ones before non-maximum suppression and threshold. Black bounding boxes are the ground truth annotations, while the bounding boxes in the viridis color scale are the \mbox  {model predictions.}}}{90}{figure.caption.13}\protected@file@percent }
\newlabel{fig:annotations_few-shot}{{8}{90}{The 50 shot CD-ViTO with ViT-B backbone predictions on the 1, 2, and 3 ID test dataset tile examples, respectively, from the left hand side to the right. The predicted bounding boxes in the images are the ones before non-maximum suppression and threshold. Black bounding boxes are the ground truth annotations, while the bounding boxes in the viridis color scale are the \mbox {model predictions.}}{figure.caption.13}{}}
\newlabel{fig:annotations_few-shot@cref}{{[figure][8][0]8}{[1][89][]90}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The figure shows the relationship between the OWLv2 model size, used prompt, and model performance. The x-axis represents the model settings and the model size. Colors represent the different prompts used. The four subplots show the $mAP$ (upper left corner), $R^2$ (upper right corner), $RMSE$ (lower left corner), and~MAPE (lower right corner) values. The red dashed horizontal line in the $R^2$ and the $RMSE$ subplots represents, respectively, the benchmarks of 0.85 and 0.39. }}{91}{figure.caption.14}\protected@file@percent }
\newlabel{fig:zeroshots_vs_performance}{{9}{91}{The figure shows the relationship between the OWLv2 model size, used prompt, and model performance. The x-axis represents the model settings and the model size. Colors represent the different prompts used. The four subplots show the $mAP$ (upper left corner), $R^2$ (upper right corner), $RMSE$ (lower left corner), and~MAPE (lower right corner) values. The red dashed horizontal line in the $R^2$ and the $RMSE$ subplots represents, respectively, the benchmarks of 0.85 and 0.39}{figure.caption.14}{}}
\newlabel{fig:zeroshots_vs_performance@cref}{{[figure][9][0]9}{[1][90][]91}{}{}{}}
\citation{davidPlantDetectionCounting2021}
\citation{andvaagCountingCanolaGeneralizable2024}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The best predictions with the OWLv2 model. The ID\_1, ID\_2, and ID\_3 datasets, respectively, from the left hand side to the right. Prediction with owlv2-base-patch16-ensemble model of the ID\_1 dataset, and with owlv2-base-patch16 model on the other two datasets. All the predictions are made with the prompt ``seedling''. The predicted bounding boxes in the images are the ones before non-maximum suppression and threshold. Black bounding boxes are the ground truth annotations, while the bounding boxes in the viridis color scale are the model predictions.}}{92}{figure.caption.15}\protected@file@percent }
\newlabel{fig:annotations_zero-shot}{{10}{92}{The best predictions with the OWLv2 model. The ID\_1, ID\_2, and ID\_3 datasets, respectively, from the left hand side to the right. Prediction with owlv2-base-patch16-ensemble model of the ID\_1 dataset, and with owlv2-base-patch16 model on the other two datasets. All the predictions are made with the prompt ``seedling''. The predicted bounding boxes in the images are the ones before non-maximum suppression and threshold. Black bounding boxes are the ground truth annotations, while the bounding boxes in the viridis color scale are the model predictions}{figure.caption.15}{}}
\newlabel{fig:annotations_zero-shot@cref}{{[figure][10][0]10}{[1][91][]92}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1.4}Discussion}{92}{section.0.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4.1}Dataset Source Impact on Object Detection~Performance}{92}{subsection.0.4.1}\protected@file@percent }
\citation{badgujarAgriculturalObjectDetection2024}
\citation{karamiAutomaticPlantCounting2020}
\citation{nguyenEvaluationDeepLearning2020,sunRevisitingUnreasonableEffectiveness2017}
\citation{rekavandiTransformersSmallObject2023,liTransformerObjectDetection2023}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4.2}Many-Shot Object Detection: Architecture and Dataset~Requirements}{94}{subsection.0.4.2}\protected@file@percent }
\citation{khanSurveyVisionTransformers2023}
\citation{badgujarAgriculturalObjectDetection2024}
\citation{carionEndtoEndObjectDetection2020}
\citation{torres-sanchez_early_2021}
\citation{badgujarAgriculturalObjectDetection2024}
\citation{sunRevisitingUnreasonableEffectiveness2017,hestnessDeepLearningScaling2017}
\citation{davidPlantDetectionCounting2021,andvaagCountingCanolaGeneralizable2024}
\citation{mahmoodHowMuchMore2022}
\citation{alhazmiEffectsAnnotationQuality2021}
\citation{sunRevisitingUnreasonableEffectiveness2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4.3}Dataset Quality~Trade-Offs}{96}{subsection.0.4.3}\protected@file@percent }
\citation{badgujarAgriculturalObjectDetection2024}
\citation{xuDeViTDecomposingVision2023,mindererScalingOpenVocabularyObject2023}
\citation{farjon_deep-learning-based_2023}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4.4}Few-Shot and Zero-Shot Approaches: Current~Limitations}{97}{subsection.0.4.4}\protected@file@percent }
\citation{garcia-martinezDigitalCountCorn2020,zhang2020cut}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4.5}Handcrafted Methods in the Deep Learning~Era}{98}{subsection.0.4.5}\protected@file@percent }
\citation{torres-sanchez_early_2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4.6}Implications for Practical~Applications}{100}{subsection.0.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4.7}Future~Work}{101}{subsection.0.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.1.5}Conclusions}{102}{section.0.5}\protected@file@percent }
\citation{wuOptimizingConnectedComponent2005}
\citation{fischlerRandomSampleConsensus1987}
\@writefile{toc}{\contentsline {section}{\numberline {2.1.6}Appendix~2.1.6}{105}{section.0.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.6.1}Appendix~2.1.6.1}{105}{subsection.0.6.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces H1}}{105}{algorithm.1}\protected@file@percent }
\newlabel{alg:H1}{{1}{105}{H1}{algorithm.1}{}}
\newlabel{alg:H1@cref}{{[algorithm][1][]1}{[1][104][]105}{}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces H2}}{106}{algorithm.2}\protected@file@percent }
\newlabel{alg:H2}{{2}{106}{H2}{algorithm.2}{}}
\newlabel{alg:H2@cref}{{[algorithm][2][]2}{[1][105][]106}{}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces \textit  {Cont.}}}{107}{algorithm.2}\protected@file@percent }
\newlabel{alg:H2}{{2}{107}{\textit {Cont.}}{algorithm.2}{}}
\newlabel{alg:H2@cref}{{[algorithm][2][]2}{[1][106][]107}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.6.2}Appendix~2.1.6.2}{108}{subsection.0.6.2}\protected@file@percent }
\bibcite{blandinoeffetto2016}{{1}{2016}{{Blandino et~al.}}{{Blandino, Testa, Quaglini, and Reyneri}}}
\bibcite{10287390}{{2}{2023}{{Lu et~al.}}{{Lu, Ye, Wang, and Yu}}}
\bibcite{Saatkamp2019AResearchAF}{{3}{2019}{{Saatkamp et~al.}}{{Saatkamp, Cochrane, Commander, Guja, Jimenez-Alfaro, Larson, Nicotra, Poschlod, Silveira, Cross, Dalziell, Dickie, Erickson, Fidelis, Fuchs, Golos, Hope, Lewandrowski, Merritt, Miller, Miller, Offord, Ooi, Satyanti, Sommerville, Tangney, Tomlinson, Turner, and Walck}}}
\bibcite{rs13051030}{{4}{2021}{{De~Petris et~al.}}{{De~Petris, Sarvia, Gullino, Tarantino, and Borgogno-Mondino}}}
\bibcite{PP13332024}{{5}{2024}{{PP1}}{{}}}
\bibcite{zouMaizeTasselsDetection2020}{{6}{2020}{{Zou et~al.}}{{Zou, Lu, Li, Liu, and Cao}}}
\bibcite{linMicrosoftCOCOCommon2015}{{7}{2015}{{Lin et~al.}}{{Lin, Maire, Belongie, Bourdev, Girshick, Hays, Perona, Ramanan, Zitnick, and Doll{\'a}r}}}
\bibcite{krausPhotogrammetryGeometryImages2011}{{8}{2011}{{Kraus}}{{}}}
\bibcite{pugh_comparison_2021}{{9}{2021}{{Pugh et~al.}}{{Pugh, Thorp, Gonzalez, Elshikha, and Pauli}}}
\bibcite{dhonju_web_2023}{{10}{2023}{{Dhonju et~al.}}{{Dhonju, Walsh, and Bhattarai}}}
\bibcite{habib_automated_2016}{{11}{2016}{{Habib et~al.}}{{Habib, Han, Xiong, He, Zhang, and Crawford}}}
\bibcite{de_petris_rpas-based_2020}{{12}{2020}{{De~Petris et~al.}}{{De~Petris, Sarvia, and Borgogno-Mondino}}}
\bibcite{zhang_georeferencing_2022}{{13}{2022}{{Zhang et~al.}}{{Zhang, Barrett, Baros, Neville, Talasila, and Sinclair}}}
\bibcite{farjon_deep-learning-based_2023}{{14}{2023}{{Farjon et~al.}}{{Farjon, Huijun, and Edan}}}
\bibcite{meierBBCHSystemCoding2009}{{15}{2009}{{Meier et~al.}}{{Meier, Bleiholder, Buhr, Feller, Hack, He{\ss }, Lancashire, Schnock, Stau{\ss }, van~den Boom, Weber, and Zwerger}}}
\bibcite{davidPlantDetectionCounting2021}{{16}{2021}{{David et~al.}}{{David, Daubige, Joudelat, Burger, Comar, {de Solan}, and Baret}}}
\bibcite{liuIntegrateNetDeepLearning2022}{{17}{2022}{{Liu et~al.}}{{Liu, Zhou, Wang, Costa, Kaeppler, and Zhang}}}
\bibcite{Maize_seedingDatasetOverview}{{18}{}{{Mai}}{{}}}
\bibcite{MaizeseedlingdetectionDatasetOverview}{{19}{}{{Mai}}{{}}}
\bibcite{fao2024}{{20}{2024}{{FAO}}{{}}}
\bibcite{torres-sanchez_early_2021}{{21}{2021}{{Torres-Sánchez et~al.}}{{Torres-Sánchez, Mesas-Carrascosa, Jiménez-Brenes, de~Castro, and López-Granados}}}
\bibcite{zhang2020cut}{{22}{2020}{{Zhang et~al.}}{{Zhang, Cao, Peng, Liu, Sun, Zhang, and Li}}}
\bibcite{garcia-martinezDigitalCountCorn2020}{{23}{2020}{{{Garc{\'i}a-Mart{\'i}nez} et~al.}}{{{Garc{\'i}a-Mart{\'i}nez}, {Flores-Magdaleno}, {Khalil-Gardezi}, {Ascencio-Hern{\'a}ndez}, {Tijerina-Ch{\'a}vez}, {V{\'a}zquez-Pe{\~n}a}, and {Mancilla-Villa}}}}
\bibcite{lecunDeepLearning2015}{{24}{2015}{{LeCun et~al.}}{{LeCun, Bengio, and Hinton}}}
\bibcite{FasterRCNNRealTime}{{25}{}{{Fas}}{{}}}
\bibcite{YouOnlyLook}{{26}{}{{You}}{{}}}
\bibcite{vaswaniAttentionAllYou2017}{{27}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{carionEndtoEndObjectDetection2020}{{28}{2020}{{Carion et~al.}}{{Carion, Massa, Synnaeve, Usunier, Kirillov, and Zagoruyko}}}
\bibcite{dosovitskiyImageWorth16x162021}{{29}{2021}{{Dosovitskiy et~al.}}{{Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby}}}
\bibcite{14090575ImageNetLarge}{{30}{}{{140}}{{}}}
\bibcite{zongDETRsCollaborativeHybrid2023}{{31}{2023}{{Zong et~al.}}{{Zong, Song, and Liu}}}
\bibcite{khanSurveyVisionTransformers2023}{{32}{2023}{{Khan et~al.}}{{Khan, Rauf, Sohail, Khan, Asif, Asif, and Farooq}}}
\bibcite{badgujarAgriculturalObjectDetection2024}{{33}{2024}{{Badgujar et~al.}}{{Badgujar, Poulose, and Gan}}}
\bibcite{rekavandiTransformersSmallObject2023}{{34}{2023}{{Rekavandi et~al.}}{{Rekavandi, Rashidi, Boussaid, Hoefs, Akbas, and {bennamoun}}}}
\bibcite{liTransformerObjectDetection2023}{{35}{2023}{{Li et~al.}}{{Li, Miao, Ma, Shuang, and Huang}}}
\bibcite{zhaoDETRsBeatYOLOs2024}{{36}{2024}{{Zhao et~al.}}{{Zhao, Lv, Xu, Wei, Wang, Dang, Liu, and Chen}}}
\bibcite{khanamYOLOv11OverviewKey2024}{{37}{2024}{{Khanam and Hussain}}{{}}}
\bibcite{liMetaSGDLearningLearn2017}{{38}{2017}{{Li et~al.}}{{Li, Zhou, Chen, and Li}}}
\bibcite{bansalZeroShotObjectDetection2018}{{39}{2018}{{Bansal et~al.}}{{Bansal, Sikka, Sharma, Chellappa, and Divakaran}}}
\bibcite{kangFewshotObjectDetection2019}{{40}{2019}{{Kang et~al.}}{{Kang, Liu, Wang, Yu, Feng, and Darrell}}}
\bibcite{mindererScalingOpenVocabularyObject2023}{{41}{2023}{{Minderer et~al.}}{{Minderer, Gritsenko, and Houlsby}}}
\bibcite{liuGroundingDINOMarrying2025}{{42}{2025}{{Liu et~al.}}{{Liu, Zeng, Ren, Li, Zhang, Yang, Jiang, Li, Yang, Su, Zhu, and Zhang}}}
\bibcite{karamiAutomaticPlantCounting2020}{{43}{2020}{{Karami et~al.}}{{Karami, Crawford, and Delp}}}
\bibcite{wangAdvancingImageRecognition2024}{{44}{2024}{{Wang et~al.}}{{Wang, Parthasarathy, and Pan}}}
\bibcite{barretoAutomaticUAVbasedCounting2021}{{45}{2021}{{Barreto et~al.}}{{Barreto, Lottes, Ispizua~Yamati, Baumgarten, Wolf, Stachniss, Mahlein, and Paulus}}}
\bibcite{kitanoCornPlantCounting2019}{{46}{2019}{{Kitano et~al.}}{{Kitano, Mendes, Geus, Oliveira, and Souza}}}
\bibcite{andvaagCountingCanolaGeneralizable2024}{{47}{2024}{{Andvaag et~al.}}{{Andvaag, Krys, Shirtliffe, and Stavness}}}
\bibcite{sunRevisitingUnreasonableEffectiveness2017}{{48}{2017}{{Sun et~al.}}{{Sun, Shrivastava, Singh, and Gupta}}}
\bibcite{alhazmiEffectsAnnotationQuality2021}{{49}{2021}{{Alhazmi et~al.}}{{Alhazmi, Alsumari, Seppo, Podkuiko, and Simon}}}
\bibcite{hestnessDeepLearningScaling2017}{{50}{2017}{{Hestness et~al.}}{{Hestness, Narang, Ardalani, Diamos, Jun, Kianinejad, Patwary, Yang, and Zhou}}}
\bibcite{mahmoodHowMuchMore2022}{{51}{2022}{{Mahmood et~al.}}{{Mahmood, Lucas, Acuna, Li, Philion, Alvarez, Yu, Fidler, and Law}}}
\bibcite{nguyenEvaluationDeepLearning2020}{{52}{2020}{{Nguyen et~al.}}{{Nguyen, Do, Ngo, Le, and Valenti}}}
\bibcite{duSpineNetLearningScalePermuted2020}{{53}{2020}{{Du et~al.}}{{Du, Lin, Jin, Ghiasi, Tan, Cui, Le, and Song}}}
\bibcite{shortenSurveyImageData2019}{{54}{2019}{{Shorten and Khoshgoftaar}}{{}}}
\bibcite{liuEstimatingMaizeSeedling2022}{{55}{2022}{{Liu et~al.}}{{Liu, Yin, Feng, Li, Xu, Shi, and Jin}}}
\bibcite{velumaniEstimatesMaizePlant2021}{{56}{2021}{{Velumani et~al.}}{{Velumani, {Lopez-Lozano}, Madec, Guo, Gillet, Comar, and Baret}}}
\bibcite{bumbaca202515235602}{{57}{2025}{{Bumbaca}}{{}}}
\bibcite{krizhevskyImageNetClassificationDeep2012}{{58}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever, and Hinton}}}
\bibcite{fischlerRandomSampleConsensus1987}{{59}{1987}{{Fischler and Bolles}}{{}}}
\bibcite{tervenComprehensiveReviewYOLO2023}{{60}{2023}{{Terven et~al.}}{{Terven, Córdova-Esparza, and Romero-González}}}
\bibcite{jocherGitHubUltralyticsYOLO2023}{{61}{2023}{{Jocher et~al.}}{{Jocher, Qiu, and Chaurasia}}}
\bibcite{oquabDINOv2LearningRobust2024}{{62}{2024}{{Oquab et~al.}}{{Oquab, Darcet, Moutakanni, Vo, Szafraniec, Khalidov, Fernandez, Haziza, Massa, {El-Nouby}, Assran, Ballas, Galuba, Howes, Huang, Li, Misra, Rabbat, Sharma, Synnaeve, Xu, Jegou, Mairal, Labatut, Joulin, and Bojanowski}}}
\bibcite{fuCrossDomainFewShotObject2024}{{63}{2024}{{Fu et~al.}}{{Fu, Wang, Pan, Huai, Qiu, Shangguan, Liu, Fu, Gool, and Jiang}}}
\bibcite{wolfTransformersStateoftheArtNatural2020}{{64}{2020}{{Wolf et~al.}}{{Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu, Scao, Gugger, Drame, Lhoest, and Rush}}}
\bibcite{zhu_harnessing_2024}{{65}{2024}{{Zhu et~al.}}{{Zhu, Qin, Su, Lin, Li, and Gao}}}
\bibcite{s24186109}{{66}{2024}{{Zhou et~al.}}{{Zhou, Yan, Ding, Cai, and Zhang}}}
\bibcite{chen_taskclip_2024}{{67}{2024}{{Chen et~al.}}{{Chen, Huang, Ni, Yun, Liu, Wen, Velasquez, Latapie, and Imani}}}
\bibcite{chicco_coefficient_2021}{{68}{2021}{{Chicco et~al.}}{{Chicco, Warrens, and Jurman}}}
\bibcite{draper1998applied}{{69}{1998}{{Draper and Smith}}{{}}}
\bibcite{ARMSTRONG199269}{{70}{1992}{{Armstrong and Collopy}}{{}}}
\bibcite{everingham_pascal_2010}{{71}{2010}{{Everingham et~al.}}{{Everingham, Van~Gool, Williams, Winn, and Zisserman}}}
\bibcite{vianna_analysis_2024}{{72}{2024}{{Vianna et~al.}}{{Vianna, Gonçalves, and Souza}}}
\bibcite{akyonSlicingAidedHyper2022}{{73}{2022}{{Akyon et~al.}}{{Akyon, Altinuc, and Temizel}}}
\bibcite{xuDeViTDecomposingVision2023}{{74}{2023}{{Xu et~al.}}{{Xu, Hao, Luo, Hu, An, and Mao}}}
\bibcite{wuOptimizingConnectedComponent2005}{{75}{2005}{{Wu et~al.}}{{Wu, Otoo, and Shoshani}}}
\gdef \@abspage@last{71}
